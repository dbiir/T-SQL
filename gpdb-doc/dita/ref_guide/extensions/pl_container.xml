<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Composite//EN" "ditabase.dtd">
<topic id="topic1" xml:lang="en">
  <title id="pz212122">PL/Container Language</title>
  <body>
    <p>This section includes the following information about PL/Container 1.1 and later:</p>
    <ul>
      <li id="pz219023"><xref href="#topic2" type="topic" format="dita"/></li>
      <li><xref href="#topic_resmgmt" format="dita"/></li>
      <li>
        <xref href="#topic_tcm_htd_gw" format="dita"/></li>
      <li id="pz213664" otherprops="pivotal"><xref href="#topic3" type="topic" format="dita"/></li>
      <li otherprops="op-pivotal"><xref href="#topic_qbl_dkq_vcb" format="dita"/></li>
      <li otherprops="op-pivotal"><xref href="#topic_upg110" format="dita"/></li>
      <li id="pz213668"><xref href="#topic6" type="topic" format="dita"/>
      </li>
      <li id="pz215253"><xref href="#topic_rh3_p3q_dw" format="dita"/></li>
      <li><xref href="#topic_ehl_r3q_dw" format="dita"/></li>
      <li><xref href="#topic_lqz_t3q_dw" format="dita"/></li>
      <li><xref href="#topic_sk1_gdq_dw" format="dita"/></li>
      <li><xref href="#topic_ydt_rtc_rbb" format="dita"/></li>
      <li><xref href="#topic_kds_plk_rbb" format="dita"/></li>
    </ul>
    <note otherprops="pivotal">The extension PL/Container 1.1 and later is installed by
        <codeph>gppkg</codeph> as a Greenplum Database extension, while the extension PL/Container
      1.0 is installed as a Greenplum Database language. To upgrade to PL/Container 1.1 or later
      from PL/Container 1.0, you uninstall the old version and install the new version. See <xref
        href="#topic_qbl_dkq_vcb" format="dita"/>.</note>
    <note type="warning">PL/Container is compatible with Greenplum Database 5.2.0 and later.
      PL/Container has not been tested for compatibility with Greenplum Database 5.1.0 or 5.0.0.
    </note>
  </body>
  <topic id="topic2" xml:lang="en">
    <title id="pz217886">About the PL/Container Language Extension</title>
    <body>
      <p>The Greenplum Database PL/Container language extension (PL/Container) is an interface that
        allows Greenplum Database to interact with a Docker container to execute a user-defined
        function (UDF) in the container. Docker containers ensure the user code cannot access the
        file system of the source host. Also, containers are started without network access or with
        limited network access and cannot connect back to Greenplum Database or open any other
        external connections. For information about available UDF languages, see <xref
          href="#topic_tcm_htd_gw" format="dita"/></p>
      <p>Generally speaking, a Docker <i>container</i> is a Linux process that runs in a managed way
        by using Linux kernel features such as cgroups, namespaces and union file systems. A Docker
          <i>image</i> is the basis of a container. A Docker container is a running instance of a
        Docker image. When you start a Docker container you specify a Docker image. A Docker image
        is the collection of root filesystem changes and execution parameters that are used when you
        run a Docker container on the host system. An image does not have state and never changes.
        For information about Docker, see the Docker web site <xref href="https://www.docker.com/"
          format="html" scope="external">https://www.docker.com/</xref>. </p>
      <p>Greenplum Database starts a container only on the first call to a function in that
        container. For example, consider a query that selects table data using all available
        segments, and applies a transformation to the data using a PL/Container function. In this
        case, Greenplum Database would start the Docker container only once on each segment, and
        then contact the running container to obtain the results.</p>
      <p>After starting a full cycle of a query execution, the executor sends a call to the
        container. The container might respond with an SPI - SQL query executed by the container to
        get some data back from the database, returning the result to the query executor.</p>
      <p>The container shuts down when the connection to it is closed. This occurs when you close
        the Greenplum Database session that started the container. A container running in standby
        mode has almost no consumption of CPU resources as it is waiting on the socket. PL/Container
        memory consumption depends on the amount of data you cache in global dictionaries.</p>
      <note type="warning">PL/Container is not supported when Greenplum Database is run within a
        Docker container.</note>
      <p>The PL/Container language extension is available as an open source module. For information
        about the module, see the README file in the GitHub repository at <xref
          href="https://github.com/greenplum-db/plcontainer" format="html" scope="external"
          >https://github.com/greenplum-db/plcontainer</xref>.</p>
    </body>
  </topic>
  <topic id="topic_resmgmt">
    <title>About PL/Container Resource Management</title>
    <body>
      <p>Greenplum Database runs PL/Container user-defined functions in Docker containers. The
        Docker containers and the Greenplum Database server share CPU and memory resources on the
        same hosts. In the default case, Greenplum Database is unaware of the resources consumed by
        running PL/Container instances. In PL/Container version 1.2 and later, you can use Greenplum
        Database resource groups to control overall CPU and memory resource usage for running
        PL/Container instances, as described in the following section.</p>
      <p>PL/Container manages resource usage at two levels - the container level and the runtime
        level. You can control container-level CPU and memory resources with the
          <codeph>memory_mb</codeph> and <codeph>cpu_share</codeph> settings that you configure for
        the PL/Container runtime. <codeph>memory_mb</codeph> governs the memory resources available
        to each container instance. The <codeph>cpu_share</codeph> setting identifies the relative
        weighting of a container's CPU usage compared to other containers. Refer to <xref
          href="#topic_ojn_r2s_dw" format="dita"/> for PL/Container configuration information.</p>
      <p>You cannot by default restrict the number of executing PL/Container container instances,
        nor can you restrict the total amount of memory or CPU resources that they consume.</p>
    </body>
    <topic id="topic_resgroup">
      <title>Using Resource Groups to Manage PL/Container Resources</title>
      <body>
        <p>In PL/Container version 1.2.0 and later, you can use Greenplum Database resource groups
          to manage and limit the total CPU and memory resources of containers in PL/Container
          runtimes. For more information about enabling, configuring, and using Greenplum Database
          resource groups, refer to <xref href="../../admin_guide/workload_mgmt_resgroups.xml"
            format="dita" scope="peer">Using Resource Groups</xref> in the <cite>Greenplum Database
            Administrator Guide</cite>.</p>
        <note>If you do not explicitly configure resource groups for a PL/Container runtime, its
          container instances are limited only by system resources. The containers may consume
          resources at the expense of the Greenplum Database server.</note>
        <p>Resource groups for external components such as PL/Container use Linux control groups
          (cgroups) to manage component-level use of memory and CPU resources. When you manage
          PL/Container resources with resource groups, you configure both a memory limit and a CPU
          limit that Greenplum Database applies to all container instances that share the same
          PL/Container runtime configuration.</p>
        <p>When you create a resource group to manage the resources of a PL/Container runtime, you
          must specify <codeph>MEMORY_AUDITOR=cgroup</codeph> and <codeph>CONCURRENCY=0</codeph> in
          addition to the required CPU and memory limits. For example, the following command creates
          a resource group named <codeph>plpy_run1_rg</codeph> for a PL/Container runtime:
          <codeblock>CREATE RESOURCE GROUP plpy_run1_rg WITH (MEMORY_AUDITOR=cgroup, CONCURRENCY=0,
     CPU_RATE_LIMIT=10, MEMORY_LIMIT=10);</codeblock></p>
        <p>PL/Container does not use the <codeph>MEMORY_SHARED_QUOTA</codeph> and
            <codeph>MEMORY_SPILL_RATIO</codeph> resource group memory limits. Refer to the
              <codeph><xref href="../../ref_guide/sql_commands/CREATE_RESOURCE_GROUP.xml"
              format="dita" scope="peer">CREATE RESOURCE GROUP</xref></codeph> reference page for
          detailed information about this SQL command.</p>
        <p>You can create one or more resource groups to manage your running PL/Container instances.
          After you create a resource group for PL/Container, you assign the resource group to one
          or more PL/Container runtimes. You make this assignment using the <codeph>groupid</codeph>
          of the resource group. You can determine the <codeph>groupid</codeph> for a given resource
          group name from the <codeph>gp_resgroup_config</codeph>
          <codeph>gp_toolkit</codeph> view. For example, the following query displays the
            <codeph>groupid</codeph> of a resource group named
          <codeph>plpy_run1_rg</codeph>:<codeblock>SELECT groupname, groupid FROM gp_toolkit.gp_resgroup_config
     WHERE groupname='plpy_run1_rg';

  groupname   |  groupid  
--------------+----------
 plpy_run1_rg |   16391
(1 row)</codeblock></p>
        <p>You assign a resource group to a PL/Container runtime configuration by specifying the
            <codeph>-s resource_group_id=<varname>rg_groupid</varname></codeph> option to the
            <codeph>plcontainer runtime-add</codeph> (new runtime) or <codeph>plcontainer
            runtime-replace</codeph> (existing runtime) commands. For example, to assign the
            <codeph>plpy_run1_rg</codeph> resource group to a new PL/Container runtime named
            <codeph>python_run1</codeph>:
          <codeblock>plcontainer runtime-add -r python_run1 -i pivotaldata/plcontainer_python_shared:devel -l python -s resource_group_id=16391</codeblock></p>
        <p>You can also assign a resource group to a PL/Container runtime using the
            <codeph>plcontainer runtime-edit</codeph> command. For information about the
            <codeph>plcontainer</codeph> command, see <xref href="#topic_rw3_52s_dw" format="dita"
          />.</p>
        <p>After you assign a resource group to a PL/Container runtime, all container instances that
          share the same runtime configuration are subject to the memory limit and the CPU limit
          that you configured for the group. If you decrease the memory limit of a PL/Container
          resource group, queries executing in running containers in the group may fail with an out
          of memory error. If you drop a PL/Container resource group while there are running
          container instances, Greenplum Database kills the running containers.</p>
      </body>
    </topic>
    <topic id="topic_resgroupcfg">
      <title>Configuring Resource Groups for PL/Container</title>
      <body>
        <p>To use Greenplum Database resource groups to manage PL/Container resources, you must
          explicitly configure both resource groups and PL/Container.</p>
        <note>PL/Container version 1.2 utilizes the new resource group capabilities introduced in
          Greenplum Database version 5.8.0. If you downgrade to a Greenplum Database system that
          uses PL/Container version 1.1. or earlier, you must use <codeph>plcontainer
            runtime-edit</codeph> to remove any <codeph>resource_group_id</codeph> settings from
          your PL/Container runtime configuration.</note>
      </body>
      <topic id="topic_resgroupcfg_proc">
        <title>Procedure</title>
        <body>
          <p>Perform the following procedure to configure PL/Container to use Greenplum Database
            resource groups for CPU and memory resource management:</p>
          <ol>
            <li>If you have not already configured and enabled resource groups in your Greenplum
              Database deployment, configure cgroups and enable Greenplum Database resource groups
              as described in <xref
                href="../../admin_guide/workload_mgmt_resgroups.xml#topic71717999" format="dita"
                scope="peer">Using Resource Groups</xref> in the <cite>Greenplum Database
                Administrator Guide</cite>.
              <note>If you have previously configured and enabled resource groups in your
                deployment, ensure that the Greenplum Database resource group
                  <codeph>gpdb.conf</codeph> cgroups configuration file includes a <codeph>memory {
                  }</codeph> block as described in the previous link.</note></li>
            <li>Analyze the resource usage of your Greenplum Database deployment. Determine the
              percentage of resource group CPU and memory resources that you want to allocate to
              PL/Container Docker containers.</li>
            <li>Determine how you want to distribute the total PL/Container CPU and memory resources
              that you identified in the step above among the PL/Container runtimes. Identify:<ul>
                <li>The number of PL/Container resource group(s) that you require.</li>
                <li>The percentage of memory and CPU resources to allocate to each resource
                  group.</li>
                <li>The resource-group-to-PL/Container-runtime assignment(s).</li>
              </ul></li>
            <li>Create the PL/Container resource groups that you identified in the step above. For
              example, suppose that you choose to allocate 25% of both memory and CPU Greenplum
              Database resources to PL/Container. If you further split these resources among 2
              resource groups 60/40, the following SQL commands create the resource
              groups:<codeblock>CREATE RESOURCE GROUP plr_run1_rg WITH (MEMORY_AUDITOR=cgroup, CONCURRENCY=0,
    CPU_RATE_LIMIT=15, MEMORY_LIMIT=15);
CREATE RESOURCE GROUP plpy_run1_rg WITH (MEMORY_AUDITOR=cgroup, CONCURRENCY=0,
    CPU_RATE_LIMIT=10, MEMORY_LIMIT=10);</codeblock></li>
            <li>Find and note the <codeph>groupid</codeph> associated with each resource group that
              you created. For
              example:<codeblock>SELECT groupname, groupid FROM gp_toolkit.gp_resgroup_config 
    WHERE groupname IN ('plpy_run1_rg', 'plr_run1_rg');

  groupname   |  groupid
--------------+----------
 plpy_run1_rg |   16391
 plr_run1_rg  |   16393
(1 row)</codeblock></li>
            <li>Assign each resource group that you created to the desired PL/Container runtime
              configuration. If you have not yet created the runtime configuration, use the
                <codeph>plcontainer runtime-add</codeph> command. If the runtime already exists, use
              the <codeph>plcontainer runtime-replace</codeph> or <codeph>plcontainer
                runtime-edit</codeph> command to add the resource group assignment to the runtime
              configuration. For example:
                <codeblock>plcontainer runtime-add -r python_run1 -i pivotaldata/plcontainer_python_shared:devel -l python -s resource_group_id=16391
plcontainer runtime-replace -r r_run1 -i pivotaldata/plcontainer_r_shared:devel -l r -s resource_group_id=16393</codeblock><p>For
                information about the <codeph>plcontainer</codeph> command, see <xref
                  href="#topic_rw3_52s_dw" format="dita"/>.</p></li>
          </ol>
        </body>
      </topic>
    </topic>
  </topic>
  <topic id="topic_tcm_htd_gw">
    <title>PL/Container Docker Images</title>
    <body>
      <p>A PL/Python image and a PL/R image are available from the Greenplum Database product
        download site of Pivotal Network at <xref
          href="https://network.pivotal.io/products/pivotal-gpdb" format="html" scope="external"
          >https://network.pivotal.io/</xref>.</p>
      <ul id="ul_epg_t2v_qbb">
        <li>PL/Container for Python - Docker image with Python 2.7.12 installed.<p>The Python Data
            Science Module is also installed. The module contains a set python libraries related to
            data science. <ph otherprops="pivotal">For information about the module, see <xref
                href="../../install_guide/install_python_dsmod.xml" format="dita" scope="peer"
                >Python Data Science Module Package</xref>.</ph></p></li>
      </ul>
      <ul id="ul_fpg_t2v_qbb">
        <li>PL/Container for R - A Docker image with container with R-3.3.3 installed. <p>The R Data
            Science package is also installed. The package contains a set of R libraries related to
            data science. <ph otherprops="pivotal">For information about the module, see <xref
                href="../../install_guide/install_r_dslib.xml" format="dita" scope="peer">R Data
                Science Library Package</xref>.</ph></p></li>
      </ul>
      <p>The Docker image tag represents the PL/Container release version (for example, 1.0.0). For
        example, the full Docker image name for the PL/Container for Python Docker image is similar
        to <codeph>pivotaldata/plc_python_shared:1.0.0</codeph>. This is the name that is referred
        to in the default PL/Container configuration. Also, You can create custom Docker images,
        install the image and add the image to the PL/Container configuration. </p>
    </body>
  </topic>
  <topic id="topic_i31_3tr_dw">
    <title>Prerequisites</title>
    <body>
      <p>Ensure your Greenplum Database system meets the following prerequisites:</p>
      <ul id="ul_ztj_kzp_dw">
        <li>PL/Container is supported on <ph otherprops="pivotal">Pivotal </ph>Greenplum Database
          5.2.x on Red Hat Enterprise Linux (RHEL) 7.x (or later) and CentOS 7.x (or later).<note
            id="plc-issue">PL/Container is not supported on RHEL/CentOS 6.x systems, because those
            platforms do not officially support Docker.</note></li>
        <li>These are Docker host operating system prerequisites.<p>RHEL or CentOS 7.x - Minimum
            supported Linux OS kernel version is 3.10. RHEL 7.x and CentOS 7.x use this kernel
            version.</p><p>You can check your kernel version with the command <codeph>uname
              -r</codeph></p>
          <note>The Red Hat provided, maintained, and supported version of Docker is only available
            on RHEL 7. Docker feature developments are tied to RHEL7.x infrastructure components for
            kernel, devicemapper (thin provisioning, direct lvm), sVirt and systemd.</note></li>
      </ul>
      <ul id="ul_b5j_kzp_dw">
        <li>Docker is installed on Greenplum Database hosts (master, primary and all standby
            hosts)<ul id="ul_z2t_bxd_rbb">
            <li>For RHEL or CentOS 7.x - Docker 17.05</li>
          </ul><p>See <xref href="#topic_ydt_rtc_rbb" format="dita"/>.</p></li>
        <li>On each Greenplum Database host the <codeph>gpadmin</codeph> user should be part of the
            <codeph>docker</codeph> group for the user to be able to manage Docker images and
          containers.</li>
      </ul>
    </body>
  </topic>
  <topic id="topic3" xml:lang="en">
    <title id="pz214493">Installing the PL/Container Language Extension</title>
    <body>
      <p>To use PL/Container, install the PL/Container language extension, install Docker images,
        and configure PL/Container to use the images.<ol id="ol_uw5_xdn_sbb">
          <li>Ensure the Greenplum Database hosts meet the prerequisites, see <xref
              href="#topic_i31_3tr_dw" format="dita"/>.</li>
          <li otherprops="pivotal">Install the PL/Container extension, see <xref
              href="#topic_ifk_2tr_dw" format="dita"/>.<p>If you are upgrading from PL/Container
              1.0, see <xref href="#topic_qbl_dkq_vcb" format="dita"/>.</p></li>
          <li otherprops="oss-only">Build and Install the PL/Container extension from source, see
              <xref href="#topic_i2t_v2n_sbb" format="dita"/>.</li>
          <li>Install Docker images and configure PL/Container, see <xref href="#topic_qcr_bfk_rbb"
              format="dita"/>.</li>
        </ol></p>
    </body>
    <topic id="topic_ifk_2tr_dw" otherprops="pivotal">
      <title>Installing the PL/Container Language Extension Package</title>
      <!--Pivotal conent-->
      <body>
        <p>Install the PL/Container language extension with the Greenplum Database
            <codeph>gppkg</codeph> utility.</p>
        <ol id="ul_w5b_nzp_dw">
          <li>Copy the PL/Container language extension package to the Greenplum Database master host
            as the <codeph>gpadmin</codeph> user.</li>
          <li>Make sure Greenplum Database is up and running. If not, bring it up with this
            command.<codeblock>gpstart -a</codeblock></li>
          <li>Run the package installation
            command.<codeblock>gppkg -i plcontainer-1.1.0-rhel7-x86_64.gppkg</codeblock></li>
          <li>Source the file
            <codeph>$GPHOME/greenplum_path.sh</codeph>.<codeblock>source $GPHOME/greenplum_path.sh</codeblock></li>
          <li>Restart Greenplum Database.<codeblock>gpstop -ra</codeblock></li>
          <li>Enable PL/Container for specific databases by running this command.<ol
              id="ol_ydy_bjq_vcb">
              <li>For PL/Container 1.1 and later, log into the database as a Greenplum Database
                superuser (<codeph>gpadmin</codeph>) and run this
                  command.<codeblock>CREATE EXTENSION plcontainer; </codeblock><p>The command
                  registers PL/Container and creates PL/Container-specific functions and
                views.</p></li>
              <li>For PL/Container 1.0, run this
                  command.<codeblock>psql -d <varname>your_database</varname> -f $GPHOME/share/postgresql/plcontainer/plcontainer_install.sql</codeblock><p>The
                  SQL script registers the language <codeph>plcontainer</codeph> in the database and
                  creates PL/Container-specific functions and views.</p></li>
            </ol></li>
        </ol>
        <p>After installing PL/Container, you can manage Docker images and manage the PL/Container
          configuration with the Greenplum Database <codeph>plcontainer</codeph> utility.</p>
      </body>
    </topic>
    <topic id="topic_qbl_dkq_vcb" otherprops="pivotal">
      <title>Upgrading from PL/Container 1.0</title>
      <body>
        <p>To upgrade to version 1.1 or higher, uninstall version 1.0 and install the new version.
          The <codeph>gppkg</codeph> utility installs PL/Container version 1.1 and later as a
          Greenplum Database extension, while PL/Container 1.0 is installed as a Greenplum Database
          language. The Docker images and the PL/Container configuration do not change when
          upgrading PL/Container, only the PL/Container extension installation changes.</p>
        <p>As part of the upgrade process, you must drop PL/Container from all databases that are
          configured with PL/Container.</p>
        <note type="important">Dropping PL/Container from a database drops all PL/Container UDFs
          from the database, including user-created PL/Container UDFs. If the UDFs are required,
          ensure you can re-create the UDFs before dropping PL/Container. This
            <codeph>SELECT</codeph> command lists the names of and body of PL/Container UDFs in a
            database.<codeblock>SELECT proname, prosrc FROM pg_proc WHERE prolang = (SELECT oid FROM pg_language WHERE lanname = 'plcontainer');</codeblock><p>For
            information about the catalog tables, <codeph>pg_proc</codeph> and
              <codeph>pg_language</codeph>, see <xref
              href="../system_catalogs/catalog_ref-tables.xml" format="dita"/>. </p></note>
        <p>These steps upgrade from PL/Container 1.0 to PL/Container 1.1 or later in a database. The
          steps save the PL/Container 1.0 configuration and restore the configuration for use with
          PL/Container 1.1 or later.<ol id="ol_axf_mmq_vcb">
            <li>Save the PL/Container configuration. This example saves the configuration to
                <codeph>plcontainer10-backup.xml</codeph> in the local
              directory.<codeblock>plcontainer runtime-backup -f plcontainer10-backup.xml</codeblock></li>
            <li>Remove any <codeph>setting</codeph> elements that contain the
                <codeph>use_container_network</codeph> attribute from the configuration file. For
              example, this <codeph>setting</codeph> element must be removed from the configuration
              file.<codeblock>&lt;setting use_container_network="yes"/></codeblock></li>
            <li>Run the <codeph>plcontainer_uninstall.sql</codeph> script as the
                <codeph>gpadmin</codeph> user for each database that is configured with
              PL/Container. For example, this command drops the <codeph>plcontainer</codeph>
              language in the <codeph>mytest</codeph> database.
                <codeblock>psql -d mytest -f $GPHOME/share/postgresql/plcontainer/plcontainer_uninstall.sql</codeblock><p>The
                script drops the <codeph>plcontainer</codeph> language with the
                  <codeph>CASCADE</codeph> clause that drops PL/Container-specific functions and
                views in the database. </p></li>
            <li>Use the Greenplum Database <codeph>gppkg</codeph> utility with the
                <codeph>-r</codeph> option to uninstall the PL/Container language extension. This
              example uninstalls the PL/Container language extension on a Linux
              system.<codeblock>$ gppkg -r plcontainer-1.0.0</codeblock></li>
            <li>Run the package installation command. This example installs the PL/Container 1.1
              language extension on a Linux
              system.<codeblock>gppkg -i plcontainer-1.1.0-rhel7-x86_64.gppkg</codeblock></li>
            <li>Source the file
              <codeph>$GPHOME/greenplum_path.sh</codeph>.<codeblock>source $GPHOME/greenplum_path.sh</codeblock></li>
            <li>Update the PL/Container configuration. This command restores the saved
              configuration.<codeblock>plcontainer runtime-restore -f plcontainer10-backup.xml</codeblock></li>
            <li>Restart Greenplum Database.<codeblock>gpstop -ra</codeblock></li>
            <li>Register the new PL/Container extension as an extension for each database that uses
              PL/Container UDFs. This <codeph>psql</codeph> command runs a <codeph>CREATE
                EXTENSION</codeph> command to register PL/Container in the database
                <codeph>mytest</codeph>.
                <codeblock>psql -d mytest -c 'CREATE EXTENSION plcontainer;'</codeblock><p>The
                command registers PL/Container as an extension and creates PL/Container-specific
                functions and views.</p></li>
          </ol></p>
        <p>After upgrading PL/Container for a database, re-install any user-created PL/Container
          UDFs that are required.</p>
      </body>
    </topic>
    <topic id="topic_upg110" otherprops="pivotal">
      <title>Upgrading from PL/Container 1.1</title>
      <body>
        <p>To upgrade from PL/Container version 1.1 or higher, you save the current configuration,
          upgrade PL/Container, and then restore the configuration after upgrade. There is no need
          to update the Docker images when you upgrade PL/Container.</p>
        <note>Before you perform this upgrade procedure, ensure that you have migrated your
          PL/Container 1.1 package from your previous Greenplum Database installation to your new
          Greenplum Database installation. Refer to the <xref
            href="../../utility_guide/ref/gppkg.html#topic1" format="dita" scope="peer"
            >gppkg</xref> command for package installation and migration information.</note>
        <p>Perform the following procedure to upgrade from PL/Container 1.1 to PL/Container version
          1.2 or later.<ol id="ol_axf_mmq_vcb">
            <li>Save the PL/Container configuration. For example, to save the configuration to a
              file named <codeph>plcontainer110-backup.xml</codeph> in the local
              directory:<codeblock>$ plcontainer runtime-backup -f plcontainer110-backup.xml</codeblock></li>
            <li>Use the Greenplum Database <codeph>gppkg</codeph> utility with the
                <codeph>-u</codeph> option to update the PL/Container language extension. For
              example, the following command updates the PL/Container language extension to version
              1.2 on a Linux
              system:<codeblock>$ gppkg -u plcontainer-1.2.0-rhel7-x86_64.gppkg</codeblock></li>
            <li>Source the Greenplum Database environment file
                <codeph>$GPHOME/greenplum_path.sh</codeph>.<codeblock>$ source $GPHOME/greenplum_path.sh</codeblock></li>
            <li>Restore the PL/Container configuration. For example, this command restores the
              PL/Container configuration that you saved in a previous step:
              <codeblock>$ plcontainer runtime-restore -f plcontainer110-backup.xml</codeblock></li>
            <li>Restart Greenplum Database.<codeblock>$ gpstop -ra</codeblock></li>
            <li>You do not need to re-register the PL/Container extension in the databases in which
              you previously created the extension. Do register the PL/Container extension in each
              new database that will run PL/Container UDFs. For example, the following command
              registers PL/Container in a database named <codeph>mytest</codeph>: <codeblock>$ psql -d mytest -c 'CREATE EXTENSION plcontainer;'</codeblock>
              <p>The command also creates PL/Container-specific functions and views.</p></li>
          </ol></p>
        <note>PL/Container version 1.2 utilizes the new resource group capabilities introduced in
          Greenplum Database version 5.8.0. If you downgrade to a Greenplum Database system that
          uses PL/Container version 1.1. or earlier, you must use <codeph>plcontainer
            runtime-edit</codeph> to remove any <codeph>resource_group_id</codeph> settings from
          your PL/Container runtime configuration.</note>
      </body>
    </topic>
    <topic id="topic_i2t_v2n_sbb" otherprops="oss-only">
      <title>Building and Installing the PL/Container Language Extension</title>
      <!--oss only conent-->
      <body>
        <p>The PL/Container language extension is available as an open source module. For
          information about the building and installing the module as part of Greenplum Database,
          see the README file in the GitHub repository at <xref
            href="https://github.com/greenplum-db/plcontainer" format="html" scope="external"
            >https://github.com/greenplum-db/plcontainer</xref>.</p>
      </body>
    </topic>
  </topic>
  <topic id="topic_qcr_bfk_rbb">
    <title>Installing PL/Container Docker Images</title>
    <body>
      <p>The PL/Container language extension includes the <codeph>plcontainer</codeph> utility that
        installs Docker images on the Greenplum Database hosts and adds configuration information to
        the PL/Container configuration file. The configuration information allows PL/Container to
        create Docker containers with the Docker images. For information about
          <codeph>plcontainer</codeph>, see <xref href="#topic_rw3_52s_dw" format="dita"/>.</p>
      <!--Pivotal conent-->
      <p otherprops="pivotal">Download the <codeph>tar.gz</codeph> file that contains the Docker
        images from <xref href="https://network.pivotal.io/products/pivotal-gpdb" scope="external"
          format="html" class="- topic/xref ">Pivotal Network</xref>. <ul id="ul_vsj_pxb_tbb">
          <li><codeph>plcontainer-python-images-1.0.0.tar.gz</codeph></li>
          <li><codeph>plcontainer-r-images-1.0.0.tar.gz</codeph></li>
        </ul></p>
      <!--oss only conent-->
      <p otherprops="oss-only">The PL/Container open source module contains dockerfiles to build
        Docker images that can be used with PL/Container. You can build a Docker image to run
        PL/Python UDFs and a Docker image to run PL/R UDFs. See the dockerfiles in the GitHub
        repository at <xref href="https://github.com/greenplum-db/plcontainer" format="html"
          scope="external">https://github.com/greenplum-db/plcontainer</xref>.</p>
      <p>Install the Docker images on the Greenplum Database hosts. This example uses the
          <codeph>plcontainer</codeph> utility to install a Docker image for Python and to update
        the PL/Container configuration. The example assumes the Docker image to be installed is in a
        file in <codeph>/home/gpadmin</codeph>.</p>
      <p>This <codeph>plcontainer</codeph> command installs the Docker image for PL/Python from a
        Docker image file.
        <codeblock>plcontainer image-add -f /home/gpadmin/plcontainer-python-images-1.0.0.tar.gz</codeblock></p>
      <p>The utility displays progress information as it installs the Docker image on the Greenplum
        Database hosts. </p>
      <p>Use the <codeph>plcontainer image-list</codeph> command to display the installed Docker
        images on the local host.</p>
      <p>This command adds information to the PL/Container configuration file so that PL/Container
        can access the Docker image to create a Docker
        container.<codeblock>plcontainer runtime-add -r plc_py -i pivotaldata/plcontainer_python_shared:devel -l python</codeblock></p>
      <p>The utility displays progress information as it updates the PL/Container configuration file
        on the Greenplum Database instances.</p>
      <p>You can view the PL/Container configuration information with the <codeph>plcontainer
          runtime-show -r plc_py</codeph> command. You can view the PL/Container configuration XML
        file with the <codeph>plcontainer runtime-edit</codeph> command. </p>
    </body>
  </topic>
  <topic id="topic6" xml:lang="en">
    <title id="pz213704">Uninstalling PL/Container</title>
    <body>
      <p>To uninstall PL/Container, remove Docker containers and images, and then remove the
        PL/Container support from Greenplum Database.</p>
      <p>When you remove support for PL/Container, the <codeph>plcontainer</codeph> user-defined
        functions that you created in the database will no longer work. </p>
    </body>
    <topic id="topic_rnb_4s5_lw">
      <title>Uninstall Docker Containers and Images</title>
      <body>
        <p>On the Greenplum Database hosts, uninstall the Docker containers and images that are no
          longer required. </p>
        <p>The <codeph>plcontainer image-list</codeph> command lists the Docker images that are
          installed on the local Greenplum Database host. </p>
        <p>The <codeph>plcontainer image-delete</codeph> command deletes a specified Docker image
          from all Greenplum Database hosts. </p>
        <p>Some Docker containers might exist on a host if the containers were not managed by
          PL/Container. You might need to remove the containers with Docker commands. These
            <codeph>docker</codeph> commands manage Docker containers and images on a local host.<ul
            id="ul_emd_ts5_lw">
            <li>The command <codeph>docker ps -a</codeph> lists all containers on a host. The
              command <codeph>docker stop</codeph> stops a container.</li>
            <li>The command <codeph>docker images</codeph> lists the images on a host.</li>
            <li>The command <codeph>docker rmi</codeph> removes images.</li>
            <li>The command <codeph>docker rm</codeph> removes containers. </li>
          </ul></p>
      </body>
    </topic>
    <topic xml:lang="en" id="topic_qnb_3cj_kw">
      <title>Remove PL/Container Support for a Database</title>
      <body>
        <p>For a database that no long requires PL/Container, remove support for PL/Container.</p>
        <section>
          <title>PL/Container 1.1 and Later</title>
          <p>For PL/Container 1.1 and later, drop the extension from the database. This
              <codeph>psql</codeph> command runs a <codeph>DROP EXTENION</codeph> command to remove
            PL/Container in the database <codeph>mytest</codeph>.
            <codeblock>psql -d mytest -c 'DROP EXTENSION plcontainer cascade;'</codeblock></p>
          <p>The command drops the <codeph>plcontainer</codeph> extension and drops
            PL/Container-specific functions and views from the database.</p>
        </section>
        <section>
          <title>PL/Container 1.0</title>
          <p>Run the <codeph>plcontainer_uninstall.sql</codeph> script as the
              <codeph>gpadmin</codeph> user. For example, this command removes the
              <codeph>plcontainer</codeph> language in the <codeph>mytest</codeph> database. </p>
          <codeblock>psql -d mytest -f $GPHOME/share/postgresql/plcontainer/plcontainer_uninstall.sql</codeblock>
          <p>The script drops the <codeph>plcontainer</codeph> language with
              <codeph>CASCADE</codeph> to drop PL/Container-specific functions and views from the
            database.</p>
        </section>
      </body>
    </topic>
    <topic xml:lang="en" id="topic_dty_fcj_kw" otherprops="pivotal">
      <title>Uninstalling PL/Container Language Extension</title>
      <body>
        <p>If no databases have <codeph>plcontainer</codeph> as a registered language, uninstall the
          Greenplum Database PL/Container language extension with the <codeph>gppkg</codeph>
          utility. </p>
        <ol id="ol_ety_fcj_kw">
          <li>Use the Greenplum Database <codeph>gppkg</codeph> utility with the <codeph>-r</codeph>
            option to uninstall the PL/Container language extension. This example uninstalls the
            PL/Container language extension on a Linux
              system:<codeblock>$ gppkg -r plcontainer-1.1.0</codeblock><p>You can run the
                <codeph>gppkg</codeph> utility with the options <codeph>-q --all</codeph> to list
              the installed extensions and their versions.</p></li>
          <li>Reload
            <codeph>greenplum_path.sh</codeph>.<codeblock>$ source $GPHOME/greenplum_path.sh</codeblock></li>
          <li>Restart the database.<codeblock>$ gpstop -ra</codeblock></li>
        </ol>
      </body>
    </topic>
  </topic>
  <topic id="topic_rh3_p3q_dw">
    <title>Using PL/Container Functions</title>
    <body>
      <p>When you enable PL/Container in a database of a Greenplum Database system, the language
          <codeph>plcontainer</codeph> is registered in the database. You can create and run
        user-defined functions in the procedural languages supported by the PL/Container Docker
        images when you specify <codeph>plcontainer</codeph> as a language in a UDF definition.</p>
      <p>A UDF definition that uses PL/Container must have the these items.</p>
      <ul id="ul_z2m_1kj_kw">
        <li>The first line of the UDF must be <codeph># container:
          <varname>ID</varname></codeph></li>
        <li>The <codeph>LANGUAGE</codeph> attribute must be <codeph>plcontainer</codeph></li>
      </ul>
      <p>The <varname>ID</varname> is the name that PL/Container uses to identify a Docker image.
        When Greenplum Database executes a UDF on a host, the Docker image on the host is used to
        start a Docker container that runs the UDF. In the XML configuration file
          <codeph>plcontainer_configuration.xml</codeph>, there is a <codeph>runtime</codeph> XML
        element that contains a corresponding <codeph>id</codeph> XML element that specifies the
        Docker container startup information. See <xref href="#topic_sk1_gdq_dw" format="dita"/> for
        information about how PL/Container maps the <varname>ID</varname> to a Docker image. See
          <xref href="#topic9" format="dita"/> for example UDF definitions.</p>
      <p>The PL/Container configuration file is read only on the first invocation of a PL/Container
        function in each Greenplum Database session that runs PL/Container functions. You can force
        the configuration file to be re-read by performing a <codeph>SELECT</codeph> command on the
        view <codeph>plcontainer_refresh_config</codeph> during the session. For example, this
          <codeph>SELECT</codeph> command forces the configuration file to be read.</p>
      <codeblock>SELECT * FROM plcontainer_refresh_config;</codeblock>
      <p>Running the command executes a PL/Container function that updates the configuration on the
        master and segment instances and returns the status of the
        refresh.<codeblock> gp_segment_id | plcontainer_refresh_local_config
---------------+----------------------------------
             1 | ok
             0 | ok
            -1 | ok
(3 rows)</codeblock></p>
      <p>Also, you can show all the configurations in the session by performing a
          <codeph>SELECT</codeph> command on the view <codeph>plcontainer_show_config</codeph>. For
        example, this <codeph>SELECT</codeph> command returns the PL/Container configurations. </p>
      <codeblock>SELECT * FROM plcontainer_show_config;</codeblock>
      <p>Running the command executes a PL/Container function that displays configuration
        information from the master and segment instances. This is an example of the start and end
        of the view
        output.<codeblock>INFO:  plcontainer: Container 'plc_py_test' configuration
INFO:  plcontainer:     image = 'pivotaldata/plcontainer_python_shared:devel'
INFO:  plcontainer:     memory_mb = '1024'
INFO:  plcontainer:     use container network = 'no'
INFO:  plcontainer:     use container logging  = 'no'
INFO:  plcontainer:     shared directory from host '/usr/local/greenplum-db/./bin/plcontainer_clients' to container '/clientdir'
INFO:  plcontainer:         access = readonly

    ...

INFO:  plcontainer: Container 'plc_r_example' configuration  (seg0 slice3 192.168.180.45:40000 pid=3304)
INFO:  plcontainer:     image = 'pivotaldata/plcontainer_r_without_clients:0.2'  (seg0 slice3 192.168.180.45:40000 pid=3304)
INFO:  plcontainer:     memory_mb = '1024'  (seg0 slice3 192.168.180.45:40000 pid=3304)
INFO:  plcontainer:     use container network = 'no'  (seg0 slice3 192.168.180.45:40000 pid=3304)
INFO:  plcontainer:     use container logging  = 'yes'  (seg0 slice3 192.168.180.45:40000 pid=3304)
INFO:  plcontainer:     shared directory from host '/usr/local/greenplum-db/bin/plcontainer_clients' to container '/clientdir'  (seg0 slice3 192.168.180.45:40000 pid=3304)
INFO:  plcontainer:         access = readonly  (seg0 slice3 192.168.180.45:40000 pid=3304) 
gp_segment_id | plcontainer_show_local_config
---------------+-------------------------------
             0 | ok
            -1 | ok
             1 | ok</codeblock></p>
      <p>The PL/Container function <codeph>plcontainer_containers_summary()</codeph> displays
        information about the currently running Docker
        containers.<codeblock>SELECT * FROM plcontainer_containers_summary();</codeblock></p>
      <p>If a normal (non-superuser) Greenplum Database user runs the function, the function
        displays information only for containers created by the user. If a Greenplum Database
        superuser runs the function, information for all containers created by Greenplum Database
        users is displayed. This is sample output when 2 containers are running.</p>
      <codeblock> SEGMENT_ID |                           CONTAINER_ID                           |   UP_TIME    |  OWNER  | MEMORY_USAGE(KB)
------------+------------------------------------------------------------------+--------------+---------+------------------
 1          | 693a6cb691f1d2881ec0160a44dae2547a0d5b799875d4ec106c09c97da422ea | Up 8 seconds | gpadmin | 12940
 1          | bc9a0c04019c266f6d8269ffe35769d118bfb96ec634549b2b1bd2401ea20158 | Up 2 minutes | gpadmin | 13628
(2 rows)</codeblock>
    </body>
    <topic id="topic9" xml:lang="en">
      <title id="pz215232">Examples</title>
      <body>
        <p>The values in the <codeph># container</codeph> lines of the examples,
            <codeph>plc_python_shared</codeph> and <codeph>plc_r_shared</codeph>, are the
            <codeph>id</codeph> XML elements defined in the <codeph>plcontainer_config.xml</codeph>
          file. The <codeph>id</codeph> element is mapped to the <codeph>image</codeph> element that
          specifies the Docker image to be started. If you configured PL/Container with a different
          ID, change the value of the <codeph># container</codeph> line. For information about
          configuring PL/Container and viewing the configuration settings, see <xref
            href="#topic_sk1_gdq_dw" format="dita"/>.</p>
        <p>This is an example of PL/Python function that runs using the
            <codeph>plc_python_shared</codeph>
          container:<codeblock>CREATE OR REPLACE FUNCTION pylog100() RETURNS double precision AS $$
# container: plc_python_shared
import math
return math.log10(100)
$$ LANGUAGE plcontainer;</codeblock></p>
        <p>This is an example of a similar function using the <codeph>plc_r_shared</codeph>
          container:<codeblock>CREATE OR REPLACE FUNCTION rlog100() RETURNS text AS $$
# container: plc_r_shared
return(log10(100))
$$ LANGUAGE plcontainer;</codeblock></p>
        <p>If the <codeph># container</codeph> line in a UDF specifies an ID that is not in the
          PL/Container configuration file, Greenplum Database returns an error when you try to
          execute the UDF.</p>
      </body>
    </topic>
  </topic>
  <topic id="topic_ehl_r3q_dw">
    <title>About PL/Container Running PL/Python </title>
    <body>
      <p>In the Python language container, the module <codeph>plpy</codeph> is implemented. The
        module contains these methods:</p>
      <ul id="ul_qfd_mdq_dw">
        <li><codeph>plpy.execute(stmt)</codeph> - Executes the query string <codeph>stmt</codeph>
          and returns query result in a list of dictionary objects. To be able to access the result
          fields ensure your query returns named fields.</li>
        <li><codeph>plpy.prepare(stmt[, argtypes])</codeph> - Prepares the execution plan for a
          query. It is called with a query string and a list of parameter types, if you have
          parameter references in the query.</li>
        <li><codeph>plpy.execute(plan[, argtypes])</codeph> - Executes a prepared plan.</li>
        <li><codeph>plpy.debug(msg)</codeph> - Sends a DEBUG2 message to the Greenplum Database
          log.</li>
        <li><codeph>plpy.log(msg)</codeph> - Sends a LOG message to the Greenplum Database log.</li>
        <li><codeph>plpy.info(msg)</codeph> - Sends an INFO message to the Greenplum Database
          log.</li>
        <li><codeph>plpy.notice(msg)</codeph> - Sends a NOTICE message to the Greenplum Database
          log.</li>
        <li><codeph>plpy.warning(msg)</codeph> - Sends a WARNING message to the Greenplum Database
          log.</li>
        <li><codeph>plpy.error(msg)</codeph> - Sends an ERROR message to the Greenplum Database log.
          An ERROR message raised in Greenplum Database causes the query execution process to stop
          and the transaction to rollback.</li>
        <li><codeph>plpy.fatal(msg)</codeph> - Sends a FATAL message to the Greenplum Database log.
          A FATAL message causes Greenplum Database session to be closed and transaction to be
          rolled back.</li>
        <li><codeph>plpy.subtransaction()</codeph> - Manages <codeph>plpy.execute</codeph> calls in
          an explicit subtransaction. See <xref
            href="https://www.postgresql.org/docs/9.4/plpython-subtransaction.html"
            format="html" scope="external">Explicit Subtransactions</xref> in the PostgreSQL
          documentation for additional information about
          <codeph>plpy.subtransaction()</codeph>.</li>
      </ul>
      <p>If an error of level ERROR or FATAL is raised in a nested Python function call, the message
        includes the list of enclosing functions.</p>
      <p>The Python language container supports these string quoting functions that are useful when
        constructing ad-hoc queries. <ul id="ul_jwf_nd2_kfb">
          <li><codeph>plpy.quote_literal(string)</codeph> - Returns the string quoted to be used as
            a string literal in an SQL statement string. Embedded single-quotes and backslashes are
            properly doubled. <codeph>quote_literal()</codeph> returns null on null input (empty
            input). If the argument might be null, <codeph>quote_nullable()</codeph> might be more
            appropriate.</li>
          <li><codeph>plpy.quote_nullable(string)</codeph> - Returns the string quoted to be used as
            a string literal in an SQL statement string. If the argument is null, returns
              <codeph>NULL</codeph>. Embedded single-quotes and backslashes are properly
            doubled.</li>
          <li>
            <codeph>plpy.quote_ident(string)</codeph> - Returns the string quoted to be used as an
            identifier in an SQL statement string. Quotes are added only if necessary (for example,
            if the string contains non-identifier characters or would be case-folded). Embedded
            quotes are properly doubled. </li>
        </ul></p>
      <p>When returning text from a PL/Python function, PL/Container converts a Python unicode
        object to text in the database encoding. If the conversion cannot be performed, an error is
        returned.</p>
      <p>PL/Container does not support this Greenplum Database PL/Python feature:<ul
          id="ul_qzd_1w5_ncb">
          <li> Multi-dimensional arrays.</li>
        </ul></p>
      <p>Also, the Python module has two global dictionary objects that retain the data between
        function calls. They are named GD and SD. GD is used to share the data between all the
        function running within the same container, while SD is used for sharing the data between
        multiple calls of each separate function. Be aware that accessing the data is possible only
        within the same session, when the container process lives on a segment or master. Be aware
        that for idle sessions Greenplum Database terminates segment processes, which means the
        related containers would be shut down and the data from GD and SD lost.</p>
      <p>For information about PL/Python, see <xref href="pl_python.xml#topic1"/>. </p>
      <p>For information about the <codeph>plpy</codeph> methods, see <xref
          href="https://www.postgresql.org/docs/9.4/plpython-database.html" format="html"
          scope="external">https://www.postgresql.org/docs/9.4/plpython-database.htm</xref>.
      </p>
    </body>
  </topic>
  <topic id="topic_lqz_t3q_dw">
    <title>About PL/Container Running PL/R</title>
    <body>
      <p>In the R language container, the module <codeph>pg.spi</codeph> is implemented. The module
        contains these methods:</p>
      <ul id="ul_mqz_t3q_dw">
        <li><codeph>pg.spi.exec(stmt)</codeph> - Executes the query string <codeph>stmt</codeph> and
          returns query result in R data.frame. To be able to access the result fields make sure
          your query returns named fields.</li>
        <li><codeph>pg.spi.prepare(stmt[, argtypes])</codeph> - Prepares the execution plan for a
          query. It is called with a query string and a list of parameter types if you have
          parameter references in the query.</li>
        <li><codeph>pg.spi.execp(plan[, argtypes])</codeph> - Execute a prepared plan.</li>
        <li><codeph>pg.spi.debug(msg)</codeph> - Sends a DEBUG2 message to the Greenplum Database
          log.</li>
        <li><codeph>pg.spi.log(msg)</codeph> - Sends a LOG message to the Greenplum Database
          log.</li>
        <li><codeph>pg.spi.info(msg)</codeph> - Sends an INFO message to the Greenplum Database
          log.</li>
        <li><codeph>pg.spi.notice(msg)</codeph> - Sends a NOTICE message to the Greenplum Database
          log.</li>
        <li><codeph>pg.spi.warning(msg)</codeph> - Sends a WARNING message to the Greenplum Database
          log.</li>
        <li><codeph>pg.spi.error(msg)</codeph> - Sends an ERROR message to the Greenplum Database
          log. An ERROR message raised in Greenplum Database causes the query execution process to
          stop and the transaction to rollback.</li>
        <li><codeph>pg.spi.fatal(msg)</codeph> - Sends a FATAL message to the Greenplum Database
          log. A FATAL message causes Greenplum Database session to be closed and transaction to be
          rolled back.</li>
      </ul>
      <p>PL/Container does not support this PL/R feature:<ul id="ul_wjk_dgb_4cb">
          <li> Multi-dimensional arrays.</li>
        </ul></p>
      <p>For information about PL/R, see <xref href="pl_r.xml#topic1"/>.</p>
      <p>For information about the <codeph>pg.spi</codeph> methods, see <xref
          href="http://www.joeconway.com/plr/doc/plr-spi-rsupport-funcs-normal.html" format="html"
          scope="external"
          >http://www.joeconway.com/plr/doc/plr-spi-rsupport-funcs-normal.html</xref></p>
    </body>
  </topic>
  <topic id="topic_sk1_gdq_dw">
    <title>Configuring PL/Container</title>
    <body>
      <p>The Greenplum Database utility <codeph>plcontainer</codeph> manages the PL/Container
        configuration files in a Greenplum Database system. The utility ensures that the
        configuration files are consistent across the Greenplum Database master and segment
        instances.</p>
      <note type="warning"> Modifying the configuration files on the segment instances without using
        the utility might create different, incompatible configurations on different Greenplum
        Database segments that could cause unexpected behavior. </note>
      <p>Configuration changes that are made with the utility are applied to the XML files on all
        Greenplum Database segments. However, PL/Container configurations of currently running
        sessions use the configuration that existed during session start up. To update the
        PL/Container configuration in a running session, execute this command in the session.</p>
      <codeblock>SELECT * FROM plcontainer_refresh_config;</codeblock>
      <p>Running the command executes a PL/Container function that updates the session configuration
        on the master and segment instances.</p>
    </body>
    <topic id="topic_rw3_52s_dw">
      <title>The plcontainer Utility</title>
      <body>
        <p>The <codeph>plcontainer</codeph> utility installs Docker images and manages the
          PL/Container configuration. The utility consists of two sets of commands.</p>
        <ul id="ul_lzy_xsw_gcb">
          <li><codeph>image-*</codeph> commands manage Docker images on the Greenplum Database
            system hosts. </li>
          <li><codeph>runtime-*</codeph> commands manage the PL/Container configuration file on the
            Greenplum Database instances. You can add Docker image information to the PL/Container
            configuration file including the image name, location, and shared folder information.
            You can also edit the configuration file.</li>
        </ul>
        <p>To configure PL/Container to use a Docker image, you install the Docker image on all the
          Greenplum Database hosts and then add configuration information to the PL/Container
          configuration. </p>
        <p>PL/Container configuration values, such as image names, runtime IDs, and parameter values
          and names are case sensitive.</p>
        <section>
          <title>plcontainer Syntax</title>
          <codeblock><b>plcontainer</b> [<varname>command</varname>] [<b>-h</b> | <b>--help</b>]  [<b>--verbose</b>]</codeblock>
          <p>Where <varname>command</varname> is one of the following.</p>
          <codeblock>  image-add {{<b>-f</b> | <b>--file</b>} <varname>image_file</varname>} | {{<b>-u</b> | <b>--URL</b>} <varname>image_URL</varname>}
  image-delete {<b>-i</b> | <b>--image</b>} <varname>image_name</varname>
  image-list

  runtime-add {<b>-r</b> | <b>--runtime</b>} <varname>runtime_id</varname>
     {<b>-i</b> | <b>--image</b>} <varname>image_name</varname> {<b>-l | --language</b>} {python | r}
     [{<b>-v</b> | <b>--volume</b>} <varname>shared_volume</varname> [{<b>-v</b>| <b>--volume</b>} <varname>shared_volume</varname>...]]
     [{<b>-s</b> | <b>--setting</b>} <varname>param=value</varname> [{<b>-s</b> | <b>--setting</b>} <varname>param=value</varname> ...]]
  runtime-replace {<b>-r</b> | <b>--runtime</b>} <varname>runtime_id</varname>
     {<b>-i</b> | <b>--image</b>} <varname>image_name</varname> <b>-l</b> {r | python}
     [{<b>-v</b> | <b>--volume</b>} <varname>shared_volume</varname> [{<b>-v</b> | <b>--volume</b>} <varname>shared_volume</varname>...]]
     [{<b>-s</b> | <b>--setting</b>} <varname>param=value</varname> [{<b>-s</b> | <b>--setting</b>} <varname>param=value</varname> ...]]
  runtime-show {<b>-r</b> | <b>--runtime</b>} <varname>runtime_id</varname>
  runtime-delete {<b>-r</b> | <b>--runtime</b>} <varname>runtime_id</varname>
  runtime-edit [{<b>-e</b> | <b>--editor</b>} <varname>editor</varname>]
  runtime-backup {<b>-f</b> | <b>--file</b>} <varname>config_file</varname>
  runtime-restore {<b>-f</b> | <b>--file</b>} <varname>config_file</varname>
  runtime-verify</codeblock>
        </section>
        <section>
          <title>plcontainer Commands and Options</title>
        </section>
        <parml>
          <plentry>
            <pt>image-add <varname>location</varname></pt>
            <pd>Install a Docker image on the Greenplum Database hosts. Specify either the location
              of the Docker image file on the host or the URL to the Docker image. These are the
              supported location options.<ul id="ul_ihd_dsv_gcb">
                <li>{<b>-f</b> | <b>--file</b>} <varname>image_file</varname> Specify the tar
                  archive file on the host that contains the Docker image. This example points to an
                  image file in the gpadmin home directory
                    <codeph>/home/gpadmin/test_image.tar.gz</codeph></li>
                <li>{<b>-u</b> | <b>--URL</b>} <varname>image_URL</varname> Specify the URL of the
                  Docker repository and image. This example URL points to a local Docker repository
                    <codeph>192.168.0.1:5000/images/mytest_plc_r:devel</codeph></li>
              </ul></pd>
            <pd>After installing the Docker image, use the <codeph><xref
                  href="#topic_rw3_52s_dw/runtime_add" format="dita">runtime-add</xref></codeph>
              command to configure PL/Container to use the Docker image.</pd>
          </plentry>
          <plentry>
            <pt>image-delete {<b>-i</b> | <b>--image</b>} <varname>image_name</varname></pt>
            <pd>Remove an installed Docker image from all Greenplum Database hosts. Specify the full
              Docker image name including the tag for example
                <codeph>pivotaldata/plcontainer_python_shared:1.0.0</codeph></pd>
          </plentry>
          <plentry>
            <pt>image-list</pt>
            <pd>List the Docker images installed on the host. The command list only the images on
              the local host, not remote hosts. The command lists all installed Docker images,
              including images installed with Docker commands.</pd>
          </plentry>
          <plentry id="runtime_add">
            <pt>runtime-add <varname>options</varname></pt>
            <pd>Add configuration information to the PL/Container configuration file on all
              Greenplum Database hosts. If the specified <varname>runtime_id</varname> exists, the
              utility returns an error and the configuration information is not added. </pd>
            <pd>For information about PL/Container configuration, see <xref href="#topic_ojn_r2s_dw"
                format="dita"/>. </pd>
            <pd>These are the supported options:</pd>
            <pd>
              <parml>
                <plentry>
                  <pt>{-i | --image} <varname>docker-image</varname></pt>
                  <pd>Required. Specify the full Docker image name, including the tag, that is
                    installed on the Greenplum Database hosts. For example
                      <codeph>pivotaldata/plcontainer_python:1.0.0</codeph>. </pd>
                  <pd>The utility returns a warning if the specified Docker image is not
                    installed.</pd>
                  <pd>The <codeph>plcontainer image-list</codeph> command displays installed image
                    information including the name and tag (the Repository and Tag columns).</pd>
                </plentry>
                <plentry>
                  <pt>{-l | --language} python | r</pt>
                  <pd>Required. Specify the PL/Container language type, supported values are
                      <codeph>python</codeph> (PL/Python) and <codeph>r</codeph> (PL/R). When adding
                    configuration information for a new runtime, the utility adds a startup command
                    to the configuration based on the language you specify.</pd>
                  <pd>Startup command for the Python
                    language.<codeblock>/clientdir/pyclient.sh</codeblock></pd>
                  <pd>Startup command for the R
                    language.<codeblock>/clientdir/rclient.sh</codeblock></pd>
                </plentry>
                <plentry>
                  <pt>{<b>-r</b> | <b>--runtime</b>} <varname>runtime_id</varname>
                  </pt>
                  <pd>Required. Add the runtime ID. When adding a <codeph>runtime</codeph> element
                    in the PL/Container configuration file, this is the value of the
                      <codeph>id</codeph> element in the PL/Container configuration file. Maximum
                    length is 63 Bytes.</pd>
                  <pd>You specify the name in the Greenplum Database UDF on the <codeph>#
                      container</codeph> line. See <xref href="#topic9" format="dita"/>.</pd>
                </plentry>
                <plentry>
                  <pt>{<b>-s</b> | <b>--setting</b>}
                      <varname>param</varname>=<varname>value</varname></pt>
                  <pd>Optional. Specify a setting to add to the runtime configuration information.
                    You can specify this option multiple times. The setting applies to the runtime
                    configuration specified by the <varname>runtime_id</varname>. The parameter is
                    the XML attribute of the <codeph><xref href="#topic_ojn_r2s_dw/plc_settings"
                        format="dita">settings</xref></codeph> element in the PL/Container
                    configuration file. These are valid parameters.<ul id="ul_dsz_j4w_gcb">
                      <li><codeph>cpu_share</codeph> - Set the CPU limit for each container in the
                        runtime configuration. The default value is 1024. The value is a relative
                        weighting of CPU usage compared to other containers.</li>
                      <li><codeph>memory_mb</codeph> - Set the memory limit for each container in
                        the runtime configuration. The default value is 1024. The value is an
                        integer that specifies the amount of memory in MB. </li>
                      <li><codeph>resource_group_id</codeph> - Assign the specified resource group
                        to the runtime configuration. The resource group limits the total CPU and
                        memory resource usage for all containers that share this runtime
                        configuration. You must specify the <codeph>groupid</codeph> of the resource
                        group. For information about managing PL/Container resources, see <xref
                          href="#topic_resmgmt" format="dita">About PL/Container Resource
                          Management</xref>.</li>
                      <li><codeph>roles</codeph> - Specify the Greenplum Database roles that are
                        allowed to run a container for the runtime configuration. You can specify a
                        single role name or comma separated lists of role names. The default is no
                        restriction.</li>
                      <li><codeph>use_container_logging</codeph> - Enable or disable Docker logging
                        for the container. The value is either <codeph>yes</codeph> (enable logging)
                        or <codeph>no</codeph> (disable logging, the default). <p>The Greenplum
                          Database server configuration parameter <codeph><xref
                              href="../config_params/guc-list.xml#log_min_messages"
                              >log_min_messages</xref></codeph> controls the log level. The default
                          log level is <codeph>warning</codeph>. For information about PL/Container
                          log information, see <xref href="#plc_notes" format="dita"
                          >Notes</xref>.</p></li>
                    </ul></pd>
                </plentry>
                <plentry>
                  <pt>{-v | --volume} <varname>shared-volume</varname></pt>
                  <pd>Optional. Specify a Docker volume to bind mount. You can specify this option
                    multiple times to define multiple volumes.</pd>
                  <pd>The format for a shared volume:
                        <codeph><varname>host-dir</varname>:<varname>container-dir</varname>:[rw|ro]</codeph>.
                    The information is stored as attributes in the <codeph>shared_directory</codeph>
                    element of the <codeph>runtime</codeph> element in the PL/Container
                    configuration file. <ul id="ul_nms_vvv_gcb">
                      <li><varname>host-dir</varname> - absolute path to a directory on the host
                        system. The Greenplum Database administrator user (gpadmin) must have
                        appropriate access to the directory.</li>
                      <li><varname>container-dir</varname> - absolute path to a directory in the
                        Docker container.</li>
                      <li><codeph>[rw|ro]</codeph> - read-write or read-only access to the host
                        directory from the container. </li>
                    </ul></pd>
                  <pd>When adding configuration information for a new runtime, the utility adds this
                    read-only shared volume information. </pd>
                  <pd>
                    <codeblock><varname>greenplum-home</varname>/bin/plcontainer_clients:/clientdir:ro</codeblock>
                  </pd>
                  <pd>If needed, you can specify other shared directories. The utility returns an
                    error if the specified <varname>container-dir</varname> is the same as the one
                    that is added by the utility, or if you specify multiple shared volumes with the
                    same <varname>container-dir</varname>.
                    <note type="warning">Allowing read-write access to a host directory requires
                      special considerations.<ul id="ul_ibw_gvk_kcb">
                        <li>When specifying read-write access to host directory, ensure that the
                          specified host directory has the correct permissions. </li>
                        <li>When running PL/Container user-defined functions, multiple concurrent
                          Docker containers that are running on a host could change data in the host
                          directory. Ensure that the functions support multiple concurrent access to
                          the data in the host directory.</li>
                      </ul></note></pd>
                </plentry>
              </parml>
            </pd>
          </plentry>
          <plentry>
            <pt>runtime-backup {<b>-f</b> | <b>--file</b>} <varname>config_file</varname></pt>
            <pd>
              <p dir="ltr">Copies the PL/Container configuration file to the specified file on the
                local host. </p>
            </pd>
          </plentry>
          <plentry>
            <pt>runtime-delete {<b>-r</b> | <b>--runtime</b>} <varname>runtime_id</varname></pt>
            <pd>
              <p dir="ltr">Removes runtime configuration information in the PL/Container
                configuration file on all Greenplum Database instances. The utility returns a
                message if the specified <varname>runtime_id</varname> does not exist in the
                file.</p>
            </pd>
          </plentry>
          <plentry>
            <pt>runtime-edit [{<b>-e</b> | <b>--editor</b>} <varname>editor</varname>]</pt>
            <pd>Edit the XML file <codeph>plcontainer_configuration.xml</codeph> with the specified
              editor. The default editor is <codeph>vi</codeph>.<p>Saving the file updates the
                configuration file on all Greenplum Database hosts. If errors exist in the updated
                file, the utility returns an error and does not update the file.</p></pd>
          </plentry>
          <plentry>
            <pt>runtime-replace <varname>options</varname></pt>
            <pd>
              <p dir="ltr">Replaces runtime configuration information in the PL/Container
                configuration file on all Greenplum Database instances. If the
                  <varname>runtime_id</varname> does not exist, the information is added to the
                configuration file. The utility adds a startup command and shared directory to the
                configuration. </p>
              <p dir="ltr">See <codeph><xref href="#topic_rw3_52s_dw/runtime_add" format="dita"
                    >runtime-add</xref></codeph> for command options and information added to the
                configuration.</p>
            </pd>
          </plentry>
          <plentry>
            <pt>runtime-restore {<b>-f</b> | <b>--file</b>} <varname>config_file</varname></pt>
            <pd>
              <p dir="ltr">Replaces information in the PL/Container configuration file
                  <codeph>plcontainer_configuration.xml</codeph> on all Greenplum Database instances
                with the information from the specified file on the local host.</p>
            </pd>
          </plentry>
          <plentry>
            <pt>runtime-show [{<b>-r</b> | <b>--runtime</b>} <varname>runtime_id</varname>]</pt>
            <pd>
              <p dir="ltr">Displays formatted PL/Container runtime configuration information. If a
                  <varname>runtime_id</varname> is not specified, the configuration for all runtime
                IDs are displayed.</p>
            </pd>
          </plentry>
          <plentry>
            <pt>runtime-verify</pt>
            <pd>
              <p dir="ltr">Checks the PL/Container configuration information on the Greenplum
                Database instances with the configuration information on the master. If the utility
                finds inconsistencies, you are prompted to replace the remote copy with the local
                copy. The utility also performs XML validation.</p>
            </pd>
          </plentry>
          <plentry>
            <pt>-h | --help</pt>
            <pd>Display help text. If specified without a command, displays help for all
                <codeph>plcontainer</codeph> commands. If specified with a command, displays help
              for the command.</pd>
          </plentry>
          <plentry>
            <pt>--verbose</pt>
            <pd>Enable verbose logging for the command.</pd>
          </plentry>
        </parml>
        <section>
          <title>Examples</title>
          <p>These are examples of common commands to manage PL/Container:</p>
          <ul id="ul_ijd_xmw_gcb">
            <li>Install a Docker image on all Greenplum Database hosts. This example loads a Docker
              image from a file. The utility displays progress information on the command line as
              the utility installs the Docker image on all the
                hosts.<codeblock>plcontainer image-add -f plc_newr.tar.gz</codeblock><p>After
                installing the Docker image, you add or update a runtime entry in the PL/Container
                configuration file to give PL/Container access to the Docker image to start Docker
                containers.</p></li>
            <li>Add a container entry to the PL/Container configuration file. This example adds
              configuration information for a PL/R runtime, and specifies a shared volume and
              settings for memory and logging.
                <codeblock>plcontainer runtime-add -r runtime2 -i test_image2:0.1 -l r \
  -v /host_dir2/shared2:/container_dir2/shared2:ro \
  -s memory_mb=512 -s use_container_logging=yes</codeblock><p>The
                utility displays progress information on the command line as it adds the runtime
                configuration to the configuration file and distributes the updated configuration to
                all instances.</p></li>
            <li>Show specific runtime with given runtime id in configuration
                file<codeblock>plcontainer runtime-show -r plc_python_shared</codeblock><p>The
                utility displays the configuration information similar to this
                output.<codeblock>PL/Container Runtime Configuration:
---------------------------------------------------------
 Runtime ID: plc_python_shared
 Linked Docker Image: test1:latest
 Runtime Setting(s):
 Shared Directory:
 ---- Shared Directory From HOST '/usr/local/greenplum-db/bin/plcontainer_clients' to Container '/clientdir', access mode is 'ro'
 ---- Shared Directory From HOST '/home/gpadmin/share/' to Container '/opt/share', access mode is 'rw'
---------------------------------------------------------</codeblock></p></li>
            <li>Edit the configuration in an interactive editor of your choice. This example edits
              the configuration file with the vim
                editor.<codeblock>plcontainer runtime-edit -e vim</codeblock><p>When you save the
                file, the utility displays progress information on the command line as it
                distributes the file to the Greenplum Database hosts. </p></li>
            <li>Save the current PL/Container configuration to a file. This example saves the file
              to the local file
              <codeph>/home/gpadmin/saved_plc_config.xml</codeph><codeblock>plcontainer runtime-backup -f /home/gpadmin/saved_plc_config.xml</codeblock></li>
            <li>Overwrite PL/Container configuration file with an XML file. This example replaces
              the information in the configuration file with the information from the file in the
                <codeph>/home/gpadmin</codeph>
              directory.<codeblock>plcontainer runtime-restore -f /home/gpadmin/new_plcontainer_configuration.xml</codeblock>The
              utility displays progress information on the command line as it distributes the
              updated file to the Greenplum Database instances. </li>
          </ul>
        </section>
      </body>
    </topic>
    <topic id="topic_ojn_r2s_dw">
      <title>PL/Container Configuration File</title>
      <body>
        <p>PL/Container maintains a configuration file
            <codeph>plcontainer_configuration.xml</codeph> in the data directory of all Greenplum
          Database segments. The PL/Container configuration file is an XML file. In the XML file,
          the root element <codeph>configuration</codeph> contains one or more
            <codeph>runtime</codeph> elements. You specify the <codeph>id</codeph> of the
            <codeph>runtime</codeph> element in the <codeph># container:</codeph> line of a
          PL/Container function definition. </p>
        <p>In an XML file, names, such as element and attribute names, and values are case
          sensitive.</p>
        <p>This is an example
          file.<codeblock>&lt;?xml version="1.0" ?>
&lt;configuration>
    &lt;runtime>
        &lt;id>plc_python_example1&lt;/id>
        &lt;image>pivotaldata/plcontainer_python_with_clients:0.1&lt;/image>
        &lt;command>./pyclient&lt;/command>
    &lt;/runtime>
    &lt;runtime>
        &lt;id>plc_python_example2&lt;/id>
        &lt;image>pivotaldata/plcontainer_python_without_clients:0.1&lt;/image>
        &lt;command>/clientdir/pyclient.sh&lt;/command>
        &lt;shared_directory access="ro" container="/clientdir" host="/usr/local/greenplum-db/bin/plcontainer_clients"/>
        &lt;setting memory_mb="512"/>
        &lt;setting use_container_logging="yes"/>
        &lt;setting cpu_share="1024"/>
        &lt;setting resource_group_id="16391"/>
    &lt;/runtime>
    &lt;runtime>
        &lt;id>plc_r_example&lt;/id>
        &lt;image>pivotaldata/plcontainer_r_without_clients:0.2&lt;/image>
        &lt;command>/clientdir/rclient.sh&lt;/command>
        &lt;shared_directory access="ro" container="/clientdir" host="/usr/local/greenplum-db/bin/plcontainer_clients"/>
        &lt;setting use_container_logging="yes"/>
        &lt;setting roles="gpadmin,user1"/>
    &lt;/runtime>
    &lt;runtime>
&lt;/configuration></codeblock></p>
        <p>These are the XML elements and attributes in a PL/Container configuration file.</p>
        <parml>
          <plentry>
            <pt>configuration</pt>
            <pd>Root element for the XML file.</pd>
          </plentry>
          <plentry>
            <pt>runtime</pt>
            <pd>One element for each specific container available in the system. These are child
              elements of the <codeph>configuration</codeph> element.</pd>
            <pd>
              <parml>
                <plentry>
                  <pt>id</pt>
                  <pd>Required. The value is used to reference a Docker container from a
                    PL/Container user-defined function. The <codeph>id</codeph> value must be unique
                    in the configuration. The <codeph>id</codeph> must start with a character or
                    digit (a-z, A-Z, or 0-9) and can contain characters, digits, or the characters
                      <codeph>_</codeph> (underscore), <codeph>.</codeph> (period), or
                      <codeph>-</codeph> (dash). Maximum length is 63 Bytes.<p>The
                        <codeph>id</codeph> specifies which Docker image to use when PL/Container
                      creates a Docker container to execute a user-defined function.</p></pd>
                </plentry>
                <plentry>
                  <pt>image</pt>
                  <pd>
                    <p>Required. The value is the full Docker image name, including image tag. The
                      same way you specify them for starting this container in Docker. Configuration
                      allows to have many container objects referencing the same image name, this
                      way in Docker they would be represented by identical containers. </p>
                    <p>For example, you might have two <codeph>runtime</codeph> elements, with
                      different <codeph>id</codeph> elements, <codeph>plc_python_128</codeph> and
                        <codeph>plc_python_256</codeph>, both referencing the Docker image
                        <codeph>pivotaldata/plcontainer_python:1.0.0</codeph>. The first
                        <codeph>runtime</codeph> specifies a 128MB RAM limit and the second one
                      specifies a 256MB limit that is specified by the <codeph>memory_mb</codeph>
                      attribute of a <codeph>setting</codeph> element.</p>
                  </pd>
                </plentry>
                <plentry>
                  <pt>command</pt>
                  <pd>Required. The value is the command to be run inside of container to start the
                    client process inside in the container. When creating a <codeph>runtime</codeph>
                    element, the <codeph>plcontainer</codeph> utility adds a
                      <codeph>command</codeph> element based on the language (the
                      <codeph>-l</codeph> option).</pd>
                  <pd><codeph>command</codeph> element for the python
                    language.<codeblock>&lt;command>/clientdir/pyclient.sh&lt;/command></codeblock></pd>
                  <pd><codeph>command</codeph> element for the R
                    language.<codeblock>&lt;command>/clientdir/rclient.sh&lt;/command></codeblock></pd>
                  <pd>You should modify the value only if you build a custom container and want to
                    implement some additional initialization logic before the container starts.
                    <note>This element cannot be set with the <codeph>plcontainer</codeph> utility.
                      You can update the configuration file with the <codeph>plcontainer
                        runtime-edit</codeph> command.</note></pd>
                </plentry>
                <plentry>
                  <pt>shared_directory</pt>
                  <pd>Optional. This element specifies a shared Docker shared volume for a container
                    with access information. Multiple <codeph>shared_directory</codeph> elements are
                    allowed. Each <codeph>shared_directory</codeph> element specifies a single
                    shared volume. XML attributes for the <codeph>shared_directory</codeph>
                      element:<ul id="ul_x4d_lcs_dw">
                      <li><codeph>host</codeph> - a directory location on the host system.</li>
                      <li><codeph>container</codeph> - a directory location inside of
                        container.</li>
                      <li><codeph>access</codeph> - access level to the host directory, which can be
                        either <codeph>ro</codeph> (read-only) or <codeph>rw</codeph> (read-write).
                      </li>
                    </ul></pd>
                  <pd>When creating a <codeph>runtime</codeph> element, the
                      <codeph>plcontainer</codeph> utility adds a <codeph>shared_directory</codeph>
                    element.<codeblock>&lt;shared_directory access="ro" container="/clientdir" host="/usr/local/greenplum-db/bin/plcontainer_clients"/></codeblock></pd>
                  <pd>For each <codeph>runtime</codeph> element, the <codeph>container</codeph>
                    attribute of the <codeph>shared_directory</codeph> elements must be unique. For
                    example, a <codeph>runtime</codeph> element cannot have two
                      <codeph>shared_directory</codeph> elements with attribute
                      <codeph>container="/clientdir"</codeph>.
                    <note type="warning">Allowing read-write access to a host directory requires
                      special consideration.<ul id="ul_vzb_dvk_kcb">
                        <li>When specifying read-write access to host directory, ensure that the
                          specified host directory has the correct permissions. </li>
                        <li>When running PL/Container user-defined functions, multiple concurrent
                          Docker containers that are running on a host could change data in the host
                          directory. Ensure that the functions support multiple concurrent access to
                          the data in the host directory.</li>
                      </ul></note></pd>
                </plentry>
                <plentry id="plc_settings">
                  <pt>settings</pt>
                  <pd>Optional. This element specifies Docker container configuration information.
                    Each <codeph>setting</codeph> element contains one attribute. The element
                    attribute specifies logging, memory, or networking information. For example,
                    this element enables
                    logging.<codeblock>&lt;setting use_container_logging="yes"/></codeblock></pd>
                  <pd>These are the valid attributes.<parml>
                      <plentry>
                        <pt>cpu_share</pt>
                        <pd>Optional. Specify the CPU usage for each PL/Container container in the
                          runtime. The value of the element is a positive integer. The default value
                          is 1024. The value is a relative weighting of CPU usage compared to other
                          containers. </pd>
                        <pd>For example, a container with a <codeph>cpu_share</codeph> of 2048 is
                          allocated double the CPU slice time compared with container with the
                          default value of 1024.</pd>
                      </plentry>
                      <plentry>
                        <pt>memory_mb="<varname>size</varname>"</pt>
                        <pd>Optional. The value specifies the amount of memory, in MB, that each
                          container is allowed to use. Each container starts with this amount of RAM
                          and twice the amount of swap space. The container memory consumption is
                          limited by the host system <codeph>cgroups</codeph> configuration, which
                          means in case of memory overcommit, the container is terminated by the
                          system.</pd>
                      </plentry>
                      <plentry>
                        <pt>resource_group_id="<varname>rg_groupid</varname>"</pt>
                        <pd>Optional. The value specifies the <codeph>groupid</codeph> of the
                          resource group to assign to the PL/Container runtime. The resource group
                          limits the total CPU and memory resource usage for all running containers
                          that share this runtime configuration. You must specify the
                            <codeph>groupid</codeph> of the resource group. If you do not assign a
                          resource group to a PL/Container runtime configuration, its container
                          instances are limited only by system resources. For information about
                          managing PL/Container resources, see <xref href="#topic_resmgmt"
                            format="dita">About PL/Container Resource Management</xref>.</pd>
                      </plentry>
                      <plentry>
                        <pt>roles="<varname>list_of_roles</varname>"</pt>
                        <pd>Optional. The value is a Greenplum Database role name or a
                          comma-separated list of roles. PL/Container runs a container that uses the
                          PL/Container runtime configuration only for the listed roles. If the
                          attribute is not specified, any Greenplum Database role can run an
                          instance of this container runtime configuration. For example, you create
                          a UDF that specifies the <codeph>plcontainer</codeph> language and
                          identifies a <codeph># container:</codeph> runtime configuration that has
                          the <codeph>roles</codeph> attribute set. When a role (user) runs the UDF,
                          PL/Container checks the list of roles and runs the container only if the
                          role is on the list.</pd>
                      </plentry>
                      <plentry>
                        <pt> use_container_logging="{yes | no}"</pt>
                        <pd>Optional. Enables or disables Docker logging for the container. The
                          attribute value <codeph>yes</codeph> enables logging. The attribute value
                            <codeph>no</codeph> disables logging (the default). </pd>
                        <pd>The Greenplum Database server configuration parameter <codeph><xref
                              href="../config_params/guc-list.xml#log_min_messages"
                              >log_min_messages</xref></codeph> controls the PL/Container log level.
                          The default log level is <codeph>warning</codeph>. For information about
                          PL/Container log information, see <xref href="#plc_notes" format="dita"
                            >Notes</xref>.</pd>
                        <pd>
                          <p>By default, the PL/Container log information is sent to a system
                            service. On Red Hat 7 or CentOS 7 systems, the log information is sent
                            to the <codeph>journald</codeph> service. On Red Hat 6 or CentOS 6
                            systems, the log is sent to the <codeph>syslogd</codeph> service. </p>
                        </pd>
                      </plentry>
                    </parml></pd>
                </plentry>
              </parml>
            </pd>
          </plentry>
        </parml>
      </body>
    </topic>
    <topic id="topic_v3s_qv3_kw">
      <title>Updating the PL/Container Configuration</title>
      <body>
        <p>You can add a <codeph>runtime</codeph> element to the PL/Container configuration file
          with the <codeph>plcontainer runtime-add</codeph> command. The command options specify
          information such as the runtime ID, Docker image, and language. You can use the
            <codeph>plcontainer runtime-replace</codeph> command to update an existing
            <codeph>runtime</codeph> element. The utility updates the configuration file on the
          master and all segment instances.</p>
        <p>The PL/Container configuration file can contain multiple <codeph>runtime</codeph>
          elements that reference the same Docker image specified by the XML element
            <codeph>image</codeph>. In the example configuration file, the <codeph>runtime</codeph>
          elements contain <codeph>id</codeph> elements named <codeph>plc_python_128</codeph> and
            <codeph>plc_python_256</codeph>, both referencing the Docker container
            <codeph>pivotaldata/plcontainer_python:1.0.0</codeph>. The first
            <codeph>runtime</codeph> element is defined with a 128MB RAM limit and the second one
          with a 256MB RAM limit.</p>
        <codeblock>&lt;configuration>
  &lt;runtime>
    &lt;id>plc_python_128&lt;/id>
    &lt;image>pivotaldata/plcontainer_python:1.0.0&lt;/image>
    &lt;command>./client&lt;/command>
    &lt;shared_directory access="ro" container="/clientdir" host="/usr/local/gpdb/bin/plcontainer_clients"/>
    &lt;setting memory_mb="128"/>
  &lt;/runtime>
  &lt;runtime>
    &lt;id>plc_python_256&lt;/id>
    &lt;image>pivotaldata/plcontainer_python:1.0.0&lt;/image>
    &lt;command>./client&lt;/command>
    &lt;shared_directory access="ro" container="/clientdir" host="/usr/local/gpdb/bin/plcontainer_clients"/>
    &lt;setting memory_mb="256"/>
    &lt;setting resource_group_id="16391"/>
  &lt;/runtime>
&lt;configuration></codeblock>
      </body>
    </topic>
    <topic id="plc_notes">
      <title>Notes</title>
      <body>
        <ul id="ul_j4g_vgs_wbb">
          <li>PL/Container does not support the Greenplum Database domain object.</li>
          <li>PL/Container maintains the configuration file
              <codeph>plcontainer_configuration.xml</codeph> in the data directory of all Greenplum
            Database segment instances: master, standby master, primary, and mirror. This query
            lists the Greenplum Database system data
              directories:<codeblock>SELECT hostname, datadir FROM gp_segment_configuration;</codeblock><p>A
              sample PL/Container configuration file is in
                <codeph>$GPHOME/share/postgresql/plcontainer</codeph>. </p></li>
          <li>When Greenplum Database executes a PL/Container UDF, Query Executer (QE) processes
            start Docker containers and reuse them as needed. After a certain amount of idle time, a
            QE process quits and destroys its Docker containers. You can control the amount of idle
            time with the Greenplum Database server configuration parameter <codeph><xref
                href="../config_params/guc-list.xml#gp_vmem_idle_resource_timeout"
                >gp_vmem_idle_resource_timeout</xref></codeph>. Controlling the idle time might help
            with Docker container reuse and avoid the overhead of creating and starting a Docker
            container.
            <note type="warning">Changing <codeph>gp_vmem_idle_resource_timeout</codeph> value,
              might affect performance due to resource issues. The parameter also controls the
              freeing of Greenplum Database resources other than Docker containers.</note></li>
          <li>If a PL/Container Docker container exceeds the maximum allowed memory, it is
            terminated and an out of memory warning is displayed. On Red Hat 6 or CentOS 6 systems
            that are configured with Docker version 1.7.1, the out of memory warning is also
            displayed if the PL/Container Docker container main program (PID 1) is terminated.</li>
          <li>In some cases, when PL/Container is running in a high concurrency environment, the
            Docker daemon hangs with log entries that indicate a memory shortage. This can happen
            even when the system seems to have adequate free memory.<p>The issue seems to be
              triggered by a combination of two factors, the aggressive virtual memory requirement
              of the Go language (<codeph>golang</codeph>) runtime that is used by PL/Container, and
              the Greenplum Database Linux server kernel parameter setting for
                <codeph>overcommit_memory</codeph>. The parameter is set to 2 which does not allow
              memory overcommit. </p><p>A workaround that might help is to increase the amount of
              swap space and increase the Linux server kernel parameter
                <codeph>overcommit_ratio</codeph>. If the issue still occurs after the changes,
              there might be memory shortage. You should check free memory on the system and add
              more RAM if needed. You can also decrease the cluster load.</p></li>
          <li>PL/Container does not limit the Docker base device size, the size of the Docker
            container. In some cases, the Docker daemon controls the base device size. For example,
            if the Docker storage driver is devicemapper, the Docker daemon
              <codeph>--storage-opt</codeph> option flag <codeph>dm.basesize</codeph> controls the
            base device size. The default base device size for devicemapper is 10GB. The Docker
            command <codeph>docker info</codeph> displays Docker system information including the
            storage driver. The base device size is displayed in Docker 1.12 and later. For
            information about Docker storage drivers, see the Docker information <xref
              href="https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-storage-driver"
              format="html" scope="external">Daemon storage-driver</xref>. <p>When setting the
              Docker base device size, the size must be set on all Greenplum Database
            hosts.</p></li>
          <li>When PL/Container logging is enabled, you can set the log level with the Greenplum
            Database server configuration parameter <codeph><xref
                href="../config_params/guc-list.xml#log_min_messages"
              >log_min_messages</xref></codeph>. The default log level is <codeph>warning</codeph>.
            The parameter controls the PL/Container log level and also controls the Greenplum
            Database log level.<ul id="ul_knd_jhl_mcb">
              <li>PL/Container logging is enabled or disabled for each runtime ID with the
                  <codeph>setting</codeph> attribute <codeph>use_container_logging</codeph>. The
                default is no logging. </li>
              <li>The PL/Container log information is the information from the UDF that is run in
                the Docker container. By default, the PL/Container log information is sent to a
                system service. On Red Hat 7 or CentOS 7 systems, the log information is sent to the
                  <codeph>journald</codeph> service. On Red Hat 6 or CentOS 6 systems, the log
                information is sent to the <codeph>syslogd</codeph> service. The PL/Container log
                information is sent to the log file of the host were the Docker container runs. </li>
              <li>The Greenplum Database log information is sent to log file on the Greenplum
                Database master.</li>
            </ul><p>When testing or troubleshooting a PL/Container UDF, you can change the Greenplum
              Database log level with the <codeph>SET</codeph> command. You can set the parameter in
              the session before you run your PL/Container UDF. This example sets the log level to
                <codeph>debug1</codeph>.</p><codeblock>SET log_min_messages='debug1' ;</codeblock>
            <note>The parameter <codeph>log_min_messages</codeph> controls both the Greenplum
              Database and PL/Container logging, increasing the log level might affect Greenplum
              Database performance even if a PL/Container UDF is not running.</note></li>
        </ul>
      </body>
    </topic>
  </topic>
  <topic id="topic_ydt_rtc_rbb">
    <title>Installing Docker</title>
    <body>
      <p>To use PL/Container, Docker must be installed on all Greenplum Database host systems. The
        these instructions show how to set up the Docker service on CentOS 7. Installing on RHEL 7
        is a similar process.</p>
      <p>Before performing the Docker installation ensure these requirements are met.<ul
          id="ul_ygx_ms2_rbb">
          <li>The CentOS <codeph>extras</codeph> repository is accessible.</li>
          <li>The user has sudo privileges or is root.</li>
        </ul></p>
      <p>See also the Docker site installation instructions for CentOS <xref
          href="https://docs.docker.com/engine/installation/linux/centos/" format="html"
          scope="external">https://docs.docker.com/engine/installation/linux/centos/</xref>. For a
        list of Docker commands, see the Docker engine Run Reference <xref
          href="https://docs.docker.com/engine/reference/run/" format="html" scope="external"
          >https://docs.docker.com/engine/reference/run/</xref>.</p>
      <section>
        <title>Installing Docker on CentOS 7</title>
        <p>These steps install the docker package and start the docker service as a user with sudo
          privileges.</p>
        <ol id="ol_e4g_sb2_rbb">
          <li>Install dependencies required for
            Docker<codeblock>sudo yum install -y yum-utils device-mapper-persistent-data lvm2</codeblock></li>
          <li>Add the Docker
            repo<codeblock>sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo</codeblock></li>
          <li>Update yum cache<codeblock>sudo yum makecache fast</codeblock></li>
          <li>Install Docker<codeblock>sudo yum -y install docker-ce</codeblock></li>
          <li>Start Docker daemon.<codeblock>sudo systemctl start docker</codeblock></li>
          <li>To give access to the Docker daemon and docker commands, assign the Greenplum Database
            administrator (gpadmin) to the group
            <codeph>docker</codeph>.<codeblock>sudo usermod -aG docker gpadmin</codeblock></li>
          <li>Exit the session and login again to update the privileges.</li>
          <li>Run a Docker command to test the Docker installation. This command lists the currently
            running Docker containers. <codeblock>docker ps</codeblock></li>
        </ol>
        <p>This command configures Docker to start when the host system
          starts.<codeblock>sudo systemctl start docker.service</codeblock></p>
        <p>After you have installed Docker on all Greenplum Database hosts, restart the Greenplum
          Database system to give Greenplum Database access to Docker.
          <codeblock>gpstop -ra</codeblock></p>
      </section>
      <section>
        <title>Installing Docker on CentOS 6</title>
        <p>These steps install the Docker package and start the docker service as a user with sudo
          privileges.</p>
        <ol id="ol_lrx_zb2_rbb">
          <li>Install EPEL package<codeblock>sudo yum -y install epel-release</codeblock></li>
          <li>Install Docker<codeblock>sudo yum -y install docker-io</codeblock></li>
          <li>Create a docker group<codeblock>sudo groupadd docker</codeblock></li>
          <li>Start Docker<codeblock>sudo service docker start</codeblock></li>
          <li>To give access to the Docker daemon and docker commands, assign the Greenplum Database
            administrator (gpadmin) to the group
            <codeph>docker</codeph>.<codeblock>sudo usermod -aG docker gpadmin</codeblock></li>
          <li>Exit the session and login again to update the privileges.</li>
          <li>Run a Docker command to test the Docker installation. This command lists the currently
            running Docker containers. <codeblock>docker ps</codeblock></li>
        </ol>
        <p>This command configures Docker to start when the host system
          starts.<codeblock>sudo chkconfig docker on</codeblock></p>
        <p>After you have installed Docker on all Greenplum Database hosts, restart the Greenplum
          Database system to give Greenplum Database access to Docker.
          <codeblock>gpstop -ra</codeblock></p>
      </section>
    </body>
  </topic>
  <topic xml:lang="en" id="topic_kds_plk_rbb">
    <title>References</title>
    <body>
      <p>Docker home page <xref href="https://www.docker.com/" format="html" scope="external"
          >https://www.docker.com/</xref></p>
      <p>Docker command line interface <xref
          href="https://docs.docker.com/engine/reference/commandline/cli/" format="html"
          scope="external">https://docs.docker.com/engine/reference/commandline/cli/</xref></p>
      <p>Dockerfile reference <xref href="https://docs.docker.com/engine/reference/builder/"
          format="html" scope="external"
        >https://docs.docker.com/engine/reference/builder/</xref></p>
      <p>Installing Docker on Linux systems <xref
          href="https://docs.docker.com/engine/installation/linux/centos/" format="html"
          scope="external">https://docs.docker.com/engine/installation/linux/centos/</xref></p>
      <p>Control and configure Docker with systemd <xref
          href="https://docs.docker.com/engine/admin/systemd/" format="html" scope="external"
          >https://docs.docker.com/engine/admin/systemd/</xref></p>
    </body>
  </topic>
</topic>
