<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_hhc_z5w_r4">
  <title>Memory and Resource Management with Resource Queues</title>
  <shortdesc>Avoid memory errors and manage Greenplum Database resources.</shortdesc>
  <body>
    <note>Resource groups are a newer resource management scheme that enforces memory, CPU, and concurrent transaction limits in Greenplum Database. The <xref href="../admin_guide/wlmgmt.xml" format="dita" type="topic" scope="peer" >Managing Resources</xref> topic provides a comparison of the resource queue and the resource group management schemes. Refer to <xref href="../admin_guide/workload_mgmt_resgroups.xml" format="dita" type="topic" scope="peer">Using Resource Groups</xref> for configuration and usage information for this resource management scheme.</note>
    <p>Memory management has a significant impact on performance in a Greenplum Database cluster.
      The default settings are suitable for most environments. Do not change the default settings
      until you understand the memory characteristics and usage on your system. </p>
    <ul id="ul_agm_sbl_zt">
      <li>
        <xref href="#topic_hhc_z5w_r4/section_jqw_qbl_zt" format="dita"/>
      </li>
      <li>
        <xref href="#topic_hhc_z5w_r4/section113x" format="dita"/>
      </li>
      <li>
        <xref href="#topic_hhc_z5w_r4/section_r52_rbl_zt" format="dita"/>
      </li>
      <li>
        <xref href="#topic_hhc_z5w_r4/configuring_rq" format="dita"/>
      </li>
    </ul>
    <section id="section_jqw_qbl_zt">
      <title>Resolving Out of Memory Errors</title>
      <p>An out of memory error message identifies the Greenplum segment, host, and process that
        experienced the out of memory error. For
        example:<codeblock>Out of memory (seg27 host.example.com pid=47093)
VM Protect failed to allocate 4096 bytes, 0 MB available</codeblock></p>
      <p>Some common causes of out-of-memory conditions in Greenplum Database are: <ul
          id="ul_p2w_xhb_3r">
          <li>Insufficient system memory (RAM) available on the cluster</li>
          <li>Improperly configured memory parameters</li>
          <li>Data skew at the segment level</li>
          <li>Operational skew at the query level</li>
        </ul></p>
      <p>Following are possible solutions to out of memory conditions:</p>
      <ul id="ul_yrd_c1l_zt">
        <li>Tune the query to require less memory</li>
        <li>Reduce query concurrency using a resource queue</li>
        <li>Validate the <codeph>gp_vmem_protect_limit</codeph> configuration parameter at the
          database level. See calculations for the maximum safe setting in <xref
            href="#topic_hhc_z5w_r4/section_r52_rbl_zt" format="dita"/>.</li>
        <li> Set the memory quota on a resource queue to limit the memory used by queries executed
          within the resource queue</li>
        <li>Use a session setting to reduce the <codeph>statement_mem</codeph> used by specific
          queries</li>
        <li>Decrease <codeph>statement_mem</codeph> at the database level</li>
        <li>Decrease the number of segments per host in the Greenplum Database cluster. This
          solution requires a re-initializing Greenplum Database and reloading your data.</li>
        <li>Increase memory on the host, if possible. (Additional hardware may be required.)</li>
      </ul>
      <p>Adding segment hosts to the cluster will not in itself alleviate out of memory problems.
        The memory used by each query is determined by the <codeph>statement_mem</codeph> parameter
        and it is set when the query is invoked. However, if adding more hosts allows decreasing the
        number of segments per host, then the amount of memory allocated in
          <codeph>gp_vmem_protect_limit</codeph> can be raised.</p>
    </section>
    <section id="section113x" xml:lang="en">
      <title>Low Memory Queries</title>
        <p>A low <codeph>statement_mem</codeph> setting (for example, in the
          1-3MB range) has been shown
          to increase the performance of queries with low memory requirements. Use
          the <codeph>statement_mem</codeph> server configuration parameter to
          override the setting on a per-query basis. For example:
          <codeblock>SET statement_mem='2MB';</codeblock></p>
    </section>
    <section id="section_r52_rbl_zt">
      <title>Configuring Memory for Greenplum Database</title>
      <p>Most out of memory conditions can be avoided if memory is thoughtfully managed. </p>
      <p>It is not always possible to increase system memory, but you can prevent out-of-memory
        conditions by configuring memory use correctly and setting up resource queues to manage
        expected workloads. </p>
      <p>It is important to include memory requirements for mirror segments that become primary
        segments during a failure to ensure that the cluster can continue when primary segments or
        segment hosts fail. </p>
      <p>The following are recommended operating system and Greenplum Database memory settings:</p>
      <ul id="ul_xv2_phn_1s">
        <li>Do not configure the OS to use huge pages. </li>
        <li>
          <b>vm.overcommit_memory</b>
          <p>This is a Linux kernel parameter, set in <codeph>/etc/sysctl.conf</codeph> and it
            should always be set to 2. It determines the method the OS uses for determining how much
            memory can be allocated to processes and 2 is the only safe setting for Greenplum
            Database. Please review the sysctl parameters in the <xref
              href="../install_guide/prep_os.xml#topic3__sysctl_file" scope="peer" format="dita"
              >Greenplum Database Installation Guide</xref>.</p>
        </li>
        <li>
          <b>vm.overcommit_ratio</b>
          <p>This is a Linux kernel parameter, set in <xref
              href="../install_guide/prep_os.xml#topic3__sysctl_file" format="dita" scope="peer"
                ><codeph>/etc/sysctl.conf</codeph></xref>. It is the percentage of RAM that is used
            for application processes. The remainder is reserved for the operating system. The
            default on Red Hat is 50. </p><p>Setting <codeph>vm.overcommit_ratio</codeph> too high
            may result in not enough memory being reserved for the operating system, which can
            result in segment host failure or database failure. Setting the value too low reduces
            the amount of concurrency and query complexity that can be run by reducing the amount of
            memory available to Greenplum Database. When increasing the setting it is important to
            remember to always reserve some memory for operating system activities. </p><p>See <xref
              href="../admin_guide/wlmgmt_intro.xml" scope="peer" format="dita">Greenplum Database
              Memory Overview</xref> for instructions to calculate a value for
              <codeph>vm.overcommit_ratio</codeph>.</p></li>
        <li>
          <b>gp_vmem_protect_limit</b>
          <p>Use <codeph>gp_vmem_protect_limit</codeph> to set the maximum memory that the instance
            can allocate for <i>all</i> work being done in each segment database. Never set this
            value larger than the physical RAM on the system. If
              <codeph>gp_vmem_protect_limit</codeph> is too high, it is possible for memory to
            become exhausted on the system and normal operations may fail, causing segment failures.
            If <codeph>gp_vmem_protect_limit</codeph> is set to a safe lower value, true memory
            exhaustion on the system is prevented; queries may fail for hitting the limit, but
            system disruption and segment failures are avoided, which is the desired behavior.
            </p><p>See <xref href="sysconfig.xml#topic_dt3_fkv_r4/segment_mem_config"/> for
            instructions to calculate a safe value for <codeph>gp_vmem_protect_limit</codeph>.</p>
        </li>
        <li>
          <b>runaway_detector_activation_percent</b>
          <p>Runaway Query Termination, introduced in Greenplum Database 4.3.4, prevents out of
            memory conditions. The <codeph>runaway_detector_activation_percent</codeph> system
            parameter controls the percentage of <codeph>gp_vmem_protect_limit</codeph> memory
            utilized that triggers termination of queries. It is set on by default at 90%. If the
            percentage of <codeph>gp_vmem_protect_limit</codeph> memory that is utilized for a
            segment exceeds the specified value, Greenplum Database terminates queries based on
            memory usage, beginning with the query consuming the largest amount of memory. Queries
            are terminated until the utilized percentage of <codeph> gp_vmem_protect_limit</codeph>
            is below the specified percentage.</p>
        </li>
        <li>
          <b>statement_mem</b>
          <p>Use <codeph>statement_mem</codeph> to allocate memory used for a query per segment
            database. If additional memory is required it will spill to disk. Set the optimal value
            for <codeph>statement_mem</codeph> as follows:
            <codeblock>(vmprotect * .9) / max_expected_concurrent_queries</codeblock></p><p>The
            default value of <codeph>statement_mem</codeph> is 125MB. For example, on a system that 
            is configured with 8 segments per host, a query uses 1GB of memory on each segment server 
            (8 segments â¨‰ 125MB) with the default <codeph>statement_mem</codeph> setting. Set
              <codeph>statement_mem</codeph> at the session level for specific queries that require
            additional memory to complete. This setting works well to manage query memory on
            clusters with low concurrency. For clusters with high concurrency also use resource
            queues to provide additional control on what and how much is running on the system. </p>
        </li>
        <li>
          <b>gp_workfile_limit_files_per_query</b>
          <p>Set <codeph>gp_workfile_limit_files_per_query</codeph> to limit the maximum number of
            temporary spill files (workfiles) allowed per query. Spill files are created when a
            query requires more memory than it is allocated. When the limit is exceeded the query is
            terminated. The default is zero, which allows an unlimited number of spill files and may
            fill up the file system. </p>
        </li>
        <li>
          <p>
            <b>gp_workfile_compression</b>
          </p>
          <p>If there are numerous spill files then set
              <codeph>gp_workfile_compression</codeph> to compress the spill files.
            Compressing spill files may help to avoid overloading the disk subsystem with IO
            operations. </p>
        </li>
      </ul>
    </section>
    <section id="configuring_rq">
      <title>Configuring Resource Queues</title>
      <p>Greenplum Database resource queues provide a powerful mechanism for managing the workload
        of the cluster. Queues can be used to limit both the numbers of active queries and the
        amount of memory that can be used by queries in the queue. When a query is submitted to
        Greenplum Database, it is added to a resource queue, which determines if the query should be
        accepted and when the resources are available to execute it. </p>
      <ul id="ul_svy_b3n_1s">
        <li>
          <p>Associate all roles with an administrator-defined resource queue.</p>
          <p>Each login user (role) is associated with a single resource queue; any query the user
            submits is handled by the associated resource queue. If a queue is not explicitly
            assigned the user's queries are handed by the default queue,
            <codeph>pg_default</codeph>. </p>
        </li>
        <li>
          <p>Do not run queries with the gpadmin role or other superuser roles. </p>
          <p>Superusers are exempt from resource queue limits, therefore superuser queries always
            run regardless of the limits set on their assigned queue.</p>
        </li>
        <li>
          <p>Use the <codeph>ACTIVE_STATEMENTS</codeph> resource queue parameter to limit the number
            of active queries that members of a particular queue can run concurrently. </p>
        </li>
        <li>
          <p>Use the <codeph>MEMORY_LIMIT</codeph> parameter to control the total amount of memory
            that queries running through the queue can utilize. By combining the
              <codeph>ACTIVE_STATEMENTS</codeph> and <codeph>MEMORY_LIMIT</codeph> attributes an
            administrator can fully control the activity emitted from a given resource queue. </p>
          <p>The allocation works as follows: Suppose a resource queue,
              <codeph>sample_queue</codeph>, has <codeph>ACTIVE_STATEMENTS</codeph> set to 10 and
              <codeph>MEMORY_LIMIT</codeph> set to 2000MB. This limits the queue to approximately 2
            gigabytes of memory per segment. For a cluster with 8 segments per server, the total
            usage per server is 16 GB for <codeph>sample_queue</codeph> (2GB * 8 segments/server).
            If a segment server has 64GB of RAM, there could be no more than four of this type of
            resource queue on the system before there is a chance of running out of memory (4 queues
            * 16GB per queue). </p>
          <p>Note that by using <codeph>STATEMENT_MEM</codeph>, individual queries running in the
            queue can allocate more than their "share" of memory, thus reducing the memory available
            for other queries in the queue.</p>
        </li>
        <li>
          <p>Resource queue priorities can be used to align workloads with desired outcomes. Queues
            with <codeph>MAX</codeph> priority throttle activity in all other queues until the
              <codeph>MAX</codeph> queue completes running all queries.</p>
        </li>
        <li>
          <p>Alter resource queues dynamically to match the real requirements of the queue for the
            workload and time of day. You can script an operational flow that changes based on the
            time of day and type of usage of the system and add <codeph>crontab</codeph> entries to
            execute the scripts.</p>
        </li>
        <li>
          <p>Use gptoolkit to view resource queue usage and to understand how the queues are
            working.</p>
        </li>
      </ul>
    </section>
  </body>
</topic>
