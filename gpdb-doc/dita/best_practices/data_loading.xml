<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_fj5_b1x_r4">
  <title>Loading Data</title>
  <shortdesc>Description of the different ways to add data to Greenplum Database.</shortdesc>
  <topic id="topic_tmf_t2t_bp">
    <title>INSERT Statement with Column Values</title>
    <body>
      <p>A singleton <codeph>INSERT</codeph> statement with values adds a single row to a table. The
        row flows through the master and is distributed to a segment. This is the slowest method and
        is not suitable for loading large amounts of data.</p>
    </body>
  </topic>
  <topic id="topic_oqb_y2t_bp">
    <title>COPY Statement</title>
    <body>
      <p>The PostgreSQL <codeph>COPY</codeph> statement copies data from an external file into a
        database table. It can insert multiple rows more efficiently than an <codeph>INSERT</codeph>
        statement, but the rows are still passed through the master. All of the data is copied in
        one command; it is not a parallel process.</p>
      <p>Data input to the <codeph>COPY</codeph> command is from a file or the standard input. For
        example: <codeblock>COPY table FROM '/data/mydata.csv' WITH CSV HEADER;</codeblock>Use
          <codeph>COPY</codeph> to add relatively small sets of data, for example dimension tables
        with up to ten thousand rows, or one-time data loads. </p>
      <p>Use <codeph>COPY</codeph> when scripting a process that loads small amounts of data, less
        than 10 thousand rows. </p>
      <p>Since COPY is a single command, there is no need to disable autocommit when you use this
        method to populate a table.</p>
      <p>You can run multiple concurrent <codeph>COPY</codeph> commands to improve performance. </p>
    </body>
  </topic>
  <topic id="topic_kgp_z2t_bp">
    <title>External Tables</title>
    <body>
      <p>External tables provide access to data in sources outside of Greenplum Database. They can
        be accessed with <codeph>SELECT</codeph> statements and are commonly used with the Extract,
        Load, Transform (ELT) pattern, a variant of the Extract, Transform, Load (ETL) pattern that
        takes advantage of Greenplum Database's fast parallel data loading capability.</p>
      <p>With ETL, data is extracted from its source, transformed outside of the database using
        external transformation tools, such as Informatica or Datastage, and then loaded into the
        database.</p>
      <p>With ELT, Greenplum external tables provide access to data in external sources, which could
        be read-only files (for example, text, CSV, or XML files), Web servers, Hadoop file systems,
        executable OS programs, or the Greenplum <codeph>gpfdist</codeph> file server, described in
        the next section. External tables support SQL operations such as select, sort, and join so
        the data can be loaded and transformed simultaneously, or loaded into a <i>load table</i>
        and transformed in the database into target tables. </p>
      <p>The external table is defined with a <codeph>CREATE EXTERNAL TABLE</codeph> statement,
        which has a <codeph>LOCATION</codeph> clause to define the location of the data and a
          <codeph>FORMAT</codeph> clause to define the formatting of the source data so that the
        system can parse the input data. Files use the <codeph>file://</codeph> protocol, and must
        reside on a segment host in a location accessible by the Greenplum superuser. The data can
        be spread out among the segment hosts with no more than one file per primary segment on each
        host. The number of files listed in the <codeph>LOCATION</codeph> clause is the number of
        segments that will read the external table in parallel. </p>
    </body>
  </topic>
  <topic id="topic_x2l_bft_bp">
    <title>External Tables with Gpfdist</title>
    <body>
      <p>The fastest way to load large fact tables is to use external tables with
          <codeph>gpdist</codeph>. <codeph>gpfdist</codeph> is a file server program using an HTTP
        protocol that serves external data files to Greenplum Database segments in parallel. A
          <codeph>gpfdist</codeph> instance can serve 200 MB/second and many
          <codeph>gpfdist</codeph> processes can run simultaneously, each serving up a portion of
        the data to be loaded. When you begin the load using a statement such as <codeph>INSERT INTO
          &lt;table> SELECT * FROM &lt;external_table></codeph>, the <codeph>INSERT</codeph>
        statement is parsed by the master and distributed to the primary segments. The segments
        connect to the <codeph>gpfdist</codeph> servers and retrieve the data in parallel, parse and
        validate the data, calculate a hash from the distribution key data and, based on the hash
        key, send the row to its destination segment. By default, each <codeph>gpfdist</codeph>
        instance will accept up to 64 connections from segments. With many segments and
          <codeph>gpfdist</codeph> servers participating in the load, data can be loaded at very
        high rates. </p>
      <p>Primary segments access external files in parallel when using <codeph>gpfdist</codeph> up
        to the value of <codeph>gp_external_max_segments</codeph>. When optimizing
          <codeph>gpfdist</codeph> performance, maximize the parallelism as the number of segments
        increase. Spread the data evenly across as many ETL nodes as possible. Split very large data
        files into equal parts and spread the data across as many file systems as possible. </p>
      <p>Run two <codeph>gpfdist</codeph> instances per file system. <codeph>gpfdist</codeph> tends
        to be CPU bound on the segment nodes when loading. But if, for example, there are eight
        racks of segment nodes, there is lot of available CPU on the segments to drive more
          <codeph>gpfdist</codeph> processes. Run <codeph>gpfdist</codeph> on as many interfaces as
        possible. Be aware of bonded NICs and be sure to start enough <codeph>gpfdist</codeph>
        instances to work them. </p>
      <p>It is important to keep the work even across all these resources. The load is as fast as
        the slowest node. Skew in the load file layout will cause the overall load to bottleneck on
        that resource.</p>
      <p>The <codeph>gp_external_max_segs</codeph> configuration parameter controls the number of
        segments each <codeph>gpfdist</codeph> process serves. The default is 64. You can set a
        different value in the <codeph>postgresql.conf</codeph> configuration file on the master.
        Always keep <codeph>gp_external_max_segs</codeph> and the number of <codeph>gpfdist</codeph>
        processes an even factor; that is, the <codeph>gp_external_max_segs</codeph> value should be
        a multiple of the number of <codeph>gpfdist</codeph> processes. For example, if there are 12
        segments and 4 <codeph>gpfdist</codeph> processes, the planner round robins the segment
        connections as follows:
        <screen>Segment 1  - gpfdist 1 
Segment 2  - gpfdist 2 
Segment 3  - gpfdist 3 
Segment 4  - gpfdist 4 
Segment 5  - gpfdist 1 
Segment 6  - gpfdist 2 
Segment 7  - gpfdist 3 
Segment 8  - gpfdist 4 
Segment 9  - gpfdist 1 
Segment 10 - gpfdist 2 
Segment 11 - gpfdist 3 
Segment 12 - gpfdist 4</screen></p>
      <p>Drop indexes before loading into existing tables and re-create the index after loading.
        Creating an index on pre-existing data is faster than updating it incrementally as each row
        is loaded. </p>
      <p>Run <codeph>ANALYZE</codeph> on the table after loading. Disable automatic statistics
        collection during loading by setting <codeph>gp_autostats_mode</codeph> to
          <codeph>NONE</codeph>. Run <codeph>VACUUM</codeph> after load errors to recover space. </p>
      <p>Performing small, high frequency data loads into heavily partitioned column-oriented tables
        can have a high impact on the system because of the number of physical files accessed per
        time interval.</p>
    </body>
  </topic>
  <topic id="topic_xyt_cft_bp">
    <title>Gpload</title>
    <body>
      <p><codeph>gpload</codeph> is a data loading utility that acts as an interface to the
        Greenplum external table parallel loading feature. </p>
      <p>Beware of using <codeph>gpload</codeph> as it can cause catalog bloat by creating and
        dropping external tables. Use <codeph>gpfdist</codeph> instead, since it provides the best
        performance. </p>
      <p><codeph>gpload</codeph> executes a load using a specification defined in a YAML-formatted
        control file. It performs the following operations:<ul id="ul_zrl_dnp_y4">
          <li>Invokes <codeph>gpfdist</codeph> processes</li>
          <li>Creates a temporary external table definition based on the source data defined</li>
          <li>Executes an <codeph>INSERT</codeph>, <codeph>UPDATE</codeph>, or
              <codeph>MERGE</codeph> operation to load the source data into the target table in the
            database</li>
          <li>Drops the temporary external table</li>
          <li>Cleans up <codeph>gpfdist</codeph> processes</li>
        </ul></p>
      <p>The load is accomplished in a single transaction. </p>
    </body>
  </topic>
  <topic id="topic_ryr_2ft_bp">
    <title>Best Practices</title>
    <body>
      <ul id="ul_kvl_jvs_x4">
        <li>Drop any indexes on an existing table before loading data and recreate the indexes after
          loading. Newly creating an index is faster than updating an index incrementally as each
          row is loaded.</li>
        <li>Disable automatic statistics collection during loading by setting the
            <codeph>gp_autostats_mode</codeph> configuration parameter to
          <codeph>NONE</codeph>.</li>
        <li>External tables are not intended for frequent or ad hoc access.</li>
        <li>External tables have no statistics to inform the optimizer. You can set rough estimates
          for the number of rows and disk pages for the external table in the
            <codeph>pg_class</codeph> system catalog with a statement like the
          following:<codeblock>UPDATE pg_class SET reltuples=400000, relpages=400
WHERE relname='myexttable';</codeblock></li>
        <li>When using <codeph>gpfdist</codeph>, maximize network bandwidth by running one
            <codeph>gpfdist</codeph> instance for each NIC on the ETL server. Divide the source data
          evenly between the <codeph>gpfdist</codeph> instances.</li>
        <li>When using <codeph>gpload</codeph>, run as many simultaneous <codeph>gpload</codeph>
          instances as resources allow. Take advantage of the CPU, memory, and networking resources
          available to increase the amount of data that can be transferred from ETL servers to the
          Greenplum Database.</li>
        <li>Use the <codeph>SEGMENT REJECT LIMIT</codeph> clause of the <codeph>COPY</codeph>
          statement to set a limit for the number or percentage of rows that can have errors before
          the <codeph>COPY FROM</codeph> command is aborted. The reject limit is per segment; when
          any one segment exceeds the limit, the command is aborted and no rows are added. Use the
            <codeph>LOG ERRORS</codeph> clause to save error rows. If a row has errors in the
          formatting&#8212;for example missing or extra values, or incorrect data
          types&#8212;Greenplum Database stores the error information and row internally. Use the
            <codeph>gp_read_error_log()</codeph> built-in SQL function to access this stored
          information.</li>
        <li>If the load has errors, run <codeph>VACUUM</codeph> on the table to recover space.</li>
        <li>After you load data into a table, run <codeph>VACUUM</codeph> on heap tables, including
          system catalogs, and <codeph>ANALYZE</codeph> on all tables. It is not necessary to run
            <codeph>VACUUM</codeph> on append-optimized tables. If the table is partitioned, you can
          vacuum and analyze just the partitions affected by the data load. These steps clean up any
          rows from aborted loads, deletes, or updates and update statistics for the table.</li>
        <li>Recheck for segment skew in the table after loading a large amount of data. You can use
          a query like the following to check for
          skew:<codeblock outputclass="language-sql">SELECT gp_segment_id, count(*) 
FROM <i>schema.table</i> 
GROUP BY gp_segment_id ORDER BY 2;</codeblock></li>
        <li>By default, <codeph>gpfdist</codeph> assumes a maximum record size of 32K. To load data
          records larger than 32K, you must increase the maximum row size parameter by specifying
          the <codeph>-m &lt;<i>bytes</i>></codeph> option on the <codeph>gpfdist</codeph> command
          line. If you use <codeph>gpload</codeph>, set the <codeph>MAX_LINE_LENGTH</codeph>
          parameter in the <codeph>gpload</codeph> control file.<note>Integrations with Informatica
            Power Exchange are currently limited to the default 32K record length.</note></li>
      </ul>
      <section>
        <title>Additional Information</title>
        <p>See the <i>Greenplum Database Reference Guide</i> for detailed instructions for loading
          data using <codeph>gpfdist</codeph> and <codeph>gpload</codeph>.</p>
      </section>
    </body>
  </topic>
</topic>
