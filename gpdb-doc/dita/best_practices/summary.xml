<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_kqw_prc_3r">
  <title>Best Practices Summary</title>
  <shortdesc>A summary of best practices for Greenplum Database.</shortdesc>
  <body>
    <section>
      <title>Data Model</title>
    </section>
    <section>Greenplum Database is an analytical MPP shared-nothing database. This model is
      significantly different from a highly normalized/transactional SMP database. Because of this,
      the following best practices are recommended.<ul id="ul_knm_src_3r">
        <li>Greenplum Database performs best with a denormalized schema design suited for MPP
          analytical processing for example, Star or Snowflake schema, with large fact tables and
          smaller dimension tables. </li>
        <li>Use the same data types for columns used in joins between tables.</li>
      </ul><p>See <xref href="schema.xml#schema_design"/>.</p></section>
    <section>
      <title>Heap vs. Append-Optimized Storage</title>
      <ul id="ul_ypm_src_3r">
        <li>Use heap storage for tables and partitions that will receive iterative batch and
          singleton <codeph>UPDATE</codeph>, <codeph>DELETE</codeph>, and <codeph>INSERT</codeph>
          operations. </li>
        <li>Use heap storage for tables and partitions that will receive concurrent
            <codeph>UPDATE</codeph>, <codeph>DELETE</codeph>, and <codeph>INSERT</codeph>
          operations.</li>
        <li>Use append-optimized storage for tables and partitions that are updated infrequently
          after the initial load and have subsequent inserts only performed in large batch
          operations. </li>
        <li>Avoid performing singleton <codeph>INSERT</codeph>, <codeph>UPDATE</codeph>, or
            <codeph>DELETE</codeph> operations on append-optimized tables. </li>
        <li>Avoid performing concurrent batch <codeph>UPDATE</codeph> or <codeph>DELETE</codeph>
          operations on append-optimized tables. Concurrent batch <codeph>INSERT</codeph> operations
          are acceptable. </li>
      </ul>
      <p>See <xref href="schema.xml#storage_model/heap_vs_ao"/>.</p>
    </section>
    <section>
      <title>Row vs. Column Oriented Storage</title>
      <ul id="ul_esm_src_3r">
        <li>Use row-oriented storage for workloads with iterative transactions where updates are
          required and frequent inserts are performed. </li>
        <li>Use row-oriented storage when selects against the table are wide.</li>
        <li>Use row-oriented storage for general purpose or mixed workloads.</li>
        <li>Use column-oriented storage where selects are narrow and aggregations of data are
          computed over a small number of columns.</li>
        <li>Use column-oriented storage for tables that have single columns that are regularly
          updated without modifying other columns in the row.</li>
      </ul>
      <p>See <xref href="schema.xml#storage_model/row_vs_column"/>.</p>
    </section>
    <section>
      <title>Compression</title>
      <ul id="ul_l5m_src_3r">
        <li>Use compression on large append-optimized and partitioned tables to improve I/O across
          the system.</li>
        <li>Set the column compression settings at the level where the data resides.</li>
        <li>Balance higher levels of compression with the time and CPU cycles needed to compress and
          uncompress data.</li>
      </ul>
      <p>See <xref href="schema.xml#topic_ebd_khx_z4"/>.</p>
    </section>
    <section>
      <title>Distributions</title>
      <ul id="ul_uwm_src_3r">
        <li>Explicitly define a column or random distribution for all tables. Do not use the
          default.</li>
        <li>Use a single column that will distribute data across all segments evenly.</li>
        <li>Do not distribute on columns that will be used in the <codeph>WHERE</codeph> clause of a
          query. </li>
        <li>Do not distribute on dates or timestamps. </li>
        <li> Never distribute and partition tables on the same column. </li>
        <li>Achieve local joins to significantly improve performance by distributing on the same
          column for large tables commonly joined together.</li>
        <li>To ensure there is no data skew, validate that data is evenly distributed after the
          initial load and after incremental loads. </li>
      </ul>
      <p>See <xref href="schema.xml#distribution"/>.</p>
    </section>
    <section>
      <title>Resource Queue Memory Management</title>
      <ul id="ul_ezm_src_3r">
        <li> Set <codeph>vm.overcommit_memory</codeph> to 2.</li>
        <li> Do not configure the OS to use huge pages. </li>
        <li> Use <codeph>gp_vmem_protect_limit</codeph> to set the maximum memory that the instance
          can allocate for <i>all</i> work being done in each segment database. </li>
        <li> You can use <codeph>gp_vmem_protect_limit</codeph> by calculating:<ul
            id="ul_bdj_xkq_kv">
            <li><codeph>gp_vmem</codeph> – the total memory available to Greenplum
              Database<codeblock>gp_vmem = ((SWAP + RAM) – (7.5GB + 0.05 * RAM)) / 1.7</codeblock>where
                <codeph>SWAP</codeph> is the host's swap space in GB, and <codeph>RAM</codeph> is
              the host's RAM in GB</li>
            <li><codeph>max_acting_primary_segments</codeph> – the maximum number of primary
              segments that could be running on a host when mirror segments are activated due to a
              host or segment failure</li>
            <li><codeph>gp_vmem_protect_limit</codeph><codeblock>gp_vmem_protect_limit = gp_vmem / acting_primary_segments</codeblock>Convert
              to MB to set the value of the configuration parameter. </li>
          </ul></li>
        <li>In a scenario where a large number of workfiles are generated calculate the
            <codeph>gp_vmem</codeph> factor with this formulat to account for the
          workfiles:<codeblock>gp_vmem = ((SWAP + RAM) – (7.5GB + 0.05 * RAM - (300KB *
      <varname>total_#_workfiles</varname>))) / 1.7</codeblock>
        </li>
        <li> Never set <codeph>gp_vmem_protect_limit</codeph> too high or larger than the physical
          RAM on the system. </li>
        <li>Use the calculated <codeph>gp_vmem</codeph> value to calculate the setting for the
            <codeph>vm.overcommit_ratio</codeph> operating system
          parameter:<codeblock>vm.overcommit_ratio = (RAM - 0.026 * gp_vmem) / RAM</codeblock></li>
        <li>Use <codeph>statement_mem</codeph> to allocate memory used for a query per segment
          db.</li>
        <li> Use resource queues to set both the numbers of active queries
            (<codeph>ACTIVE_STATEMENTS</codeph>) and the amount of memory
            (<codeph>MEMORY_LIMIT</codeph>) that can be utilized by queries in the queue. </li>
        <li> Associate all users with a resource queue. Do not use the default queue. </li>
        <li> Set <codeph>PRIORITY</codeph> to match the real needs of the queue for the workload and
          time of day. Avoid using MAX priority.</li>
        <li> Ensure that resource queue memory allocations do not exceed the setting for
            <codeph>gp_vmem_protect_limit</codeph>.</li>
        <li> Dynamically update resource queue settings to match daily operations flow.</li>
      </ul>
      <p>See <xref href="../install_guide/prep_os.html#topic3__sysctl_file" format="dita"
          scope="peer">Setting the Greenplum Recommended OS Parameters</xref> and  <xref
          href="workloads.xml#topic_hhc_z5w_r4"/>.</p>
    </section>
    <section>
      <title>Partitioning</title>
      <ul id="ul_ngn_src_3r">
        <li>Partition large tables only. Do not partition small tables.</li>
        <li>Use partitioning only if partition elimination (partition pruning) can be achieved based
          on the query criteria.</li>
        <li>Choose range partitioning over list partitioning.</li>
        <li>Partition the table based on a commonly-used column, such as a date column.</li>
        <li>Never partition and distribute tables on the same column. </li>
        <li>Do not use default partitions. </li>
        <li>Do not use multi-level partitioning; create fewer partitions with more data in each
          partition.</li>
        <li>Validate that queries are selectively scanning partitioned tables (partitions are being
          eliminated) by examining the query <codeph>EXPLAIN</codeph> plan.</li>
        <li>Do not create too many partitions with column-oriented storage because of the total
          number of physical files on every segment: <codeph>physical files = segments x columns x
            partitions</codeph></li>
      </ul>
      <p>See <xref href="schema.xml#topic_fmc_lx1_3r"/>.</p>
    </section>
    <section>
      <title>Indexes</title>
      <ul id="ul_f2n_src_3r">
        <li>In general indexes are not needed in Greenplum Database.</li>
        <li>Create an index on a single column of a columnar table for drill-through purposes for
          high cardinality tables that require queries with high selectivity. </li>
        <li>Do not index columns that are frequently updated. </li>
        <li>Consider dropping indexes before loading data into a table. After the load, re-create
          the indexes for the table. </li>
        <li>Create selective B-tree indexes. </li>
        <li>Do not create bitmap indexes on columns that are updated. </li>
        <li>Avoid using bitmap indexes for unique columns, very high or very low cardinality data.
          Bitmap indexes perform best when the column has a low cardinality—100 to 100,000 distinct
          values. </li>
        <li>Do not use bitmap indexes for transactional workloads.</li>
        <li>In general do not index partitioned tables. If indexes are needed, the index columns
          must be different than the partition columns.</li>
      </ul>
      <p>See <xref href="schema.xml#indexes"/>.</p>
    </section>
    <section id="_Toc286661611">
      <title>Resource Queues</title>
      <ul id="ul_nnn_src_3r">
        <li> Use resource queues to manage the workload on the cluster.</li>
        <li>Associate all roles with a user-defined resource queue. </li>
        <li>Use the <codeph>ACTIVE_STATEMENTS</codeph> parameter to limit the number of active
          queries that members of the particular queue can run concurrently. </li>
        <li>Use the <codeph>MEMORY_LIMIT</codeph> parameter to control the total amount of memory
          that queries running through the queue can utilize. </li>
        <li>Alter resource queues dynamically to match the workload and time of day.</li>
      </ul>
      <p>See <xref href="workloads.xml#topic_hhc_z5w_r4/configuring_rq"/>.</p>
    </section>
    <section>
      <title>Monitoring and Maintenance</title>
      <ul id="ul_wd5_bj4_y4">
        <li>Implement the "Recommended Monitoring and Maintenance Tasks" in the <i>Greenplum
            Database Administrator Guide</i>.</li>
        <li>Run <codeph>gpcheckperf</codeph> at install time and periodically thereafter, saving the
          output to compare system performance over time. </li>
        <li>Use all the tools at your disposal to understand how your system behaves under different
          loads. </li>
        <li>Examine any unusual event to determine the cause.</li>
        <li>Monitor query activity on the system by running explain plans periodically to ensure the
          queries are running optimally.</li>
        <li>Review plans to determine whether index are being used and partition elimination is
          occurring as expected.</li>
        <li>Know the location and content of system log files and monitor them on a regular basis,
          not just when problems arise.</li>
      </ul>
      <p>See <xref href="maintenance.xml#topic_yrk_bcx_r4"/>, <xref
          href="../admin_guide/query/topics/query-profiling.xml#topic39/in198649">Query
          Profiling</xref> and <xref href="logfiles.xml#topic_crh_2fc_3r"/>.</p>
    </section>
    <section id="_Toc286661612">
      <title>ANALYZE</title>
      <ul id="ul_spn_src_3r">
        <li>Determine if analyzing the database is actually needed. Analyzing is not needed if
            g<codeph>p_autostats_mode</codeph> is set to <codeph>on_no_stats</codeph> (the default)
          and the table is not partitioned.</li>
        <li>Use <codeph>analyzedb</codeph> in preference to <codeph>ANALYZE</codeph> when dealing
          with large sets of tables, as it does not require analyzing the entire database. The
            <codeph>analyzedb</codeph> utility updates statistics data for the specified tables
          incrementally and concurrently. For append optimized tables, <codeph>analyzedb</codeph>
          updates statistics incrementally only if the statistics are not current. For heap tables,
          statistics are always updated. <codeph>ANALYZE</codeph> does not update the table metadata
          that the <codeph>analyzedb</codeph> utility uses to determine whether table statistics are
          up to date.</li>
        <li>Selectively run <codeph>ANALYZE</codeph> at the table level when needed.</li>
        <li>Always run <codeph>ANALYZE</codeph> after <codeph>INSERT</codeph>,
            <codeph>UPDATE</codeph>. and <codeph>DELETE</codeph> operations that significantly
          changes the underlying data. </li>
        <li>Always run <codeph>ANALYZE</codeph> after <codeph>CREATE INDEX</codeph> operations. </li>
        <li>If <codeph>ANALYZE</codeph> on very large tables takes too long, run
            <codeph>ANALYZE</codeph> only on the columns used in a join condition,
            <codeph>WHERE</codeph> clause, <codeph>SORT</codeph>, <codeph>GROUP BY</codeph>, or
            <codeph>HAVING</codeph> clause. </li>
        <li>When dealing with large sets of tables, use <codeph>analyzedb</codeph> instead of
            <codeph>ANALYZE.</codeph></li>
      </ul>
      <p>See <xref href="analyze.xml#analyze"/>.</p>
    </section>
    <section id="_Toc286661609">
      <title>Vacuum</title>
      <ul id="ul_cjn_src_3r">
        <li>Run <codeph>VACUUM</codeph> after large <codeph>UPDATE</codeph> and
            <codeph>DELETE</codeph> operations.</li>
        <li>Do not run <codeph>VACUUM FULL</codeph>. Instead run a <codeph>CREATE
            TABLE...AS</codeph> operation, then rename and drop the original table.</li>
        <li>Frequently run <codeph>VACUUM</codeph> on the system catalogs to avoid catalog bloat and
          the need to run <codeph>VACUUM FULL</codeph> on catalog tables. </li>
        <li>Never kill <codeph>VACUUM</codeph> on catalog tables.</li>
      </ul>
      <p>See <xref href="bloat.xml#topic_gft_h11_bp"/>.</p>
    </section>
    <section id="_Toc286661610">
      <title>Loading</title>
      <ul id="ul_jln_src_3r">
        <li> Maximize the parallelism as the number of segments increase. </li>
        <li> Spread the data evenly across as many ETL nodes as possible. <ul id="ul_o1t_dqr_t1b">
            <li>Split very large data files into equal parts and spread the data across as many file
              systems as possible. </li>
            <li> Run two <codeph>gpfdist</codeph> instances per file system. </li>
            <li>Run <codeph>gpfdist</codeph> on as many interfaces as possible. </li>
            <li>Use <codeph>gp_external_max_segs</codeph> to control the number of segments that
              will request data from the <codeph>gpfdist</codeph> process. </li>
            <li>Always keep <codeph>gp_external_max_segs</codeph> and the number of
                <codeph>gpfdist</codeph> processes an even factor.</li>
          </ul></li>
        <li> Always drop indexes before loading into existing tables and re-create the index after
          loading. </li>
        <li> Run <codeph>VACUUM</codeph> after load errors to recover space. </li>
      </ul>
      <p>See <xref href="data_loading.xml#topic_fj5_b1x_r4"/>.</p>
    </section>
    <section>
      <title>Security</title>
      <ul id="ul_ocm_zsx_kr">
        <li> Secure the <codeph>gpadmin</codeph> user id and only allow essential system
          administrators access to it. </li>
        <li>Administrators should only log in to Greenplum as <codeph>gpadmin</codeph> when
          performing certain system maintenance tasks (such as upgrade or expansion). </li>
        <li>Limit users who have the <codeph>SUPERUSER</codeph> role attribute. Roles that are
          superusers bypass all access privilege checks in Greenplum Database, as well as resource
          queuing. Only system administrators should be given superuser rights. See "Altering Role
          Attributes" in the <i>Greenplum Database Administrator Guide</i>. </li>
        <li>Database users should never log on as <codeph>gpadmin</codeph>, and ETL or production
          workloads should never run as <codeph>gpadmin</codeph>. </li>
        <li>Assign a distinct Greenplum Database role to each user, application, or service that
          logs in. </li>
        <li>For applications or web services, consider creating a distinct role for each application
          or service. </li>
        <li>Use groups to manage access privileges. </li>
        <li>Protect the root password.</li>
        <li>Enforce a strong password password policy for operating system passwords.</li>
        <li>Ensure that important operating system files are protected. </li>
      </ul>
      <p>See <xref href="security.xml#topic_nyc_mdf_jr"/>.</p>
    </section>
    <section>
      <title>Encryption</title>
      <ul id="ul_sms_31r_bs">
        <li>Encrypting and decrypting data has a performance cost; only encrypt data that requires
          encryption.</li>
        <li>Do performance testing before implementing any encryption solution in a production
          system.</li>
        <li>Server certificates in a production Greenplum Database system should be signed by a
          certificate authority (CA) so that clients can authenticate the server. The CA may be
          local if all clients are local to the organization.</li>
        <li>Client connections to Greenplum Database should use SSL encryption whenever the
          connection goes through an insecure link.</li>
        <li>A symmetric encryption scheme, where the same key is used to both encrypt and decrypt,
          has better performance than an asymmetric scheme and should be used when the key can be
          shared safely.</li>
        <li>Use crytographic functions to encrypt data on disk. The data is encrypted and decrypted
          in the database process, so it is important to secure the client connection with SSL to
          avoid transmitting unencrypted data.</li>
        <li>Use the gpfdists protocol to secure ETL data as it is loaded into or unloaded from the
          database. .</li>
      </ul>
      <p>See <xref href="encryption.xml"/></p>
    </section>
    <section>
      <title>High Availability</title>
      <note>The following guidelines apply to actual hardware deployments, but not to public
        cloud-based infrastructure, where high availability solutions may already exist.</note>
      <ul id="ul_ry5_syr_s4">
        <li>Use a hardware RAID storage solution with 8 to 24 disks.</li>
        <li>Use RAID 1, 5, or 6 so that the disk array can tolerate a failed disk.</li>
        <li>Configure a hot spare in the disk array to allow rebuild to begin automatically when
          disk failure is detected.</li>
        <li>Protect against failure of the entire disk array and degradation during rebuilds by
          mirroring the RAID volume.</li>
        <li>Monitor disk utilization regularly and add additional space when needed. </li>
        <li>Monitor segment skew to ensure that data is distributed evenly and storage is consumed
          evenly at all segments.</li>
        <li>Set up a standby master instance to take over if the primary master fails. </li>
        <li>Plan how to switch clients to the new master instance when a failure occurs, for
          example, by updating the master address in DNS.</li>
        <li>Set up monitoring to send notifications in a system monitoring application or by email
          when the primary fails.</li>
        <li>Set up mirrors for all segments.</li>
        <li>Locate primary segments and their mirrors on different hosts to protect against host
          failure.</li>
        <li>Recover failed segments promptly, using the <codeph>gprecoverseg</codeph> utility, to
          restore redundancy and return the system to optimal balance.</li>
        <li>Consider a Dual Cluster configuration to provide an additional level of redundancy and
          additional query processing throughput. </li>
        <li>Backup Greenplum databases regularly unless the data is easily restored from
          sources.</li>
        <li>If backups are saved to local cluster storage, move the files to a safe, off-cluster
          location when the backup is complete. </li>
        <li>If backups are saved to NFS mounts, use a scale-out NFS solution such as Dell EMC Isilon
          to prevent IO bottlenecks.</li>
        <li>Consider using Greenplum integration to stream backups to the Dell EMC Data Domain
          enterprise backup platform.</li>
      </ul>
      <p>See <xref href="ha.xml#topic_q5g_pmj_s4"/>.</p>
    </section>
  </body>
</topic>
