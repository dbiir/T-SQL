<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Composite//EN" "ditabase.dtd">
<topic id="topic1" xml:lang="en">
  <title id="np160185">Routine System Maintenance Tasks</title>
  <shortdesc>To keep a Greenplum Database system running efficiently, the database must be regularly
    cleared of expired data and the table statistics must be updated so that the query optimizer has
    accurate information. </shortdesc>
  <body>
    <p>Greenplum Database requires that certain tasks be performed regularly to achieve optimal
      performance. The tasks discussed here are required, but database administrators can automate
      them using standard UNIX tools such as <codeph>cron</codeph> scripts. An administrator sets up
      the appropriate scripts and checks that they execute successfully. See <xref
        href="../monitoring/monitoring.dita#topic_kmz_lbg_rp"/> for additional suggested maintenance
      activities you can implement to keep your Greenplum system running optimally.</p>
  </body>
  <topic id="topic2" xml:lang="en">
    <title id="np157166">Routine Vacuum and Analyze</title>
    <body>
      <p>The design of the MVCC transaction concurrency model used in Greenplum Database means that
        deleted or updated data rows still occupy physical space on disk even though they are not
        visible to new transactions. If your database has many updates and deletes, many expired
        rows exist and the space they use must be reclaimed with the <codeph>VACUUM</codeph>
        command. The <codeph>VACUUM</codeph> command also collects table-level statistics, such as
        numbers of rows and pages, so it is also necessary to vacuum append-optimized tables, even
        when there is no space to reclaim from updated or deleted rows. </p>
      <p>Vacuuming an append-optimized table follows a different process than vacuuming heap tables.
        On each segment, a new segment file is created and visible rows are copied into it from the
        current segment. When the segment file has been copied, the original is scheduled to be
        dropped and the new segment file is made available. This requires sufficient available disk
        space for a copy of the visible rows until the original segment file is dropped. </p>
      <p>If the ratio of hidden rows to total rows in a segment file is less than a threshold value
        (10, by default), the segment file is not compacted. The threshold value can be configured
        with the <codeph>gp_appendonly_compaction_threshold</codeph> server configuration parameter.
          <codeph>VACUUM FULL</codeph> ignores the value of
          <codeph>gp_appendonly_compaction_threshold</codeph> and rewrites the segment file
        regardless of the ratio.</p>
      <p>You can use the <codeph>__gp_aovisimap_compaction_info()</codeph> function in the the
          <i>gp_toolkit</i> schema to investigate the effectiveness of a <cmdname>VACUUM</cmdname>
        operation on append-optimized tables. </p>
      <p>For information about the <codeph>__gp_aovisimap_compaction_info()</codeph> function see,
        "Checking Append-Optimized Tables" in the <cite>Greenplum Database Reference
        Guide</cite>.</p>
      <p><codeph>VACUUM</codeph> can be disabled for append-optimized tables using the
          <codeph>gp_appendonly_compaction</codeph> server configuration parameter. </p>
      <p>For details about vacuuming a database, see <xref href="../dml.xml#topic9" type="topic"
          format="dita"/>. </p>
      <p>For information about the <codeph>gp_appendonly_compaction_threshold</codeph> server
        configuration parameter and the <codeph>VACUUM</codeph> command, see the <cite>Greenplum
          Database Reference Guide</cite>.</p>
    </body>
    <topic id="topic3" xml:lang="en">
      <title>Transaction ID Management</title>
      <body>
        <p>Greenplum's MVCC transaction semantics depend on comparing transaction ID (XID) numbers
          to determine visibility to other transactions. Transaction ID numbers are compared using
          modulo 2<sup>32</sup> arithmetic, so a Greenplum system that runs more than about two
          billion transactions can experience transaction ID wraparound, where past transactions
          appear to be in the future. This means past transactions' outputs become invisible.
          Therefore, it is necessary to <codeph>VACUUM</codeph> every table in every database at
          least once per two billion transactions. </p>
        <p>Greenplum Database assigns XID values only to transactions that involve DDL or DML
          operations, which are typically the only transactions that require an XID.</p>
        <note type="important">Greenplum Database monitors transaction IDs. If you do not vacuum the
          database regularly, Greenplum Database will generate a warning and error.</note>
        <p>Greenplum Database issues the following warning when a significant portion of the
          transaction IDs are no longer available and before transaction ID wraparound occurs: </p>
        <p>
          <codeph>WARNING: database "database_name" must be vacuumed within
              <i>number_of_transactions</i> transactions</codeph>
        </p>
        <p>When the warning is issued, a <codeph>VACUUM</codeph> operation is required. If a
            <codeph>VACUUM</codeph> operation is not performed, Greenplum Database stops creating
          transactions when it reaches a limit prior to when transaction ID wraparound occurs.
          Greenplum Database issues this error when it stops creating transactions to avoid possible
          data loss: </p>
        <p>
          <codeblock>FATAL: database is not accepting commands to avoid 
wraparound data loss in database "database_name"
</codeblock>
        </p>
        <p>The Greenplum Database configuration parameter <codeph>xid_warn_limit</codeph> controls
          when the warning is displayed. The parameter <codeph>xid_stop_limit</codeph> controls when
          Greenplum Database stops creating transactions. </p>
        <section id="np160654">
          <title>Recovering from a Transaction ID Limit Error</title>
          <p>When Greenplum Database reaches the <codeph>xid_stop_limit</codeph> transaction ID
            limit due to infrequent <codeph>VACUUM</codeph> maintenance, it becomes unresponsive. To
            recover from this situation, perform the following steps as database administrator:</p>
          <ol>
            <li id="np160540">Shut down Greenplum Database.</li>
            <li id="np160541">Temporarily lower the <codeph>xid_stop_limit</codeph> by
              10,000,000.</li>
            <li id="np160542">Start Greenplum Database.</li>
            <li id="np160543">Run <codeph>VACUUM FREEZE</codeph> on all affected databases.</li>
            <li id="np160517">Reset the <codeph>xid_stop_limit</codeph> to its original value.</li>
            <li id="np160622">Restart Greenplum Database.</li>
          </ol>
          <p>For information about the configuration parameters, see the <i>Greenplum Database
              Reference Guide</i>. </p>
          <p>For information about transaction ID wraparound see the <xref
              href="https://www.postgresql.org/docs/9.4/index.html" scope="external"
              format="html"><ph>PostgreSQL documentation</ph></xref>.</p>
        </section>
      </body>
    </topic>
    <topic id="topic4" xml:lang="en">
      <title>System Catalog Maintenance</title>
      <body>
        <p>Numerous database updates with <codeph>CREATE</codeph> and <codeph>DROP</codeph> commands
          increase the system catalog size and affect system performance. For example, running many
            <codeph>DROP TABLE</codeph> statements degrades the overall system performance due to
          excessive data scanning during metadata operations on catalog tables. The performance loss
          occurs between thousands to tens of thousands of <codeph>DROP TABLE</codeph> statements,
          depending on the system.</p>
        <p>You should run a system catalog maintenance procedure regularly to reclaim the space
          occupied by deleted objects. If a regular procedure has not been run for a long time, you
          may need to run a more intensive procedure to clear the system catalog. This topic
          describes both procedures.</p>
      </body>
      <topic id="topic5" xml:lang="en">
        <title>Regular System Catalog Maintenance</title>
        <body>
          <p>It is recommended that you periodically run <codeph>REINDEX</codeph> and
              <codeph>VACUUM</codeph> on the system catalog to clear the space that deleted objects
            occupy in the system indexes and tables. If regular database operations include numerous
              <codeph>DROP</codeph> statements, it is safe and appropriate to run a system catalog
            maintenance procedure with <codeph>VACUUM</codeph> daily at off-peak hours. You can do
            this while the system is available.</p>
          <p>These are Greenplum Database system catalog maintenance steps.<ol id="ol_un5_p1l_f2b">
              <li>Perform a <codeph>REINDEX</codeph> on the system catalog tables to rebuild the
                system catalog indexes. This removes bloat in the indexes and improves
                  <codeph>VACUUM</codeph> performance.
                <note>When performing <codeph>REINDEX</codeph> on the system catalog tables, locking
                  will occur on the tables and might have an impact on currently running queries.
                  You can schedule the <codeph>REINDEX</codeph> operation during a period of low
                  activity to avoid disrupting ongoing business operations.</note></li>
              <li>Perform a <codeph>VACUUM</codeph> on the system catalog tables. </li>
              <li>Perform an <codeph>ANALYZE</codeph> on the system catalog tables to update the
                catalog table statistics. </li>
            </ol></p>
          <p>This example script performs a <codeph>REINDEX</codeph>, <codeph>VACUUM</codeph>, and
              <codeph>ANALYZE</codeph> of a Greenplum Database system catalog. In the script,
            replace <codeph>&lt;<varname>database-name</varname>></codeph> with a database name.</p>
          <codeblock>#!/bin/bash
DBNAME="&lt;<varname>database-name</varname>>"
SYSTABLES="' pg_catalog.' || relname || ';' FROM pg_class a, pg_namespace b 
WHERE a.relnamespace=b.oid AND b.nspname='pg_catalog' AND a.relkind='r'"

reindexdb --system -d $DBNAME
psql -tc "SELECT 'VACUUM' || $SYSTABLES" $DBNAME | psql -a $DBNAME
analyzedb -s pg_catalog -d $DBNAME
</codeblock>
          <note>If you are performing catalog maintenance during a maintenance period and you need
            to stop a process due to time constraints, run the Greenplum Database function
                <codeph>pg_cancel_backend(&lt;<varname>PID</varname>>)</codeph> to safely stop the
            Greenplum Database process.</note>
        </body>
      </topic>
      <topic id="topic6" xml:lang="en">
        <title>Intensive System Catalog Maintenance</title>
        <body>
          <p>If system catalog maintenance has not been performed in a long time, the catalog can
            become bloated with dead space; this causes excessively long wait times for simple
            metadata operations. A wait of more than two seconds to list user tables, such as with
            the <codeph>\d</codeph> metacommand from within <codeph>psql</codeph>, is an indication
            of catalog bloat.</p>
          <p>If you see indications of system catalog bloat, you must perform an intensive system
            catalog maintenance procedure with <codeph>VACUUM FULL</codeph> during a scheduled
            downtime period. During this period, stop all catalog activity on the system; the
              <codeph>VACUUM FULL</codeph> system catalog maintenance procedure takes exclusive
            locks against the system catalog.</p>
          <p>Running regular system catalog maintenance procedures can prevent the need for this
            more costly procedure.</p>
          <p>These are steps for intensive system catalog maintenance.<ol id="ol_trp_xqs_f2b">
              <li>Stop all catalog activity on the Greenplum Database system.</li>
              <li>Perform a <codeph>REINDEX</codeph> on the system catalog tables to rebuild the
                system catalog indexes. This removes bloat in the indexes and improves
                  <codeph>VACUUM</codeph> performance.</li>
              <li>Perform a <codeph>VACUUM FULL</codeph> on the system catalog tables. See the
                following Note.</li>
              <li>Perform an <codeph>ANALYZE</codeph> on the system catalog tables to update the
                catalog table statistics. </li>
            </ol></p>
          <note>The system catalog table <codeph>pg_attribute</codeph> is usually the largest
            catalog table. If the <codeph>pg_attribute</codeph> table is significantly bloated, a
              <codeph>VACUUM FULL</codeph> operation on the table might require a significant amount
            of time and might need to be performed separately. The presence of both of these
            conditions indicate a significantly bloated <codeph>pg_attribute</codeph> table that
            might require a long <codeph>VACUUM FULL</codeph> time:<ul id="ul_fl3_dty_f2b">
              <li>The <codeph>pg_attribute</codeph> table contains a large number of records.</li>
              <li>The diagnostic message for <codeph>pg_attribute</codeph> is <codeph>significant
                  amount of bloat</codeph> in the <codeph>gp_toolkit.gp_bloat_diag</codeph>
                view.</li>
            </ul></note>
        </body>
      </topic>
    </topic>
    <topic id="topic7" xml:lang="en">
      <title>Vacuum and Analyze for Query Optimization</title>
      <body>
        <p>Greenplum Database uses a cost-based query optimizer that relies on database statistics.
          Accurate statistics allow the query optimizer to better estimate selectivity and the
          number of rows that a query operation retrieves. These estimates help it choose the most
          efficient query plan. The <codeph>ANALYZE</codeph> command collects column-level
          statistics for the query optimizer.</p>
        <p>You can run both <codeph>VACUUM</codeph> and <codeph>ANALYZE</codeph> operations in the
          same command. For example:</p>
        <p>
          <codeblock>=# VACUUM ANALYZE mytable;
</codeblock>
        </p>
        <p>Running the <cmdname>VACUUM ANALYZE</cmdname> command might produce incorrect statistics
          when the command is run on a table with a significant amount of bloat (a significant
          amount of table disk space is occupied by deleted or obsolete rows). For large tables, the
            <codeph>ANALYZE</codeph> command calculates statistics from a random sample of rows. It
          estimates the number rows in the table by multiplying the average number of rows per page
          in the sample by the number of actual pages in the table. If the sample contains many
          empty pages, the estimated row count can be inaccurate. </p>
        <p>For a table, you can view information about the amount of unused disk space (space that
          is occupied by deleted or obsolete rows) in the <i>gp_toolkit</i> view
            <i>gp_bloat_diag</i>. If the <codeph>bdidiag</codeph> column for a table contains the
          value <codeph>significant amount of bloat suspected</codeph>, a significant amount of
          table disk space consists of unused space. Entries are added to the <i>gp_bloat_diag</i>
          view after a table has been vacuumed.</p>
        <p>To remove unused disk space from the table, you can run the command <cmdname>VACUUM
            FULL</cmdname> on the table. Due to table lock requirements, <cmdname>VACUUM
            FULL</cmdname> might not be possible until a maintenance period. </p>
        <p>As a temporary workaround, run <cmdname>ANALYZE</cmdname> to compute column statistics
          and then run <cmdname>VACUUM</cmdname> on the table to generate an accurate row count.
          This example runs <cmdname>ANALYZE</cmdname> and then <cmdname>VACUUM</cmdname> on the
            <i>cust_info</i> table.<codeblock>ANALYZE cust_info;
VACUUM cust_info;</codeblock></p>
        <note type="important">If you intend to execute queries on partitioned tables with GPORCA
          enabled (the default), you must collect statistics on the partitioned table root partition
          with the <cmdname>ANALYZE</cmdname> command. For information about GPORCA, see <xref
            href="../query/topics/query-piv-opt-overview.xml"/>. </note>
        <note>You can use the Greenplum Database utility <cmdname>analyzedb</cmdname> to update
          table statistics. Tables can be analyzed concurrently. For append optimized tables,
            <cmdname>analyzedb</cmdname> updates statistics only if the statistics are not current.
          See the <codeph><xref href="../../utility_guide/ref/analyzedb.xml"
              >analyzedb</xref></codeph> utility.</note>
      </body>
    </topic>
  </topic>
  <topic id="topic8" xml:lang="en">
    <title id="np157197">Routine Reindexing</title>
    <body>
      <p>For B-tree indexes, a freshly-constructed index is slightly faster to access than one that
        has been updated many times because logically adjacent pages are usually also physically
        adjacent in a newly built index. Reindexing older indexes periodically can improve access
        speed. If all but a few index keys on a page have been deleted, there will be wasted space
        on the index page. A reindex will reclaim that wasted space. In Greenplum Database it is
        often faster to drop an index (<codeph>DROP INDEX</codeph>) and then recreate it
          (<codeph>CREATE INDEX</codeph>) than it is to use the <codeph>REINDEX</codeph>
        command.</p>
      <p>For table columns with indexes, some operations such as bulk updates or inserts to the
        table might perform more slowly because of the updates to the indexes. To enhance
        performance of bulk operations on tables with indexes, you can drop the indexes, perform the
        bulk operation, and then re-create the index.</p>
    </body>
  </topic>
  <topic id="topic9" xml:lang="en">
    <title id="np157216">Managing Greenplum Database Log Files</title>
    <body>
      <ul>
        <li id="np159000">
          <xref href="#topic10" type="topic" format="dita"/>
        </li>
        <li id="np159008">
          <xref href="#topic11" type="topic" format="dita"/>
        </li>
      </ul>
    </body>
    <topic id="topic10" xml:lang="en">
      <title id="np157220">Database Server Log Files</title>
      <body>
        <p>Greenplum Database log output tends to be voluminous, especially at higher debug levels,
          and you do not need to save it indefinitely. Administrators should purge older log files
          periodically.</p>
        <p>Greenplum Database by default has log file rotation enabled for the master and segment
          database logs. Log files are created in the <codeph>pg_log</codeph> subdirectory of the
          master and each segment data directory using the following naming convention:
              <codeph>gpdb-<i>YYYY</i>-<i>MM</i>-<i>DD_hhmmss</i>.csv</codeph>. Administrators need
          to implement scripts or programs to periodically clean up old log files in the
            <codeph>pg_log</codeph> directory of the master and each segment instance.</p>
        <p>Log rotation can be triggered by the size of the current log file or the age of the
          current log file. The <codeph>log_rotation_size</codeph> configuration parameter sets the
          size of an individual log file that triggers log rotation. When the log file size is equal
          to or greater than the specified size, the file is closed and a new log file is created.
          The <codeph>log_rotation_size</codeph> value is specified in kilobytes. The default is
          1048576 kilobytes, or 1GB. If <codeph>log_rotation_size</codeph> is set to 0, size-based
          rotation is disabled.</p>
        <p>The <codeph>log_rotation_age</codeph> configuration parameter specifies the age of a log
          file that triggers rotation. When the specified amount of time has elapsed since the log
          file was created, the file is closed and a new log file is created. The default
            <codeph>log_rotation_age</codeph>, 1d, creates a new log file 24 hours after the current
          log file was created. If <codeph>log_rotation_age</codeph> is set to 0, time-based
          rotation is disabled.</p>
        <p>For information about viewing the database server log files, see <xref
            href="monitor.xml#topic28" type="topic" format="dita"/>.</p>
      </body>
    </topic>
    <topic id="topic11" xml:lang="en">
      <title id="np158979">Management Utility Log Files</title>
      <body>
        <p>Log files for the Greenplum Database management utilities are written to
            <codeph>~/gpAdminLogs</codeph> by default. The naming convention for management log
          files is: </p>
        <codeblock><varname>script_name</varname>_<varname>date</varname>.log
</codeblock>
        <p>The log entry format is: </p>
        <codeblock><varname>timestamp</varname>:<varname>utility</varname>:<varname>host</varname>:<varname>user</varname>:[INFO|WARN|FATAL]:<varname>message</varname>
</codeblock>
        <p>The log file for a particular utility execution is appended to its daily log file each
          time that utility is run.</p>
      </body>
    </topic>
  </topic>
</topic>
