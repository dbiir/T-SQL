<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Composite//EN" "ditabase.dtd">
<topic id="topic1" xml:lang="en">
  <title id="ki180869">Enabling High Availability Features</title>
  <shortdesc>You can ensure high availability for a Greenplum Database system by setting up a hot
    standby for each system component. </shortdesc>
  <body>
    <p>This chapter describes the high-availability features of Greenplum Database and the process
      to recover a segment or master instance.</p>
    <p>For information about the Greenplum Database utilities that are used to enable high
      availability, see the <i>Greenplum Database Utility Guide</i>. </p>
  </body>
  <topic id="topic2" xml:lang="en">
    <title id="ki155204">Overview of High Availability in Greenplum Database</title>
    <body>
      <p>Greenplum Database includes features to ensure maximum uptime and high availability of your
        system. The following topics summarize these features:</p>
      <ul>
        <li id="ki155212">
          <xref href="#topic3" type="topic" format="dita"/>
        </li>
        <li id="ki155216">
          <xref href="#topic4" type="topic" format="dita"/>
        </li>
        <li id="ki155220">
          <xref href="#topic5" type="topic" format="dita"/>
        </li>
      </ul>
    </body>
    <topic id="topic3" xml:lang="en">
      <title id="ki155227">Overview of Segment Mirroring</title>
      <body>
        <p>When Greenplum Database High Availability is enabled, there are two types of segments:
            <i>primary</i> and <i>mirror</i>. Each primary segment has one corresponding mirror
          segment. A primary segment receives requests from the master to make changes to the
          segment's database and then replicates those changes to the corresponding mirror. If a
          primary segment becomes unavailable, database queries fail over to the mirror segment.</p>
        <p>Segment mirroring employs a physical file replication scheme&#8212;data file I/O at the
          primary is replicated to the secondary so that the mirror's files are identical to the
          primary's files. Data in Greenplum Database are represented with <i>tuples</i>, which are
          packed into <i>blocks</i>. Database tables are stored in disk files consisting of one or
          more blocks. A change to a tuple changes the block it is saved in, which is then written
          to disk on the primary and copied over the network to the mirror. The mirror updates the
          corresponding block in its copy of the file.</p>
        <p>For heap tables, blocks are saved in an in-memory cache until they are evicted to make
          room for newly changed blocks. This allows the system to read or update a block in memory
          multiple times without performing expensive disk I/O. When the block is evicted from the
          cache, it is written to disk and replicated to the secondary. While the block is held in
          cache, the primary and mirror have different images of the block. However, the databases
          are still consistent because the transaction log has been replicated. If a mirror takes
          over for a failed primary, the transactions in its log are applied to the database
          tables.</p>
        <p>Other database objects &#8212; for example filespaces, which are tablespaces internally
          represented with directories&#8212;also use file replication to perform various file
          operations in a synchronous way.</p>
        <p>Append-optimized tables do not use the in-memory caching mechanism. Changes made to
          append-optimized table blocks are replicated to the mirror immediately. Typically, file
          write operations are asynchronous, while opening, creating, and synchronizing files are
          "sync-replicated," which means the primary blocks until it receives the acknowledgment
          from the secondary. </p>
        <p>To configure mirroring, your Greenplum Database system must have enough nodes for a
          primary segment and its mirror to reside on different hosts. Only primary segments are
          active during database operations.</p>
        <fig id="ki169754">
          <title>Segment Data Mirroring in Greenplum Database</title>
          <image href="../../graphics/mirrorsegs.png" placement="break" width="485px" height="135px"
          />
        </fig>
        <p>If a primary segment fails, the file replication process stops and the mirror segment
          automatically starts as the active segment instance. The now active mirror's system state
          becomes <i>Change Tracking</i>, which means the mirror maintains a system table and
          change-log of all blocks updated while the primary segment is unavailable. When the failed
          primary segment is repaired and ready to be brought back online, an administrator
          initiates a recovery process and the system goes into <i>Resynchronization</i> state. The
          recovery process applies the logged changes to the repaired primary segment. The system
          state changes to <i>Synchronized</i> when the recovery process completes.</p>
        <p>If the mirror segment fails or becomes inaccessible while the primary is active, the
          primary's system state changes to <i>Change Tracking</i>, and it tracks changes to be
          applied to the mirror when it is recovered.</p>
      </body>
    </topic>
    <topic id="topic4" xml:lang="en">
      <title id="ki155249">Overview of Master Mirroring</title>
      <body>
        <p>You can deploy a backup or mirror of the master instance on a separate host machine or on
          the same host machine. A backup master or standby master serves as a warm standby if the
          primary master becomes nonoperational. You create a standby master from the primary master
          while the primary is online. </p>
        <p>The primary master continues to provide service to users while a transactional snapshot
          of the primary master instance is taken. While the transactional snapshot is taken and
          deployed on the standby master, changes to the primary master are also recorded. After the
          snapshot is deployed on the standby master, the updates are deployed to synchronize the
          standby master with the primary master.</p>
        <p>Once the primary master and standby master are synchronized, the standby master is kept
          up to date by the <codeph>walsender</codeph> and <codeph>walreceiver</codeph> a
          replication processes. The <codeph>walreceiver</codeph> is a standby master process. The
            <codeph>walsender</codeph> process is a primary master process. The two processes use
          WAL based streaming replication to keep the primary and standby masters synchronized.</p>
        <p>Since the master does not house user data, only system catalog tables are synchronized
          between the primary and standby masters. When these tables are updated, changes are
          automatically copied to the standby master to keep it current with the primary.</p>
        <fig id="ki155262">
          <title>Master Mirroring in Greenplum Database</title>
          <image href="../../graphics/standby_master.jpg" placement="break" width="271px"
            height="165px"/>
        </fig>
        <p>If the primary master fails, the replication process stops, and an administrator can
          activate the standby master. Upon activation of the standby master, the replicated logs
          reconstruct the state of the primary master at the time of the last successfully committed
          transaction. The activated standby then functions as the Greenplum Database master,
          accepting connections on the port specified when standby master was initialized.</p>
      </body>
    </topic>
    <topic id="topic5" xml:lang="en">
      <title id="ki159645">Overview of Fault Detection and Recovery</title>
      <body>
        <p>The Greenplum Database server (<codeph>postgres</codeph>) subprocess named
            <codeph>ftsprobe</codeph> handles fault detection. <codeph>ftsprobe</codeph> monitors
          the Greenplum array; it connects to and scans all segments and database processes at
          intervals that you can configure. </p>
        <p>If <codeph>ftsprobe</codeph> cannot connect to a segment, it marks the segment as "down"
          in the Greenplum Database system catalog. The segment remains nonoperational until an
          administrator initiates the recovery process.</p>
        <p>With mirroring enabled, Greenplum Database automatically fails over to a mirror copy if a
          primary copy becomes unavailable. The system is operational if a segment instance or host
          fails provided all data is available on the remaining active segments. </p>
        <p>To recover failed segments, a Greenplum administrator runs the
            <codeph>gprecoverseg</codeph> recovery utility. This utility locates the failed
          segments, verifies they are valid, and compares the transactional state with the currently
          active segment to determine changes made while the segment was offline.
            <codeph>gprecoverseg</codeph> synchronizes the changed database files with the active
          segment and brings the segment back online. Administrators perform the recovery while
          Greenplum Database is up and running.</p>
        <p>With mirroring disabled, the system automatically shuts down if a segment instance fails.
          Administrators manually recover all failed segments before operations resume.</p>
      </body>
    </topic>
  </topic>
  <topic id="topic6" xml:lang="en">
    <title id="ki155342">Enabling Mirroring in Greenplum Database</title>
    <body>
      <p>You can configure your Greenplum Database system with mirroring at setup time using
          <codeph>gpinitsystem</codeph> or enable mirroring later using
          <codeph>gpaddmirrors</codeph> and <codeph>gpinitstandby</codeph>. This topic assumes you
        are adding mirrors to an existing system that was initialized without mirrors.</p>
      <p>You can enable the following types of mirroring:</p>
      <ul>
        <li id="ki155350">
          <xref href="#topic7" type="topic" format="dita"/>
        </li>
        <li id="ki155354">
          <xref href="#topic8" type="topic" format="dita"/>
        </li>
      </ul>
    </body>
    <topic id="topic7" xml:lang="en">
      <title id="ki155357">Enabling Segment Mirroring</title>
      <body>
        <p>Mirror segments allow database queries to fail over to a backup segment if the primary
          segment is unavailable. To configure mirroring, your Greenplum Database system must have
          enough nodes to allow the mirror segment to reside on a different host than its primary.
          By default, mirrors are configured on the same array of hosts as the primary segments. You
          may choose a completely different set of hosts for your mirror segments so they do not
          share machines with any of your primary segments.</p>
        <note type="important">During the online data replication process, Greenplum Database should
          be in a quiescent state, workloads and other queries should not be running.</note>
        <section id="ki169450">
          <title>To add segment mirrors to an existing system (same hosts as primaries)</title>
          <ol>
            <li id="ki156981">Allocate the data storage area for mirror data on all segment hosts.
              The data storage area must be different from your primary segments' file system
              location.</li>
            <li id="ki156983">Use <codeph>gpssh-exkeys</codeph> to ensure that the segment hosts can
              SSH and SCP to each other without a password prompt.</li>
            <li id="ki156993">Run the <codeph>gpaddmirrors</codeph> utility to enable mirroring in
              your Greenplum Database system. For example, to add 10000 to your primary segment port
              numbers to calculate the mirror segment port
                numbers:<codeblock>$ gpaddmirrors -p 10000</codeblock><p>Where <codeph>-p</codeph>
                specifies the number to add to your primary segment port numbers.</p></li>
          </ol>
        </section>
        <section>
          <title>To add segment mirrors to an existing system (different hosts from
            primaries)</title>
          <ol>
            <li id="ki155416">Ensure the Greenplum Database software is installed on all hosts. See
              the <i>Greenplum Database Installation Guide</i> for detailed installation
              instructions.</li>
            <li id="ki161799">Allocate the data storage area for mirror data on all segment
              hosts.</li>
            <li id="ki160816">Use <codeph>gpssh-exkeys</codeph> to ensure the segment hosts can SSH
              and SCP to each other without a password prompt.</li>
            <li id="ki155422">Create a configuration file that lists the host names, ports, and data
              directories on which to create mirrors. To create a sample configuration file to use
              as a starting point,
                run:<codeblock>$ gpaddmirrors -o <i>filename</i></codeblock><p>The format of the
                mirror configuration file is:
                </p><codeblock>filespaceOrder=[<i>filespace1_fsname</i>[|<i>filespace2_fsname</i>|...] 
[<i>content</i>]=<i>content</i>|<i>address</i>|<i>port</i>|<i>mir_replication_port</i>|
<i>pri_replication_port</i>|<i>fselocation</i>[|<i>fselocation</i>|...]</codeblock><p>For
                example, a configuration for two segment hosts and two segments per host, with no
                additional filespaces configured besides the default <i>pg_system</i>
                filespace):</p><codeblock>filespaceOrder=
0=0|sdw1|sdw1-1|52001|53001|54001|/gpdata/mir1/gp0
1=1|sdw1|sdw1-2|52002|53002|54002|/gpdata/mir1/gp1
2=2|sdw2|sdw2-1|52001|53001|54001|/gpdata/mir1/gp2
3=3|sdw2|sdw2-2|52002|53002|54002|/gpdata/mir1/gp3
</codeblock></li>
            <li id="ki155430">Run the <codeph>gpaddmirrors</codeph> utility to enable mirroring in
              your Greenplum Database
                system:<codeblock>$ gpaddmirrors -i <i>mirror_config_file</i></codeblock><p>Where
                  <codeph>-i</codeph> names the mirror configuration file you just created.</p></li>
          </ol>
        </section>
      </body>
    </topic>
    <topic id="topic8" xml:lang="en">
      <title id="ki155443">Enabling Master Mirroring</title>
      <body>
        <p>You can configure a new Greenplum Database system with a standby master using
            <codeph>gpinitsystem</codeph> or enable it later using <codeph>gpinitstandby</codeph>.
          This topic assumes you are adding a standby master to an existing system that was
          initialized without one.</p>
        <section id="ki160203">
          <title>To add a standby master to an existing system</title>
          <ol>
            <li id="ki160206">Ensure the standby master host is installed and configured:
                <codeph>gpadmin</codeph> system user created, Greenplum Database binaries installed,
              environment variables set, SSH keys exchanged, and data directory created. See the
                <i>Greenplum Database Installation Guide</i> for detailed installation
              instructions.</li>
            <li id="ki155475">Run the <codeph>gpinitstandby</codeph> utility on the currently active
                <i>primary</i> master host to add a standby master host to your Greenplum Database
              system. For example:<codeblock>$ gpinitstandby -s smdw</codeblock><p>Where
                  <codeph>-s</codeph> specifies the standby master host name.</p></li>
            <li id="ki155485">To switch operations to a standby master, see <xref href="#topic16"
                type="topic" format="dita"/>.</li>
          </ol>
          <title>To check the status of the master mirroring process (optional)</title>
          <p>You can display the information in the Greenplum Database system view
            pg_stat_replication. The view lists information about the <codeph>walsender</codeph>
            process that is used for Greenplum Database master mirroring. For example, this command
            displays the process ID and state of the <codeph>walsender</codeph> process:</p>
          <codeblock>$ psql dbname -c 'SELECT pid, state FROM pg_stat_replication;'</codeblock>
          <p>For information about the <codeph>pg_stat_replication</codeph> system view, see the
              <i>Greenplum Database Reference Guide</i>. </p>
        </section>
      </body>
    </topic>
  </topic>
  <topic id="topic9" xml:lang="en">
    <title id="ki155550">Detecting a Failed Segment</title>
    <body>
      <p>With mirroring enabled, Greenplum Database automatically fails over to a mirror segment
        when a primary segment goes down. The mirror segment assumes the role of the primary segment
        and the failed primary segment becomes the mirror. Transactions in progress when a fault
        occurs roll back and must be restarted. When restarted the transactions run on the new
        primary segment.</p>
      <p>To return the Greenplum Database cluster to full redundancy and balance, an administrator
        recovers the down mirror segment and exchanges the roles of the primary and mirror
        segments so that both segments are in their preferred roles.</p>
      <p>If the entire Greenplum Database system becomes nonoperational due to a segment failure
        (for example, if mirroring is not enabled or not enough segments are online to access all
        user data), users will see errors when trying to connect to a database. The errors returned
        to the client program may indicate the failure. For example:</p>
      <codeblock>ERROR: All segment databases are unavailable</codeblock>
    </body>
    <topic id="topic11" xml:lang="en">
      <title>Checking for Failed Segments</title>
      <body>
        <p>With mirroring enabled, you may have failed segments in the system without interruption
          of service or any indication that a failure has occurred. You can verify the status of
          your system using the <codeph>gpstate</codeph> utility. <codeph>gpstate</codeph> provides
          the status of each individual component of a Greenplum Database system, including primary
          segments, mirror segments, master, and standby master.</p>
        <section id="ki155580">
          <title>To check for failed segments</title>
          <ol>
            <li id="ki155584">On the master, run the <codeph>gpstate</codeph> utility with the
                <codeph>-e</codeph> option to show segments with error
                conditions:<codeblock>$ gpstate -e</codeblock><p>Segments in <i>Change Tracking</i>
                mode indicate the corresponding mirror segment is down. When a segment is not in its
                  <i>preferred role</i>, the segment does not operate in the role to which it was
                assigned at system initialization. This means the system is in a potentially
                unbalanced state, as some segment hosts may have more active segments than is
                optimal for top system performance.</p><p>See <xref href="#topic14" type="topic"
                  format="dita"/> for instructions to fix this situation.</p></li>
            <li id="ki165350">To get detailed information about a failed segment, check the
                <i>gp_segment_configuration</i> catalog table. For
              example:<codeblock>$ psql -c "SELECT * FROM gp_segment_configuration WHERE status='d';"</codeblock></li>
            <li id="ki162539">For failed segment instances, note the host, port, preferred role, and
              data directory. This information will help determine the host and segment instances to
              troubleshoot.</li>
            <li id="ki155591">To show information about mirror segment instances,
              run:<codeblock>$ gpstate -m</codeblock></li>
          </ol>
        </section>
      </body>
    </topic>
    <topic id="topic12" xml:lang="en">
      <title id="ki155597">Checking the Log Files</title>
      <body>
        <p>Log files can provide information to help determine an error's cause. The master and
          segment instances each have their own log file in <codeph>pg_log</codeph> of the data
          directory. The master log file contains the most information and you should always check
          it first. </p>
        <p>Use the <codeph>gplogfilter</codeph> utility to check the Greenplum Database log files
          for additional information. To check the segment log files, run
            <codeph>gplogfilter</codeph> on the segment hosts using <codeph>gpssh</codeph>.</p>
        <section id="ki170080">
          <title>To check the log files</title>
          <ol>
            <li id="ki168755">Use <codeph>gplogfilter</codeph> to check the master log file for
                <codeph>WARNING</codeph>, <codeph>ERROR</codeph>, <codeph>FATAL</codeph> or
                <codeph>PANIC</codeph> log level
              messages:<codeblock>$ gplogfilter -t</codeblock></li>
            <li id="ki166372">Use <codeph>gpssh</codeph> to check for <codeph>WARNING</codeph>,
                <codeph>ERROR</codeph>, <codeph>FATAL</codeph>, or <codeph>PANIC</codeph> log level
              messages on each segment instance. For
              example:<codeblock>$ gpssh -f seg_hosts_file -e 'source 
/usr/local/greenplum-db/greenplum_path.sh ; gplogfilter -t 
/data1/primary/*/pg_log/gpdb*.log' &gt; seglog.out
</codeblock></li>
          </ol>
        </section>
      </body>
    </topic>
  </topic>
  <topic id="topic13" xml:lang="en">
    <title id="ki155618">Recovering a Failed Segment</title>
    <body>
      <p>If the master cannot connect to a segment instance, it marks that segment as down in the
        Greenplum Database system catalog. The segment instance remains offline until an
        administrator takes steps to bring the segment back online. The process for recovering a
        failed segment instance or host depends on the failure cause and whether or not mirroring is
        enabled. A segment instance can be unavailable for many reasons:</p>
      <ul>
        <li id="ki155624">A segment host is unavailable; for example, due to network or hardware
          failures.</li>
        <li id="ki155625">A segment instance is not running; for example, there is no
            <codeph>postgres</codeph> database listener process.</li>
        <li id="ki155626">The data directory of the segment instance is corrupt or missing; for
          example, data is not accessible, the file system is corrupt, or there is a disk
          failure.</li>
      </ul>
      <p><xref href="#topic13/ki155628" type="fig" format="dita"/> shows the high-level steps for
        each of the preceding failure scenarios.</p>
      <fig id="ki155628">
        <title>Segment Failure Troubleshooting Matrix</title>
        <image href="../../graphics/recovermatrix.png" placement="break" width="460" height="430px"
          id="image_cdg_nfp_34"/>
      </fig>
    </body>
    <topic id="topic14" xml:lang="en">
      <title>Recovering From Segment Failures</title>
      <body>
        <p>Segment host failures usually cause multiple segment failures: all primary or mirror
          segments on the host are marked as down and nonoperational. If mirroring is not enabled
          and a segment goes down, the system automatically becomes nonoperational.</p>
        <section id="ki155642">
          <title>To recover with mirroring enabled</title>
          <ol>
            <li id="ki155643">Ensure you can connect to the segment host from the master host. For
              example:<codeblock>$ ping <i>failed_seg_host_address</i></codeblock></li>
            <li id="ki155645">Troubleshoot the problem that prevents the master host from connecting
              to the segment host. For example, the host machine may need to be restarted or
              replaced.</li>
            <li id="ki155646">After the host is online and you can connect to it, run the
                <codeph>gprecoverseg</codeph> utility from the master host to reactivate the failed
              segment instances. For example:<codeblock>$ gprecoverseg</codeblock></li>
            <li id="ki158500">The recovery process brings up the failed segments and identifies the
              changed files that need to be synchronized. The process can take some time; wait for
              the process to complete. During this process, database write activity is suspended. </li>
            <li id="ki164382">After <codeph>gprecoverseg</codeph> completes, the system goes into
                <i>Resynchronizing</i> mode and begins copying the changed files. This process runs
              in the background while the system is online and accepting database requests.</li>
            <li id="ki158504">When the resynchronization process completes, the system state is
                <i>Synchronized</i>. Run the <codeph>gpstate</codeph> utility to verify the status
              of the resynchronization process:<codeblock>$ gpstate -m</codeblock></li>
          </ol>
          <title id="ki155666">To return all segments to their preferred role</title>
          <p>When a primary segment goes down, the mirror activates and becomes the primary segment.
            After running <codeph>gprecoverseg</codeph>, the currently active segment remains the
            primary and the failed segment becomes the mirror. The segment instances are not
            returned to the preferred role that they were given at system initialization time. This
            means that the system could be in a potentially unbalanced state if segment hosts have
            more active segments than is optimal for top system performance. To check for unbalanced
            segments and rebalance the system, run:</p>
          <codeblock>$ gpstate -e</codeblock>
          <p>All segments must be online and fully synchronized to rebalance the system. Database
            sessions remain connected during rebalancing, but queries in progress are canceled and
            rolled back. </p>
          <ol>
            <li id="ki165540">Run <codeph>gpstate -m</codeph> to ensure all mirrors are
                <i>Synchronized</i>. <codeblock>$ gpstate -m</codeblock></li>
            <li id="ki165577">If any mirrors are in <i>Resynchronizing</i> mode, wait for them to
              complete.</li>
            <li id="ki165591">Run gprecoverseg with the -r option to return the segments to their
              preferred roles.<codeblock>$ gprecoverseg -r</codeblock></li>
            <li id="ki166668">After rebalancing, run <codeph>gpstate -e</codeph> to confirm all
              segments are in their preferred roles.<codeblock>$ gpstate -e</codeblock></li>
          </ol>
          <title>To recover from a double fault</title>
          <p>In a double fault, both a primary segment and its mirror are down. This can occur if
            hardware failures on different segment hosts happen simultaneously. Greenplum Database
            is unavailable if a double fault occurs. To recover from a double fault:</p>
          <ol>
            <li id="ki165670">Restart Greenplum Database:<codeblock>$ gpstop -r</codeblock></li>
            <li id="ki165671">After the system restarts, run
              <codeph>gprecoverseg</codeph>:<codeblock>$ gprecoverseg</codeblock></li>
            <li id="ki165709">After <codeph>gprecoverseg</codeph> completes, use
                <codeph>gpstate</codeph> to check the status of your
              mirrors:<codeblock>$ gpstate -m</codeblock></li>
            <li id="ki165730">If you still have segments in Change Tracking mode, run a full copy
              recovery:<codeblock>$ gprecoverseg -F</codeblock></li>
          </ol>
          <title>To recover without mirroring enabled</title>
          <ol>
            <li id="ki155667">Ensure you can connect to the segment host from the master host. For
              example:<codeblock>$ ping <i>failed_seg_host_address
</i></codeblock></li>
            <li id="ki155669">Troubleshoot the problem that is preventing the master host from
              connecting to the segment host. For example, the host machine may need to be
              restarted.</li>
            <li id="ki155670">After the host is online, verify that you can connect to it and
              restart Greenplum Database. For example:<codeblock>$ gpstop -r</codeblock></li>
            <li id="ki155681">Run the <codeph>gpstate</codeph> utility to verify that all segment
              instances are online:<codeblock>$ gpstate</codeblock></li>
          </ol>
          <p>If a segment host is not recoverable and you lost one or more segments, recreate your
            Greenplum Database system from backup files. See <xref href="backup.xml#topic1"
              type="topic" format="dita"/>.</p>
        </section>
      </body>
      <topic id="topic15" xml:lang="en">
        <title>When a segment host is not recoverable</title>
        <body>
          <p>If a host is nonoperational, for example, due to hardware failure, recover the segments
            onto a spare set of hardware resources. If mirroring is enabled, you can recover a
            segment from its mirror onto an alternate host using <codeph>gprecoverseg</codeph>. For
            example:</p>
          <codeblock>$ gprecoverseg -i <i>recover_config_file</i></codeblock>
          <p>Where the format of <codeph><i>recover_config_file</i></codeph> is:</p>
          <codeblock>filespaceOrder=[<i>filespace1_name</i>[|<i>filespace2_name</i>|...]<i>failed_host_address</i>|
<i>port</i>|<i>fselocation</i> [<i>recovery_host_address</i>|<i>port</i>|<i>replication_port</i>|<i>fselocation</i>
[|<i>fselocation</i>|...]]</codeblock>
          <p>For example, to recover to a different host than the failed host without additional
            filespaces configured (besides the default <i>pg_system</i> filespace):</p>
          <codeblock>filespaceOrder=sdw5-2|50002|/gpdata/gpseg2 sdw9-2|50002|53002|/gpdata/gpseg2</codeblock>
          <p>The <i>gp_segment_configuration</i> and <i>pg_filespace_entry</i> system catalog tables
            can help determine your current segment configuration so you can plan your mirror
            recovery configuration. For example, run the following query:</p>
          <codeblock>=# SELECT dbid, content, hostname, address, port, 
   replication_port, fselocation as datadir 
   FROM gp_segment_configuration, pg_filespace_entry 
   WHERE dbid=fsedbid 
   ORDER BY dbid;</codeblock>
          <p>The new recovery segment host must be pre-installed with the Greenplum Database
            software and configured exactly as the existing segment hosts.</p>
        </body>
      </topic>
    </topic>
  </topic>
  <topic id="topic16" xml:lang="en">
    <title id="ki155774">Recovering a Failed Master</title>
    <body>
      <p>If the primary master fails, log replication stops. Use <codeph>gpactivatestandby</codeph>
        to activate the standby master. Upon activation of the standby master, Greenplum Database
        reconstructs the master host state at the time of the last successfully committed
        transaction. </p>
      <section id="ki181117">
        <title>To activate the standby master</title>
        <ol>
          <li id="ki155784">Ensure a standby master host is configured for the system. See <xref
              href="#topic8" type="topic" format="dita"/>.</li>
          <li id="ki155791">Run the <codeph>gpactivatestandby</codeph> utility from the standby
            master host you are activating. For
              example:<codeblock>$ gpactivatestandby -d /data/master/gpseg-1</codeblock><p>Where
                <codeph>-d</codeph> specifies the data directory of the master host you are
              activating.</p><p>After you activate the standby, it becomes the <i>active</i> or
                <i>primary</i> master for your Greenplum Database array. </p></li>
          <li id="ki155816">After the utility finishes, run <codeph>gpstate</codeph> to check the
              status:<codeblock>$ gpstate -f</codeblock><p>The newly activated master's status
              should be <i>Active</i>. If you configured a new standby host, its status is
                <i>Passive</i>. When a standby master is not configured, the command displays
                <codeph>-No entries found</codeph> and the message indicates that a standby master
              instance is not configured.</p></li>
          <li id="ki169625">After switching to the newly active master host, run
              <codeph>ANALYZE</codeph> on it. For
            example:<codeblock>$ psql <codeph>dbname</codeph> -c 'ANALYZE;'</codeblock></li>
          <li id="ki155823">Optional: If you did not specify a new standby host when running the
              <codeph>gpactivatestandby</codeph> utility, use <codeph>gpinitstandby</codeph> to
            configure a new standby master at a later time. Run <codeph>gpinitstandby</codeph> on
            your active master host. For
            example:<codeblock>$ gpinitstandby -s <i>new_standby_master_hostname</i></codeblock></li>
        </ol>
      </section>
    </body>
    <topic id="topic17" xml:lang="en">
      <title>Restoring Master Mirroring After a Recovery</title>
      <body>
        <p>After you activate a standby master for recovery, the standby master becomes the primary
          master. You can continue running that instance as the primary master if it has the same
          capabilities and dependability as the original master host. </p>
        <p>You must initialize a new standby master to continue providing master mirroring unless
          you have already done so while activating the prior standby master. Run
            <codeph>gpinitstandby</codeph> on the active master host to configure a new standby
          master.</p>
        <p>You may restore the primary and standby master instances on the original hosts. This
          process swaps the roles of the primary and standby master hosts, and it should be
          performed only if you strongly prefer to run the master instances on the same hosts they
          occupied prior to the recovery scenario.</p>
        <section id="ki160986">
          <title>To restore the master and standby instances on original hosts (optional)</title>
          <ol>
            <li id="ki160936">Ensure the original master host is in dependable running condition;
              ensure the cause of the original failure is fixed.</li>
            <li>On the original master host, move or remove the data directory,
                <codeph>gpseg-1</codeph>. This example moves the directory to
                <codeph>backup_gpseg-1</codeph>:<codeblock>$ mv /data/master/gpseg-1 /data/master/backup_gpseg-1</codeblock><p>You
                can remove the backup directory once the standby is successfully
              configured.</p></li>
            <li id="ki160940">Initialize a standby master on the original master host. For example,
              run this command from the current master host,
              smdw:<codeblock>$ gpinitstandby -s mdw</codeblock></li>
            <li>After the initialization completes, check the status of standby master, mdw, run
                <codeph>gpstate</codeph> with the <codeph>-f </codeph> option to check the
                status:<codeblock>$ gpstate -f</codeblock><p>The status should be <i>In
                Synch</i>.</p></li>
            <li>Stop Greenplum Database master instance on the standby master. For
              example:<codeblock>$ gpstop -m</codeblock></li>
            <li id="ki160961">Run the <codeph>gpactivatestandby</codeph> utility from the original
              master host, mdw, that is currently a standby master. For
                example:<codeblock>$ gpactivatestandby -d $MASTER_DATA_DIRECTORY</codeblock><p>Where
                the <codeph>-d</codeph> option specifies the data directory of the host you are
                activating.</p></li>
            <li id="ki165618">After the utility completes, run <codeph>gpstate</codeph> to check the
                status:<codeblock>$ gpstate -f </codeblock><p>Verify the original primary master
                status is <i>Active</i>. When a standby master is not configured, the command
                displays <codeph>-No entries found</codeph> and the message indicates that a standby
                master instance is not configured.</p></li>
            <li>On the standby master host, move or remove the data directory,
                <codeph>gpseg-1</codeph>. This example moves the
                directory:<codeblock>$ mv /data/master/gpseg-1 /data/master/backup_gpseg-1</codeblock><p>You
                can remove the backup directory once the standby is successfully
              configured.</p></li>
            <li id="ki165609">After the original master host runs the primary Greenplum Database
              master, you can initialize a standby master on the original standby master host. For
              example:<codeblock>$ gpinitstandby -s smdw</codeblock></li>
          </ol>
          <title>To check the status of the master mirroring process (optional)</title>
          <p>You can display the information in the Greenplum Database system view
            pg_stat_replication. The view lists information about the <codeph>walsender</codeph>
            process that is used for Greenplum Database master mirroring. For example, this command
            displays the process ID and state of the <codeph>walsender</codeph> process:</p>
          <codeblock>$ psql dbname -c 'SELECT pid, state FROM pg_stat_replication;'</codeblock>
        </section>
      </body>
    </topic>
  </topic>
</topic>
