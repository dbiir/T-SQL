<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Composite//EN" "ditabase.dtd">
<topic id="topic1">
  <title>Loading and Unloading Data</title>
  <shortdesc>The topics in this section describe methods for loading and writing data into and out
    of a Greenplum Database, and how to format data files.</shortdesc>
  <body>
    <p>Greenplum Database supports high-performance parallel data loading and unloading, and for
      smaller amounts of data, single file, non-parallel data import and export. </p>
    <p>Greenplum Database can read from and write to several types of external data sources,
      including text files, Hadoop file systems, Amazon S3, and web servers.<ul id="ul_umr_mlh_kr">
        <li>The <codeph>COPY</codeph> SQL command transfers data between an external text file on
          the master host, or multiple text files on segment hosts, and a Greenplum Database
          table.</li>
        <li>Readable external tables allow you to query data outside of the database directly and in
          parallel using SQL commands such as <codeph>SELECT</codeph>, <codeph>JOIN</codeph>, or
            <codeph>SORT EXTERNAL TABLE DATA</codeph>, and you can create views for external tables.
          External tables are often used to load external data into a regular database table using a
          command such as <codeph>CREATE TABLE <varname>table</varname> AS SELECT * FROM
              <varname>ext_table</varname></codeph>. </li>
        <li>External web tables provide access to dynamic data. They can be backed with data from
          URLs accessed using the HTTP protocol or by the output of an OS script running on one or
          more segments.</li>
        <li>The <codeph>gpfdist</codeph> utility is the Greenplum Database parallel file
          distribution program. It is an HTTP server that is used with external tables to allow
          Greenplum Database segments to load external data in parallel, from multiple file systems.
          You can run multiple instances of <codeph>gpfdist</codeph> on different hosts and network
          interfaces and access them in parallel. </li>
        <li>The <codeph>gpload</codeph> utility automates the steps of a load task using
            <codeph>gpfdist</codeph> and a YAML-formatted control file. </li>
        <li>You can create readable and writable external tables with the Greenplum
           Platform Extension Framework (PXF), and use these tables to load data into,
           or offload data from, Greenplum Database. For information about using
           PXF, refer to <xref href="../../external/pxf-overview.xml" format="dita" scope="peer">Accessing External Data with PXF</xref>.</li>
        <li otherprops="pivotal">The Greenplum-Kafka Integration provides high-speed, parallel
           data transfer from Kafka to Greenplum Database. For information about using
           these tools, refer to the documentation at <xref
            href="../../../greenplum-kafka/intro.xml" format="dita" scope="peer">Pivotal Greenplum-Kafka Integration</xref>.</li>
        <li otherprops="pivotal">The Greenplum-Spark Connector provides high speed, parallel data
          transfer between Pivotal Greenplum Database and Apache Spark. For information about using
          the Greenplum-Spark Connector, refer to the documentation at <xref
            href="https://greenplum-spark.docs.pivotal.io/" format="html" scope="external"
            >https://greenplum-spark.docs.pivotal.io/</xref>.</li>
        <li otherprops="pivotal">The Greenplum-Informatica Connector provides high speed data
          transfer from an Informatica PowerCenter cluster to a Pivotal Greenplum Database cluster
          for batch and streaming ETL operations. For information about using the
          Greenplum-Informatica Connector, refer to the documentation at <xref
            href="https://greenplum-informatica.docs.pivotal.io/" format="html" scope="external"
            >https://greenplum-informatica.docs.pivotal.io/</xref>.</li>
      </ul>The method you choose to load data depends on the characteristics of the source
      data&#8212;its location, size, format, and any transformations required. </p>
    <p>In the simplest case, the <codeph>COPY</codeph> SQL command loads data into a table from a
      text file that is accessible to the Greenplum Database master instance. This requires no setup
      and provides good performance for smaller amounts of data. With the <codeph>COPY</codeph>
      command, the data copied into or out of the database passes between a single file on the
      master host and the database. This limits the total size of the dataset to the capacity of the
      file system where the external file resides and limits the data transfer to a single file
      write stream. </p>
    <p>More efficient data loading options for large datasets take advantage of the Greenplum
      Database MPP architecture, using the Greenplum Database segments to load data in parallel.
      These methods allow data to load simultaneously from multiple file systems, through multiple
      NICs, on multiple hosts, achieving very high data transfer rates. External tables allow you to
      access external files from within the database as if they are regular database tables. When
      used with <codeph>gpfdist</codeph>, the Greenplum Database parallel file distribution program,
      external tables provide full parallelism by using the resources of all Greenplum Database
      segments to load or unload data. </p>
    <p>Greenplum Database leverages the parallel architecture of the Hadoop Distributed File System
      to access files on that system.</p>
  </body>
</topic>
