<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="testing">
  <title>Testing and Debugging Text Search</title>
  <shortdesc>This topic introduces the Greenplum Database functions you can use
    to test and debug a search configuration or the individual parser and
    dictionaries specified in a configuration. </shortdesc>
  <body>
    <p>The behavior of a custom text search configuration can easily become confusing. The functions
      described in this section are useful for testing text search objects. You can test a complete
      configuration, or test parsers and dictionaries separately.</p>
    <p>This section contains the following subtopics:<ul id="ul_m4h_h1x_4fb">
        <li><xref href="#testing/configuration" format="dita"/></li>
        <li><xref href="#testing/parser" format="dita"/></li>
        <li><xref href="#testing/dictionary" format="dita"/></li>
      </ul></p>
    <section id="configuration">
      <title>Configuration Testing</title>
      <p>The function <codeph>ts_debug</codeph> allows easy testing of a text
        search configuration.</p>
      <codeblock>ts_debug([<i>config</i> regconfig, ] <i>document</i> text,
         OUT <i>alias</i> text,
         OUT <i>description</i> text,
         OUT <i>token</i> text,
         OUT <i>dictionaries</i> regdictionary[],
         OUT <i>dictionary</i> regdictionary,
         OUT <i>lexemes</i> text[])
         returns setof record</codeblock>
      <p><codeph>ts_debug</codeph> displays information about every token of
            <codeph><i>document</i></codeph> as produced by the parser and
        processed by the configured dictionaries. It uses the configuration
        specified by <codeph><i>config</i></codeph>, or
          <codeph>default_text_search_config</codeph> if that argument is
        omitted.</p>
      <p><codeph>ts_debug</codeph> returns one row for each token identified in
        the text by the parser. The columns returned are</p>
      <ul id="ul_uwp_yg1_lfb">
        <li><codeph><i>alias</i> text</codeph> — short name of the token
          type</li>
        <li><codeph><i>description</i> text</codeph> — description of the token
          type</li>
        <li><codeph><i>token</i> text </codeph>— text of the token</li>
        <li><codeph><i>dictionaries</i> regdictionary[]</codeph> — the
          dictionaries selected by the configuration for this token type</li>
        <li><codeph><i>dictionary</i> regdictionary</codeph> — the dictionary
          that recognized the token, or <codeph>NULL</codeph> if none did</li>
        <li><codeph><i>lexemes</i> text[]</codeph> — the lexeme(s) produced by
          the dictionary that recognized the token, or <codeph>NULL</codeph> if
          none did; an empty array (<codeph>{}</codeph>) means it was recognized
          as a stop word</li>
      </ul>
      <p>Here is a simple example:</p>
      <codeblock>SELECT * FROM ts_debug('english','a fat  cat sat on a mat - it ate a fat rats');
   alias   |   description   | token |  dictionaries  |  dictionary  | lexemes 
-----------+-----------------+-------+----------------+--------------+---------
 asciiword | Word, all ASCII | a     | {english_stem} | english_stem | {}
 blank     | Space symbols   |       | {}             |              | 
 asciiword | Word, all ASCII | fat   | {english_stem} | english_stem | {fat}
 blank     | Space symbols   |       | {}             |              | 
 asciiword | Word, all ASCII | cat   | {english_stem} | english_stem | {cat}
 blank     | Space symbols   |       | {}             |              | 
 asciiword | Word, all ASCII | sat   | {english_stem} | english_stem | {sat}
 blank     | Space symbols   |       | {}             |              | 
 asciiword | Word, all ASCII | on    | {english_stem} | english_stem | {}
 blank     | Space symbols   |       | {}             |              | 
 asciiword | Word, all ASCII | a     | {english_stem} | english_stem | {}
 blank     | Space symbols   |       | {}             |              | 
 asciiword | Word, all ASCII | mat   | {english_stem} | english_stem | {mat}
 blank     | Space symbols   |       | {}             |              | 
 blank     | Space symbols   | -     | {}             |              | 
 asciiword | Word, all ASCII | it    | {english_stem} | english_stem | {}
 blank     | Space symbols   |       | {}             |              | 
 asciiword | Word, all ASCII | ate   | {english_stem} | english_stem | {ate}
 blank     | Space symbols   |       | {}             |              | 
 asciiword | Word, all ASCII | a     | {english_stem} | english_stem | {}
 blank     | Space symbols   |       | {}             |              | 
 asciiword | Word, all ASCII | fat   | {english_stem} | english_stem | {fat}
 blank     | Space symbols   |       | {}             |              | 
 asciiword | Word, all ASCII | rats  | {english_stem} | english_stem | {rat}</codeblock>
      <p>For a more extensive demonstration, we first create a
          <codeph>public.english</codeph> configuration and Ispell dictionary
        for the English language:</p>
      <codeblock>CREATE TEXT SEARCH CONFIGURATION public.english ( COPY = pg_catalog.english );

CREATE TEXT SEARCH DICTIONARY english_ispell (
    TEMPLATE = ispell,
    DictFile = english,
    AffFile = english,
    StopWords = english
);

ALTER TEXT SEARCH CONFIGURATION public.english
   ALTER MAPPING FOR asciiword WITH english_ispell, english_stem;</codeblock>
      <codeblock>SELECT * FROM ts_debug('public.english','The Brightest supernovaes');
   alias   |   description   |    token    |         dictionaries          |   dictionary   |   lexemes   
-----------+-----------------+-------------+-------------------------------+----------------+-------------
 asciiword | Word, all ASCII | The         | {english_ispell,english_stem} | english_ispell | {}
 blank     | Space symbols   |             | {}                            |                | 
 asciiword | Word, all ASCII | Brightest   | {english_ispell,english_stem} | english_ispell | {bright}
 blank     | Space symbols   |             | {}                            |                | 
 asciiword | Word, all ASCII | supernovaes | {english_ispell,english_stem} | english_stem   | {supernova}</codeblock>
      <p>In this example, the word <codeph>Brightest</codeph> was recognized by
        the parser as an <codeph>ASCII</codeph> word (alias
          <codeph>asciiword</codeph>). For this token type the dictionary list
        is <codeph>english_ispell</codeph> and <codeph>english_stem</codeph>.
        The word was recognized by <codeph>english_ispell</codeph>, which
        reduced it to the noun <codeph>bright</codeph>. The word
          <codeph>supernovaes</codeph> is unknown to the
          <codeph>english_ispell</codeph> dictionary so it was passed to the
        next dictionary, and, fortunately, was recognized (in fact,
          <codeph>english_stem</codeph> is a Snowball dictionary which
        recognizes everything; that is why it was placed at the end of the
        dictionary list).</p>
      <p>The word <codeph>The</codeph> was recognized by the
          <codeph>english_ispell</codeph> dictionary as a stop word (<xref
          href="dictionaries.xml#dictionaries/stop-words"/>) and will not be
        indexed. The spaces are discarded too, since the configuration provides
        no dictionaries at all for them.</p>
      <p>You can reduce the width of the output by explicitly specifying which
        columns you want to see:</p>
      <codeblock>SELECT alias, token, dictionary, lexemes FROM ts_debug('public.english','The Brightest supernovaes'); 
  alias    |    token    |   dictionary   |    lexemes 
-----------+-------------+----------------+------------- 
 asciiword | The         | english_ispell | {} 
 blank     |             |                | 
 asciiword | Brightest   | english_ispell | {bright} 
 blank     |             |                | 
 asciiword | supernovaes | english_stem   | {supernova}</codeblock>
    </section>
    <section id="parser">
      <title>Parser Testing</title>
      <p>The following functions allow direct testing of a text search
        parser.</p>
      <codeblock>ts_parse(<i>parser_name</i> text, <i>document</i> text,
         OUT <i>tokid</i> integer, OUT <i>token</i> text) returns setof record
ts_parse(<i>parser_oid</i> oid, <i>document</i> text,
         OUT <i>tokid</i> integer, OUT <i>token</i> text) returns setof record</codeblock>
      <p><codeph>ts_parse</codeph> parses the given document and returns a
        series of records, one for each token produced by parsing. Each record
        includes a <codeph>tokid</codeph> showing the assigned token type and a
          <codeph>token</codeph>, which is the text of the token. For
        example:</p>
      <codeblock>SELECT * FROM ts_parse('default', '123 - a number');
 tokid | token
-------+--------
    22 | 123
    12 |
    12 | -
     1 | a
    12 |
     1 | number</codeblock>
      <pre>ts_token_type(<i>parser_name</i> text, OUT <i>tokid</i> integer,
              OUT <i>alias</i> text, OUT <i>description</i> text) returns setof record
ts_token_type(<i>parser_oid</i> oid, OUT <i>tokid</i> integer,
              OUT <i>alias</i> text, OUT <i>description</i> text) returns setof record</pre>
      <p><codeph>ts_token_type</codeph> returns a table which describes each
        type of token the specified parser can recognize. For each token type,
        the table gives the integer <codeph>tokid</codeph> that the parser uses
        to label a token of that type, the <codeph>alias</codeph> that names the
        token type in configuration commands, and a short
          <codeph>description</codeph>. For example:</p>
      <codeblock>SELECT * FROM ts_token_type('default');
 tokid |      alias      |               description                
-------+-----------------+------------------------------------------
     1 | asciiword       | Word, all ASCII
     2 | word            | Word, all letters
     3 | numword         | Word, letters and digits
     4 | email           | Email address
     5 | url             | URL
     6 | host            | Host
     7 | sfloat          | Scientific notation
     8 | version         | Version number
     9 | hword_numpart   | Hyphenated word part, letters and digits
    10 | hword_part      | Hyphenated word part, all letters
    11 | hword_asciipart | Hyphenated word part, all ASCII
    12 | blank           | Space symbols
    13 | tag             | XML tag
    14 | protocol        | Protocol head
    15 | numhword        | Hyphenated word, letters and digits
    16 | asciihword      | Hyphenated word, all ASCII
    17 | hword           | Hyphenated word, all letters
    18 | url_path        | URL path
    19 | file            | File or path name
    20 | float           | Decimal notation
    21 | int             | Signed integer
    22 | uint            | Unsigned integer
    23 | entity          | XML entity</codeblock>
    </section>
    <section id="dictionary">
      <title>Dictionary Testing</title>
      <p>The <codeph>ts_lexize</codeph> function facilitates dictionary
        testing.</p>
      <pre>ts_lexize(<i>dictreg</i> dictionary, <i>token</i> text) returns text[]</pre>
      <p><codeph>ts_lexize</codeph> returns an array of lexemes if the input
            <codeph><i>token</i></codeph> is known to the dictionary, or an
        empty array if the token is known to the dictionary but it is a stop
        word, or <codeph>NULL</codeph> if it is an unknown word.</p>
      <p>Examples:</p>
      <codeblock>SELECT ts_lexize('english_stem', 'stars');
 ts_lexize
-----------
 {star}

SELECT ts_lexize('english_stem', 'a');
 ts_lexize
-----------
 {}</codeblock>
      <note>The <codeph>ts_lexize</codeph> function expects a single token, not
        text. Here is a case where this can be
          confusing:<codeblock>SELECT ts_lexize('thesaurus_astro','supernovae stars') is null;
 ?column?
----------
 t</codeblock><p>The
          thesaurus dictionary <codeph>thesaurus_astro</codeph> does know the
          phrase <codeph>supernovae stars</codeph>, but
            <codeph>ts_lexize</codeph> fails since it does not parse the input
          text but treats it as a single token. Use
            <codeph>plainto_tsquery</codeph> or <codeph>to_tsvector</codeph> to
          test thesaurus dictionaries, for
        example:</p><codeblock>SELECT plainto_tsquery('supernovae stars');
 plainto_tsquery
-----------------
 'sn'</codeblock></note>
    </section>
  </body>
</topic>
