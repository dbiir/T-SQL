<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="controlling">
  <title>Controlling Text Search</title>
  <shortdesc>This topic shows how to create search and query vectors, how to
    rank search results, and how to highlight search terms in the results of
    text search queries. </shortdesc>
  <body>
    <p>To implement full text searching there must be a function to create a
        <codeph>tsvector</codeph> from a document and a <codeph>tsquery</codeph> from a user query.
      Also, we need to return results in a useful order, so we need a function that compares
      documents with respect to their relevance to the query. It's also important to be able to
      display the results nicely. Greenplum Database provides support for all of these
      functions.</p>
    <p>This topic contains the following subtopics:</p>
    <ul id="ul_pnw_xxw_4fb">
      <li><xref href="#controlling/parsing-documents" format="dita"/></li>
      <li><xref href="#controlling/parsing-queries" format="dita"/></li>
      <li><xref href="#controlling/ranking" format="dita"/></li>
      <li><xref href="#controlling/highlighting" format="dita"/></li>
    </ul>
    <section id="parsing-documents">
      <title>Parsing Documents</title>
      <p>Greenplum Database provides the function <codeph>to_tsvector</codeph> for converting a
        document to the <codeph>tsvector</codeph> data type.</p>
      <codeblock>to_tsvector([<i>config</i> regconfig, ] <i>document</i> text) returns tsvector</codeblock>
      <p><codeph>to_tsvector</codeph> parses a textual document into tokens, reduces the tokens to
        lexemes, and returns a <codeph>tsvector</codeph> which lists the lexemes together with their
        positions in the document. The document is processed according to the specified or default
        text search configuration. Here is a simple example:</p>
      <codeblock>SELECT to_tsvector('english', 'a fat  cat sat on a mat - it ate a fat rats');
                  to_tsvector
-----------------------------------------------------
 'ate':9 'cat':3 'fat':2,11 'mat':7 'rat':12 'sat':4</codeblock>
      <p>In the example above we see that the resulting tsvector does not contain the words
          <codeph>a</codeph>, <codeph>on</codeph>, or <codeph>it</codeph>, the word
          <codeph>rats</codeph> became <codeph>rat</codeph>, and the punctuation sign
          <codeph>-</codeph> was ignored.</p>
      <p>The <codeph>to_tsvector</codeph> function internally calls a parser
        which breaks the document text into tokens and assigns a type to each
        token. For each token, a list of dictionaries (<xref
          href="dictionaries.xml#dictionaries"/>) is consulted, where the list
        can vary depending on the token type. The first dictionary that
          <i>recognizes</i> the token emits one or more normalized
          <i>lexemes</i> to represent the token. For example,
          <codeph>rats</codeph> became <codeph>rat</codeph> because one of the
        dictionaries recognized that the word <codeph>rats</codeph> is a plural
        form of <codeph>rat</codeph>. Some words are recognized as <i>stop
          words</i>, which causes them to be ignored since they occur too
        frequently to be useful in searching. In our example these are
          <codeph>a</codeph>, <codeph>on</codeph>, and <codeph>it</codeph>. If
        no dictionary in the list recognizes the token then it is also ignored.
        In this example that happened to the punctuation sign <codeph>-</codeph>
        because there are in fact no dictionaries assigned for its token type
          (<codeph>Space symbols</codeph>), meaning space tokens will never be
        indexed. The choices of parser, dictionaries and which types of tokens
        to index are determined by the selected text search configuration (<xref
          href="configuration.xml#configuration"/>). It is possible to have many
        different configurations in the same database, and predefined
        configurations are available for various languages. In our example we
        used the default configuration <codeph>english</codeph> for the English
        language.</p>
      <p>The function <codeph>setweight</codeph> can be used to label the entries of a
          <codeph>tsvector</codeph> with a given <i>weight</i>, where a weight is one of the letters
          <codeph>A</codeph>, <codeph>B</codeph>, <codeph>C</codeph>, or <codeph>D</codeph>. This is
        typically used to mark entries coming from different parts of a document, such as
          <codeph>title</codeph> versus <codeph>body</codeph>. Later, this information can be used
        for ranking of search results.</p>
      <p>Because <codeph>to_tsvector(NULL)</codeph> will return <codeph>NULL</codeph>, it is
        recommended to use <codeph>coalesce</codeph> whenever a field might be null. Here is the
        recommended method for creating a tsvector from a structured document:</p>
      <codeblock>UPDATE tt SET ti = setweight(to_tsvector(coalesce(title,'')), 'A') 
  || setweight(to_tsvector(coalesce(keyword,'')), 'B') 
  || setweight(to_tsvector(coalesce(abstract,'')), 'C') 
  || setweight(to_tsvector(coalesce(body,'')), 'D');</codeblock>
      <p id="p_xn1_lqb_lfb">Here we have used <codeph>setweight</codeph> to
        label the source of each lexeme in the finished
          <codeph>tsvector</codeph>, and then merged the labeled
          <codeph>tsvector</codeph> values using the <codeph>tsvector</codeph>
        concatenation operator <codeph>||</codeph>. (<xref
          href="features.xml#features"/> gives details about these
        operations.)</p>
    </section>
    <section id="parsing-queries">
      <title>Parsing Queries</title>
      <p>Greenplum Database provides the functions <codeph>to_tsquery</codeph> and
          <codeph>plainto_tsquery</codeph> for converting a query to the <codeph>tsquery</codeph>
        data type. <codeph>to_tsquery</codeph> offers access to more features than
          <codeph>plainto_tsquery</codeph>, but is less forgiving about its input.</p>
      <codeblock>to_tsquery([<i>config</i> regconfig, ] <i>querytext</i> text) returns tsquery</codeblock>
      <p><codeph>to_tsquery</codeph> creates a <codeph>tsquery</codeph> value from <i>querytext</i>,
        which must consist of single tokens separated by the Boolean operators
          <codeph>&amp;</codeph> (AND), <codeph>|</codeph> (OR), and <codeph>!</codeph>(NOT). These
        operators can be grouped using parentheses. In other words, the input to
          <codeph>to_tsquery</codeph> must already follow the general rules for
          <codeph>tsquery</codeph> input, as described in <xref
          href="../../ref_guide/datatype-textsearch.xml"/>. The difference is that while basic
          <codeph>tsquery</codeph> input takes the tokens at face value, <codeph>to_tsquery</codeph>
        normalizes each token to a lexeme using the specified or default configuration, and discards
        any tokens that are stop words according to the configuration. For example:</p>
      <codeblock>SELECT to_tsquery('english', 'The &amp; Fat &amp; Rats');
  to_tsquery   
---------------
 'fat' &amp; 'rat'</codeblock>
      <p>As in basic <codeph>tsquery</codeph> input, weight(s) can be attached to each lexeme to
        restrict it to match only <codeph>tsvector</codeph> lexemes of those weight(s). For
        example:</p>
      <codeblock>SELECT to_tsquery('english', 'Fat | Rats:AB');
    to_tsquery    
------------------
 'fat' | 'rat':AB</codeblock>
      <p>Also, <codeph>*</codeph> can be attached to a lexeme to specify prefix matching:</p>
      <codeblock>SELECT to_tsquery('supern:*A &amp; star:A*B');
        to_tsquery        
--------------------------
 'supern':*A &amp; 'star':*AB</codeblock>
      <p>Such a lexeme will match any word in a <codeph>tsvector</codeph> that begins with the given
        string.</p>
      <p><codeph>to_tsquery</codeph> can also accept single-quoted phrases. This is primarily useful
        when the configuration includes a thesaurus dictionary that may trigger on such phrases. In
        the example below, a thesaurus contains the rule <codeph>supernovae stars : sn</codeph>:</p>
      <codeblock>SELECT to_tsquery('''supernovae stars'' &amp; !crab');
  to_tsquery
---------------
 'sn' &amp; !'crab'</codeblock>
      <p>Without quotes, <codeph>to_tsquery</codeph> will generate a syntax error for tokens that
        are not separated by an AND or OR operator.</p>
      <codeblock>plainto_tsquery([ <i>config</i> regconfig, ] <i>querytext</i> ext) returns tsquery</codeblock>
      <p><codeph>plainto_tsquery</codeph> transforms unformatted text
          <codeph><i>querytext</i></codeph> to <codeph>tsquery</codeph>. The text is parsed and
        normalized much as for <codeph>to_tsvector</codeph>, then the <codeph>&amp;</codeph> (AND)
        Boolean operator is inserted between surviving words.</p>
      <p>Example:</p>
      <codeblock>SELECT plainto_tsquery('english', 'The Fat Rats');
 plainto_tsquery 
-----------------
 'fat' &amp; 'rat'</codeblock>
      <p>Note that <codeph>plainto_tsquery</codeph> cannot recognize Boolean operators, weight
        labels, or prefix-match labels in its input:</p>
      <codeblock>SELECT plainto_tsquery('english', 'The Fat &amp; Rats:C');
   plainto_tsquery   
---------------------
 'fat' &amp; 'rat' &amp; 'c'</codeblock>
      <p>Here, all the input punctuation was discarded as being space symbols.</p>
    </section>
    <section id="ranking">
      <title>Ranking Search Results</title>
      <p>Ranking attempts to measure how relevant documents are to a particular
        query, so that when there are many matches the most relevant ones can be
        shown first. Greenplum Database provides two predefined ranking
        functions, which take into account lexical, proximity, and structural
        information; that is, they consider how often the query terms appear in
        the document, how close together the terms are in the document, and how
        important is the part of the document where they occur. However, the
        concept of relevancy is vague and very application-specific. Different
        applications might require additional information for ranking, e.g.,
        document modification time. The built-in ranking functions are only
        examples. You can write your own ranking functions and/or combine their
        results with additional factors to fit your specific needs.</p>
      <p>The two ranking functions currently available are:</p>
      <dl>
        <dlentry>
          <dt><codeph>ts_rank([ <i>weights</i> float4[], ] <i>vector</i>
              tsvector, <i>query</i> tsquery [, <i>normalization</i> integer ])
              returns float4</codeph></dt>
          <dd>
            <p>Ranks vectors based on the frequency of their matching
              lexemes.</p>
          </dd>
        </dlentry>
        <dlentry>
          <dt><codeph>ts_rank_cd([ <i>weights</i> float4[], ] <i>vector</i>
              tsvector, <i>query</i> tsquery [, <i>normalization</i> integer ])
              returns float4</codeph></dt>
          <dd>
            <p>This function computes the <i>cover density</i> ranking for the
              given document vector and query, as described in Clarke, Cormack,
              and Tudhope's "Relevance Ranking for One to Three Term Queries" in
              the journal "Information Processing and Management", 1999. Cover
              density is similar to <codeph>ts_rank</codeph> ranking except that
              the proximity of matching lexemes to each other is taken into
              consideration.</p>
            <p>This function requires lexeme positional information to perform
              its calculation. Therefore, it ignores any "stripped" lexemes in
              the <codeph>tsvector</codeph>. If there are no unstripped lexemes
              in the input, the result will be zero. (See <xref
                href="features.xml#features/manipulating-documents"/> for more
              information about the <codeph>strip</codeph> function and
              positional information in <codeph>tsvector</codeph>s.)</p>
          </dd>
        </dlentry>
      </dl>
      <p>For both these functions, the optional <codeph><i>weights</i></codeph>
        argument offers the ability to weigh word instances more or less heavily
        depending on how they are labeled. The weight arrays specify how heavily
        to weigh each category of word, in the order:</p>
      <codeblock>{D-weight, C-weight, B-weight, A-weight}</codeblock>
      <p>If no <codeph><i>weights</i></codeph> are provided, then these defaults
        are used:</p>
      <codeblock>{0.1, 0.2, 0.4, 1.0}</codeblock>
      <p>Typically weights are used to mark words from special areas of the
        document, like the title or an initial abstract, so they can be treated
        with more or less importance than words in the document body.</p>
      <p>Since a longer document has a greater chance of containing a query term
        it is reasonable to take into account document size, e.g., a
        hundred-word document with five instances of a search word is probably
        more relevant than a thousand-word document with five instances. Both
        ranking functions take an integer <codeph><i>normalization</i></codeph>
        option that specifies whether and how a document's length should impact
        its rank. The integer option controls several behaviors, so it is a bit
        mask: you can specify one or more behaviors using <codeph>|</codeph>
        (for example, <codeph>2|4</codeph>).</p>
      <ul id="ul_zbv_y25_kfb">
        <li>0 (the default) ignores the document length</li>
        <li>1 divides the rank by 1 + the logarithm of the document length</li>
        <li>2 divides the rank by the document length</li>
        <li>4 divides the rank by the mean harmonic distance between extents
          (this is implemented only by <codeph>ts_rank_cd</codeph>)</li>
        <li>8 divides the rank by the number of unique words in document</li>
        <li>16 divides the rank by 1 + the logarithm of the number of unique
          words in document</li>
        <li>32 divides the rank by itself + 1</li>
      </ul>
      <p>If more than one flag bit is specified, the transformations are applied
        in the order listed.</p>
      <p>It is important to note that the ranking functions do not use any
        global information, so it is impossible to produce a fair normalization
        to 1% or 100% as sometimes desired. Normalization option <codeph>32
          (rank/(rank+1))</codeph> can be applied to scale all ranks into the
        range zero to one, but of course this is just a cosmetic change; it will
        not affect the ordering of the search results.</p>
      <p>Here is an example that selects only the ten highest-ranked
        matches:</p>
      <codeblock>SELECT title, ts_rank_cd(textsearch, query) AS rank
FROM apod, to_tsquery('neutrino|(dark &amp; matter)') query
WHERE query @@ textsearch
ORDER BY rank DESC
LIMIT 10;
                     title                     |   rank
-----------------------------------------------+----------
 Neutrinos in the Sun                          |      3.1
 The Sudbury Neutrino Detector                 |      2.4
 A MACHO View of Galactic Dark Matter          |  2.01317
 Hot Gas and Dark Matter                       |  1.91171
 The Virgo Cluster: Hot Plasma and Dark Matter |  1.90953
 Rafting for Solar Neutrinos                   |      1.9
 NGC 4650A: Strange Galaxy and Dark Matter     |  1.85774
 Hot Gas and Dark Matter                       |   1.6123
 Ice Fishing for Cosmic Neutrinos              |      1.6
 Weak Lensing Distorts the Universe            | 0.818218</codeblock>
      <p>This is the same example using normalized ranking:</p>
      <codeblock>SELECT title, ts_rank_cd(textsearch, query, 32 /* rank/(rank+1) */ ) AS rank
FROM apod, to_tsquery('neutrino|(dark &amp; matter)') query
WHERE  query @@ textsearch
ORDER BY rank DESC
LIMIT 10;
                     title                     |        rank
-----------------------------------------------+-------------------
 Neutrinos in the Sun                          | 0.756097569485493
 The Sudbury Neutrino Detector                 | 0.705882361190954
 A MACHO View of Galactic Dark Matter          | 0.668123210574724
 Hot Gas and Dark Matter                       |  0.65655958650282
 The Virgo Cluster: Hot Plasma and Dark Matter | 0.656301290640973
 Rafting for Solar Neutrinos                   | 0.655172410958162
 NGC 4650A: Strange Galaxy and Dark Matter     | 0.650072921219637
 Hot Gas and Dark Matter                       | 0.617195790024749
 Ice Fishing for Cosmic Neutrinos              | 0.615384618911517
 Weak Lensing Distorts the Universe            | 0.450010798361481</codeblock>
      <p>Ranking can be expensive since it requires consulting the tsvector of
        each matching document, which can be I/O bound and therefore slow.
        Unfortunately, it is almost impossible to avoid since practical queries
        often result in large numbers of matches.</p>
    </section>
    <section id="highlighting">
      <title>Highlighting Results</title>
      <p>To present search results it is ideal to show a part of each document
        and how it is related to the query. Usually, search engines show
        fragments of the document with marked search terms. Greenplum Database
        provides a function <codeph>ts_headline</codeph> that implements this
        functionality.</p>
      <codeblock>ts_headline([<i>config</i> regconfig, ] <i>document</i> text, <i>query</i> tsquery [, <i>options</i> text ]) returns text</codeblock>
      <p><codeph>ts_headline</codeph> accepts a document along with a query, and
        returns an excerpt from the document in which terms from the query are
        highlighted. The configuration to be used to parse the document can be
        specified by <codeph><i>config</i></codeph>; if
          <codeph><i>config</i></codeph> is omitted, the
          <codeph>default_text_search_config</codeph> configuration is used.</p>
      <p>If an <codeph><i>options</i></codeph> string is specified it must
        consist of a comma-separated list of one or more
            <codeph><i>option=value</i></codeph> pairs. The available options
        are:</p>
      <ul id="ul_ndr_bh5_kfb">
        <li><codeph>StartSel</codeph>, <codeph>StopSel</codeph>: the strings
          with which to delimit query words appearing in the document, to
          distinguish them from other excerpted words. You must double-quote
          these strings if they contain spaces or commas.</li>
        <li><codeph>MaxWords</codeph>, <codeph>MinWords</codeph>: these numbers
          determine the longest and shortest headlines to output.</li>
        <li><codeph>ShortWord</codeph>: words of this length or less will be
          dropped at the start and end of a headline. The default value of three
          eliminates common English articles.</li>
        <li><codeph>HighlightAll</codeph>: Boolean flag; if
            <codeph>true</codeph> the whole document will be used as the
          headline, ignoring the preceding three parameters.</li>
        <li><codeph>MaxFragments</codeph>: maximum number of text excerpts or
          fragments to display. The default value of zero selects a
          non-fragment-oriented headline generation method. A value greater than
          zero selects fragment-based headline generation. This method finds
          text fragments with as many query words as possible and stretches
          those fragments around the query words. As a result query words are
          close to the middle of each fragment and have words on each side. Each
          fragment will be of at most <codeph>MaxWords</codeph> and words of
          length <codeph>ShortWord</codeph> or less are dropped at the start and
          end of each fragment. If not all query words are found in the
          document, then a single fragment of the first
            <codeph>MinWords</codeph> in the document will be displayed.</li>
        <li><codeph>FragmentDelimiter</codeph>: When more than one fragment is
          displayed, the fragments will be separated by this string.</li>
      </ul>
      <p>Any unspecified options receive these defaults:</p>
      <codeblock>StartSel=&lt;b>, StopSel=&lt;/b>,
MaxWords=35, MinWords=15, ShortWord=3, HighlightAll=FALSE,
MaxFragments=0, FragmentDelimiter=" ... "</codeblock>
      <p>For example:</p>
      <codeblock>SELECT ts_headline('english',
  'The most common type of search
is to find all documents containing given query terms
and return them in order of their similarity to the
query.',
  to_tsquery('query &amp; similarity'));
                        ts_headline                         
------------------------------------------------------------
 containing given &lt;b>query&lt;/b> terms
 and return them in order of their &lt;b>similarity&lt;/b> to the
 &lt;b>query&lt;/b>.

SELECT ts_headline('english',
  'The most common type of search
is to find all documents containing given query terms
and return them in order of their similarity to the
query.',
  to_tsquery('query &amp; similarity'),
  'StartSel = &lt;, StopSel = >');
                      ts_headline                      
-------------------------------------------------------
 containing given &lt;query> terms
 and return them in order of their &lt;similarity> to the
 &lt;query>.</codeblock>
      <p><codeph>ts_headline</codeph> uses the original document, not a
          <codeph>tsvector</codeph> summary, so it can be slow and should be
        used with care. A typical mistake is to call
          <codeph>ts_headline</codeph> for <i>every</i> matching document when
        only ten documents are to be shown. SQL subqueries can help; here is an
        example:</p>
      <codeblock>SELECT id, ts_headline(body, q), rank
FROM (SELECT id, body, q, ts_rank_cd(ti, q) AS rank
      FROM apod, to_tsquery('stars') q
      WHERE ti @@ q
      ORDER BY rank DESC
      LIMIT 10) AS foo;</codeblock>
    </section>
  </body>
</topic>
