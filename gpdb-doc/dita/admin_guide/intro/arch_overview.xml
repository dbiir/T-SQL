<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Composite//EN" "ditabase.dtd">
<topic id="topic2" xml:lang="en">
  <title id="iw157419">About the Greenplum Architecture</title>
  <shortdesc>Greenplum Database is a massively parallel processing (MPP) database server with an
    architecture specially designed to manage large-scale analytic data warehouses and business
    intelligence workloads. </shortdesc>
  <body>
    <p id="iw157381">MPP (also known as a <i>shared nothing</i> architecture) refers to systems with
      two or more processors that cooperate to carry out an operation, each processor with its own
      memory, operating system and disks. Greenplum uses this high-performance system architecture
      to distribute the load of multi-terabyte data warehouses, and can use all of a system's
      resources in parallel to process a query.</p>
    <p>Greenplum Database is based on PostgreSQL open-source technology. It is essentially several
      PostgreSQL disk-oriented database instances acting together as one cohesive database
      management system (DBMS). It is based on PostgreSQL 9.4, and in most cases is very similar
      to PostgreSQL with regard to SQL support, features, configuration options, and end-user
      functionality. Database users interact with Greenplum Database as they would with a regular
      PostgreSQL DBMS. </p>
    <p>Greenplum Database can use the append-optimized (AO) storage format for bulk loading and
      reading of data, and provides performance advantages over HEAP tables. Append-optimized
      storage provides checksums for data protection, compression and row/column orientation. Both
      row-oriented or column-oriented append-optimized tables can be compressed. </p>
    <p>The main differences between Greenplum Database and PostgreSQL are as follows:<ul
        id="ul_tnk_ymb_c1b">
        <li>GPORCA is leveraged for query planning, in addition to the Postgres Planner.</li>
        <li>Greenplum Database can use append-optimized storage.</li>
        <li>Greenplum Database has the option to use column storage, data that is logically
          organized as a table, using rows and columns that are physically stored in a
          column-oriented format, rather than as rows. Column storage can only be used with
          append-optimized tables. Column storage is compressible. It also can provide performance
          improvements as you only need to return the columns of interest to you. All compression
          algorithms can be used with either row or column-oriented tables, but Run-Length Encoded
          (RLE) compression can only be used with column-oriented tables. Greenplum Database
          provides compression on all Append-Optimized tables that use column storage.</li>
      </ul></p>
    <p>The internals of PostgreSQL have been modified or supplemented to support the parallel
      structure of Greenplum Database. For example, the system catalog, optimizer, query executor,
      and transaction manager components have been modified and enhanced to be able to execute
      queries simultaneously across all of the parallel PostgreSQL database instances. The Greenplum
        <i>interconnect</i> (the networking layer) enables communication between the distinct
      PostgreSQL instances and allows the system to behave as one logical database. </p>
    <p>Greenplum Database also can use declarative partitions and sub-partitions to implicitly
      generate partition constraints.</p>
    <p>Greenplum Database also includes features designed to optimize PostgreSQL for business
      intelligence (BI) workloads. For example, Greenplum has added parallel data loading (external
      tables), resource management, query optimizations, and storage enhancements, which are not
      found in standard PostgreSQL. Many features and optimizations developed by Greenplum make
      their way into the PostgreSQL community. For example, table partitioning is a feature first
      developed by Greenplum, and it is now in standard PostgreSQL.</p>
    <p>Greenplum Database queries use a Volcano-style query engine model, where the execution engine
      takes an execution plan and uses it to generate a tree of physical operators, evaluates tables
      through physical operators, and delivers results in a query response.</p>
    <p>Greenplum Database stores and processes large amounts of data by distributing the data and
      processing workload across several servers or <i>hosts</i>. Greenplum Database is an
        <i>array</i> of individual databases based upon PostgreSQL 9.4 working together to present a
      single database image. The <i>master</i> is the entry point to the Greenplum Database system.
      It is the database instance to which clients connect and submit SQL statements. The master
      coordinates its work with the other database instances in the system, called <i>segments</i>,
      which store and process the data.</p>
    <fig id="iw157440">
      <title>High-Level Greenplum Database Architecture</title>
      <image height="316px" href="../graphics/highlevel_arch.jpg" placement="break" width="397px"/>
    </fig>
    <p>The following topics describe the components that make up a Greenplum Database system and how
      they work together. <ul id="ul_cz4_xhy_dq" otherprops="op-help">
        <!-- hack for HTML/PDF -->
        <li><xref href="#arch_master" format="dita"/></li>
        <li><xref href="#arch_segments" format="dita"/></li>
        <li><xref href="#arch_interconnect" format="dita"/></li>
      </ul></p>
  </body>
  <topic id="arch_master" xml:lang="en">
    <title id="iw157474">About the Greenplum Master</title>
    <shortdesc>The Greenplum Database master is the entry to the Greenplum Database system,
      accepting client connections and SQL queries, and distributing work to the segment instances. </shortdesc>
    <body>
      <p>Greenplum Database end-users interact with Greenplum Database (through the master) as they
        would with a typical PostgreSQL database. They connect to the database using client programs
        such as <codeph>psql</codeph> or application programming interfaces (APIs) such as JDBC,
        ODBC or <xref href="https://www.postgresql.org/docs/9.4/libpq.html" format="html"
          scope="external">libpq</xref> (the PostgreSQL C API).</p>
      <p>The master is where the <i>global system catalog</i> resides. The global system catalog is
        the set of system tables that contain metadata about the Greenplum Database system itself.
        The master does not contain any user data; data resides only on the <i>segments</i>. The
        master authenticates client connections, processes incoming SQL commands, distributes
        workloads among segments, coordinates the results returned by each segment, and presents the
        final results to the client program.</p>
      <p>Greenplum Database uses Write-Ahead Logging (WAL) for master/standby master mirroring. In
        WAL-based logging, all modifications are written to the log before being applied, to ensure
        data integrity for any in-process operations.<note>WAL logging is not yet available for
          segment mirroring.</note>
      </p>
    </body>
  </topic>
  <topic id="arch_segments" xml:lang="en">
    <title id="iw157496">About the Greenplum Segments</title>
    <shortdesc>Greenplum Database segment instances are independent PostgreSQL databases that each
      store a portion of the data and perform the majority of query processing. </shortdesc>
    <body>
      <p>When a user connects to the database via the Greenplum master and issues a query, processes
        are created in each segment database to handle the work of that query. For more information
        about query processes, see <xref href="../query/topics/parallel-proc.xml#topic1"/>.</p>
      <p>User-defined tables and their indexes are distributed across the available segments in a
        Greenplum Database system; each segment contains a distinct portion of data. The database
        server processes that serve segment data run under the corresponding segment instances.
        Users interact with segments in a Greenplum Database system through the master.</p>
      <p>Segments run on a servers called <i>segment hosts</i>. A segment host typically executes
        from two to eight Greenplum segments, depending on the CPU cores, RAM, storage, network
        interfaces, and workloads. Segment hosts are expected to be identically configured. The key
        to obtaining the best performance from Greenplum Database is to distribute data and
        workloads <i>evenly</i> across a large number of equally capable segments so that all
        segments begin working on a task simultaneously and complete their work at the same time.
      </p>
    </body>
  </topic>
  <topic id="arch_interconnect" xml:lang="en">
    <title id="iw157520">About the Greenplum Interconnect</title>
    <shortdesc>The interconnect is the networking layer of the Greenplum Database
      architecture.</shortdesc>
    <body>
      <p>The <i>interconnect</i> refers to the inter-process communication between segments and the
        network infrastructure on which this communication relies. The Greenplum interconnect uses a
        standard Ethernet switching fabric. For performance reasons, a 10-Gigabit system, or faster,
        is recommended.</p>
      <p>By default, the interconnect uses User Datagram Protocol with flow control (UDPIFC) for
        interconnect traffic to send messages over the network. The Greenplum software performs
        packet verification beyond what is provided by UDP. This means the reliability is equivalent
        to Transmission Control Protocol (TCP), and the performance and scalability exceeds TCP. If
        the interconnect is changed to TCP, Greenplum Database has a scalability limit of 1000
        segment instances. With UDPIFC as the default protocol for the interconnect, this limit is
        not applicable.</p>
    </body>
  </topic>
</topic>
