<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Composite//EN" "ditabase.dtd">
<topic id="topic1" xml:lang="en">
  <title id="ji162018">Configuring Your Systems</title>
  <shortdesc>Describes how to prepare your operating system environment for Greenplum Database
    software installation.</shortdesc>
  <!--GPDB 6.x +-->
  <body>
    <p>Perform the following tasks in order:</p>
    <ol>
      <li id="ji162035">Make sure your host systems meet the requirements described in <xref
          href="platform-requirements.xml"/>.</li>
      <li><xref href="#topic_sqj_lt1_nfb" format="dita">Disable SELinux and firewall
          software.</xref></li>
      <li id="ji162039">
        <xref href="#topic3" type="topic" format="dita">Set the required operating system
          parameters.</xref>
      </li>
      <li><xref href="#topic_qst_s5t_wy" type="topic" format="dita">Synchronize system
          clocks.</xref></li>
      <li id="ji1620399">
        <xref href="#topic23" type="topic" format="dita">Create the gpadmin account.</xref>
      </li>
    </ol>
    <p>Unless noted, these tasks should be performed for <i>all</i> hosts in your Greenplum Database
      array (master, standby master, and segment hosts). </p>
    <p>The Greenplum Database host naming convention for the master host is <codeph>mdw</codeph> and
      for the standby master host is <codeph>smdw</codeph>.</p>
    <p>The segment host naming convention is <varname>sdwN</varname> where <varname>sdw</varname> is
      a prefix and <varname>N</varname> is an integer. For example, segment host names would be
        <codeph>sdw1</codeph>, <codeph>sdw2</codeph> and so on. NIC bonding is recommended for hosts
      with multiple interfaces, but when the interfaces are not bonded, the convention is to append
      a dash (<codeph>-</codeph>) and number to the host name. For example, <codeph>sdw1-1</codeph>
      and <codeph>sdw1-2</codeph> are the two interface names for host <codeph>sdw1</codeph>.</p>
    <!--Pivotal-->
    <p otherprops="pivotal">For information about running Greenplum Database in the cloud see
        <cite>Cloud Services</cite> in the <xref
        href="https://pivotal.io/pivotal-greenplum/greenplum-partner-marketplace" format="html"
        scope="external">Pivotal Greenplum Partner Marketplace</xref>.</p>
    <note type="important"> When data loss is not acceptable for a Pivotal Greenplum Database
      system, Greenplum Database master and segment mirroring must be enabled<ph
        otherprops="pivotal"> in order for the cluster to be supported by Pivotal</ph>. Without
      mirroring, <ph otherprops="oss-only">a single segment instance failure stops the
        cluster.</ph><ph otherprops="pivotal">system and data availability is not guaranteed.
        Pivotal will make best efforts to restore a cluster in this case.</ph> For information about
      master and segment mirroring, see <xref href="../admin_guide/intro/about_ha.xml" type="topic"
        format="dita" scope="peer">About Redundancy and Failover</xref> in the <i>Greenplum Database
        Administrator Guide</i>.</note>
    <!--Pivotal-->
    <note type="note" otherprops="pivotal">For information about upgrading Pivotal Greenplum
      Database from a previous version, see the <cite>Greenplum Database Release Notes</cite> for
      the release that you are installing.</note>
    <note type="note">Automating the configuration steps described in this topic and <xref
        href="install_gpdb.xml#topic1"/> with a system provisioning tool, such as Ansible, Chef, or
      Puppet, can save time and ensure a reliable and repeatable Greenplum Database installation.
    </note>
  </body>
  <topic id="topic_sqj_lt1_nfb">
    <title>Disabling SELinux and Firewall Software</title>
    <body>
      <p>For all Greenplum Database host systems running RHEL or CentOS, SELinux must be disabled.
        Follow these steps:<ol id="ol_ysm_5t1_nfb">
          <li>As the root user, check the status of
            SELinux:<codeblock># sestatus
SELinuxstatus: disabled</codeblock></li>
          <li>If SELinux is not disabled, disable it by editing the
              <codeph>/etc/selinux/config</codeph> file. As root, change the value of the
              <codeph>SELINUX</codeph> parameter in the <codeph>config</codeph> file as
            follows:<codeblock>SELINUX=disabled</codeblock></li>
          <li>If the System Security Services Daemon (SSSD) is installed on your systems, edit the
            SSSD configuration file and set the <codeph>selinux_provider</codeph> parameter to
              <codeph>none</codeph> to prevent SELinux-related SSH authentication denials that could
            occur even with SELinux disabled. As root, edit <codeph>/etc/sssd/sssd.conf</codeph> and
            add this parameter:<codeblock>selinux_provider=none</codeblock></li>
          <li>Reboot the system to apply any changes that you made and verify that SELinux is
            disabled.</li>
        </ol></p>
      <p>For information about disabling SELinux, see the SELinux documentation.</p>
      <p>You should also disable firewall software such as <codeph>iptables</codeph> (on systems
        such as RHEL 6.x and CentOS 6.x ), <codeph>firewalld</codeph> (on systems such as RHEL 7.x
        and CentOS 7.x), or <codeph>ufw</codeph> (on Ubuntu systems, disabled by default). </p>
      <p>If you decide to enable <codeph>iptables</codeph> with Greenplum Database for security
        purposes, see <xref href="enable_iptables.xml#topic1" format="dita"/>.</p>
      <p>Follow these steps to disable <codeph>iptables</codeph>: <ol id="ul_wrw_3v1_nfb">
          <li>As the root user, check the status of
              <codeph>iptables</codeph>:<codeblock># /sbin/chkconfig --list iptables</codeblock><p>If
                <codeph>iptables</codeph> is disabled, the command output
            is:</p><codeblock>iptables 0:off 1:off 2:off 3:off 4:off 5:off 6:off</codeblock></li>
          <li>If necessary, execute this command as root to disable
              <codeph>iptables</codeph>:<codeblock>/sbin/chkconfig iptables off</codeblock><p>You
              will need to reboot your system after applying the change.</p></li>
          <li>For systems with <codeph>firewalld</codeph>, check the status of
              <codeph>firewalld</codeph> with the
              command:<codeblock># systemctl status firewalld</codeblock><p>If
                <codeph>firewalld</codeph> is disabled, the command output
            is:</p><codeblock>* firewalld.service - firewalld - dynamic firewall daemon
   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled; vendor preset: enabled)
   Active: inactive (dead)</codeblock></li>
          <li>If necessary, execute these commands as root to disable
            <codeph>firewalld</codeph>:<codeblock># systemctl stop firewalld.service
# systemctl disable firewalld.service</codeblock></li>
        </ol></p>
      <p>For more information about configuring your firewall software, see the documentation for
        the firewall or your operating system.</p>
    </body>
  </topic>
  <topic id="topic3" xml:lang="en">
    <title id="ji162171">Recommended OS Parameters Settings</title>
    <body>
      <p>Greenplum requires that certain Linux operating system (OS) parameters be set on all hosts
        in your Greenplum Database system (masters and segments). </p>
      <p>In general, the following categories of system parameters need to be altered: </p>
      <ul>
        <li id="ji162186"><b>Shared Memory</b> - A Greenplum Database instance will not work unless
          the shared memory segment for your kernel is properly sized. Most default OS installations
          have the shared memory values set too low for Greenplum Database. On Linux systems, you
          must also disable the OOM (out of memory) killer. For information about Greenplum Database
          shared memory requirements, see the Greenplum Database server configuration parameter
              <codeph><xref href="../ref_guide/config_params/guc-list.xml#shared_buffers"
              scope="peer" format="dita">shared_buffers</xref></codeph> in the <cite>Greenplum
            Database Reference Guide</cite>.</li>
        <li id="ji162187"><b>Network</b> - On high-volume Greenplum Database systems, certain
          network-related tuning parameters must be set to optimize network connections made by the
          Greenplum interconnect. </li>
        <li id="ji162188"><b>User Limits</b> - User limits control the resources available to
          processes started by a user's shell. Greenplum Database requires a higher limit on the
          allowed number of file descriptors that a single process can have open. The default
          settings may cause some Greenplum Database queries to fail because they will run out of
          file descriptors needed to process the query. </li>
      </ul>
    
    <p>More specifically, you need to edit the following Linux configuration settings:</p>
    <ul id="ul_t1k_rl3_2jb">
      <li><xref href="#topic3/linux_hosts_file" format="dita"></xref></li>
      <li><xref href="#topic3/sysctl_file" format="dita"></xref></li>
      <li><xref href="#topic3/system_resources" format="dita"></xref></li>
      <li><xref href="#topic3/xfs_mount" format="dita"></xref></li>
      <li><xref href="#topic3/disk_io_settings" format="dita"></xref>
        <ul id="ul_fm1_2yc_2jb">
          <li>Read ahead values</li>
          <li>Disk I/O scheduler disk access</li>
        </ul></li>
      <li><xref href="#topic3/huge_pages" format="dita"></xref></li>
      <li><xref href="#topic3/ipc_object_removal" format="dita"></xref></li>
      <li><xref href="#topic3/ssh_max_connections" format="dita"></xref></li>
    </ul>
    <section id="linux_hosts_file">
      <title>The <codeph>hosts</codeph> File</title>
      <p>Edit the <codeph>/etc/hosts</codeph> file and make sure that it includes the host names and
        all interface address names for every machine participating in your Greenplum Database
        system.</p>
    </section>
    <section id="sysctl_file">
        <title>The <codeph>sysctl.conf</codeph> File</title>
        <p>The <codeph>sysctl.conf</codeph> parameters listed in this topic are for performance,
          optimization, and consistency in a wide variety of environments. Change these settings
          according to your specific situation and setup.</p>
        <p>Set the parameters in the <codeph>/etc/sysctl.conf</codeph> file and reload with
            <codeph>sysctl -p</codeph>: </p>
        <codeblock>
# kernel.shmall = _PHYS_PAGES / 2 # <b>See <xref href="#topic3/shared_memory_pages" format="dita">Shared Memory Pages</xref></b>
kernel.shmall = 4000000000
# kernel.shmmax = kernel.shmall * PAGE_SIZE 
kernel.shmmax = 500000000
kernel.shmmni = 4096
vm.overcommit_memory = 2 # <b>See <xref href="#topic3/segment_host_memory" format="dita">Segment Host Memory</xref></b>
vm.overcommit_ratio = 95 # <b>See <xref href="#topic3/segment_host_memory" format="dita">Segment Host Memory</xref></b>

net.ipv4.ip_local_port_range = 10000 65535 # <b>See <xref href="#topic3/port_settings" format="dita">Port Settings</xref></b>
kernel.sem = 500 2048000 200 40960
kernel.sysrq = 1
kernel.core_uses_pid = 1
kernel.msgmnb = 65536
kernel.msgmax = 65536
kernel.msgmni = 2048
net.ipv4.tcp_syncookies = 1
net.ipv4.conf.default.accept_source_route = 0
net.ipv4.tcp_max_syn_backlog = 4096
net.ipv4.conf.all.arp_filter = 1
net.core.netdev_max_backlog = 10000
net.core.rmem_max = 2097152
net.core.wmem_max = 2097152
vm.swappiness = 10
vm.zone_reclaim_mode = 0
vm.dirty_expire_centisecs = 500
vm.dirty_writeback_centisecs = 100
vm.dirty_background_ratio = 0 # <b>See <xref href="#topic3/system_memory" format="dita">System Memory</xref></b>
vm.dirty_ratio = 0
vm.dirty_background_bytes = 1610612736
vm.dirty_bytes = 4294967296</codeblock>
        <p id="shared_memory_pages"><b>Shared Memory Pages</b></p>
        <p>Greenplum Database uses shared memory to communicate between <codeph>postgres</codeph>
          processes that are part of the same <codeph>postgres</codeph> instance.
            <codeph>kernel.shmall</codeph> sets the total amount of shared memory, in pages, that
          can be used system wide. <codeph>kernel.shmmax</codeph> sets the maximum size of a single
          shared memory segment in bytes.</p>
        <p>Set <codeph>kernel.shmall</codeph> and <codeph>kernel.shmax</codeph> values based on your
          system's physical memory and page size. In general, the value for both parameters should
          be one half of the system physical memory. </p>
        <p>Use the operating system variables <codeph>_PHYS_PAGES</codeph> and
            <codeph>PAGE_SIZE</codeph> to set the parameters.</p>
        <codeblock>kernel.shmall = ( _PHYS_PAGES / 2)
kernel.shmmax = ( _PHYS_PAGES / 2) * PAGE_SIZE</codeblock>
        <p>To calculate the values for <codeph>kernel.shmall</codeph> and
            <codeph>kernel.shmax</codeph>, run the following commands using the
            <codeph>getconf</codeph> command, which returns the value of an operating system
          variable.</p>
        <codeblock>$ echo $(expr $(getconf _PHYS_PAGES) / 2) 
$ echo $(expr $(getconf _PHYS_PAGES) / 2 \* $(getconf PAGE_SIZE))</codeblock>
        <p>As best practice, we recommend you set the following values in the
            <codeph>/etc/sysctl.conf</codeph> file:</p>
        <codeblock>kernel.shmmax = 500000000
kernel.shmmni = 4096
kernel.shmall = 4000000000</codeblock>
        <p id="segment_host_memory"><b>Segment Host Memory</b></p>
        <p>The <codeph>vm.overcommit_memory</codeph> Linux kernel parameter is used by the OS to
          determine how much memory can be allocated to processes. For Greenplum Database, this
          parameter should always be set to 2. </p>
        <p><codeph>vm.overcommit_ratio</codeph> is the percent of RAM that is used for application
          processes and the remainder is reserved for the operating system. The default is 50 on Red
          Hat Enterprise Linux. </p>
        <p>For <codeph>vm.overcommit_ratio</codeph> tuning and calculation recommendations with
          resource group-based resource management or resource queue-based resource management,
          refer to <xref href="../admin_guide/wlmgmt_intro.html" scope="peer" format="dita">Options
            for Configuring Segment Host Memory</xref> in the <cite>Geenplum Database Administrator
            Guide</cite>. Also refer to the Greenplum Database server configuration parameter
              <codeph><xref href="../ref_guide/config_params/guc-list.xml#gp_vmem_protect_limit"
              scope="peer">gp_vmem_protect_limit</xref></codeph> in the <cite>Greenplum Database
            Reference Guide</cite>. </p>
        <p id="port_settings"><b>Port Settings</b></p>
        <p>To avoid port conflicts between Greenplum Database and other applications during
          Greenplum initialization, make a note of the port range specified by the operating system
          parameter <codeph>net.ipv4.ip_local_port_range</codeph>. When initializing Greenplum using
          the <codeph>gpinitsystem</codeph> cluster configuration file, do not specify Greenplum
          Database ports in that range. For example, if <codeph>net.ipv4.ip_local_port_range = 10000
            65535</codeph>, set the Greenplum Database base port numbers to these values. </p>
        <codeblock>PORT_BASE = 6000
MIRROR_PORT_BASE = 7000</codeblock>
        <p>For information about the the <codeph>gpinitsystem</codeph> cluster configuration file,
          see <xref href="../install_guide/init_gpdb.html#topic4" scope="peer" format="dita"
            >Initializing a Greenplum Database System</xref>.</p>
        <p>For Azure deployments with Greenplum Database avoid using port 65330; add the following
          line to <filepath>sysctl.conf</filepath>:
          <codeblock>net.ipv4.ip_local_reserved_ports=65330 </codeblock></p>
        <p>For additional requirements and recommendations for cloud deployments, see <i><xref
              href="../cloud/gpdb-cloud-tech-rec.html" format="html" scope="peer">Greenplum Database
              Cloud Technical Recommendations</xref></i>. </p>
        <p id="system_memory"><b>System Memory</b></p>
        <p>For host systems with more than 64GB of memory, these settings are recommended:
          <codeblock>vm.dirty_background_ratio = 0
vm.dirty_ratio = 0
vm.dirty_background_bytes = 1610612736 # 1.5GB
vm.dirty_bytes = 4294967296 # 4GB</codeblock></p>
        <p>For host systems with 64GB of memory or less, remove
            <codeph>vm.dirty_background_bytes</codeph> and <codeph>vm.dirty_bytes</codeph> and set
          the two <codeph>ratio</codeph> parameters to these values:
          <codeblock>vm.dirty_background_ratio = 3
vm.dirty_ratio = 10</codeblock></p>
        <p>Increase <codeph>vm.min_free_kbytes</codeph> to ensure <codeph>PF_MEMALLOC</codeph>
          requests from network and storage drivers are easily satisfied. This is especially
          critical on systems with large amounts of system memory. The default value is often far
          too low on these systems. Use this awk command to set <codeph>vm.min_free_kbytes</codeph>
          to a recommended 3% of system physical memory:
          <codeblock>awk 'BEGIN {OFMT = "%.0f";} /MemTotal/ {print "vm.min_free_kbytes =", $2 * .03;}'
               /proc/meminfo >> /etc/sysctl.conf </codeblock></p>
        <p>Do not set <codeph>vm.min_free_kbytes</codeph> to higher than 5% of system memory as
          doing so might cause out of memory conditions.</p>
      </section>
    <section id="system_resources">
      <title>System Resources Limits</title>
      <p>Set the following parameters in the <codeph>/etc/security/limits.conf</codeph> file:</p>
      <codeblock>* soft nofile 524288
* hard nofile 524288
* soft nproc 131072
* hard nproc 131072</codeblock>
      <p>For Red Hat Enterprise Linux (RHEL) and CentOS systems, parameter values in the
          <codeph>/etc/security/limits.d/90-nproc.conf</codeph> file (RHEL/CentOS 6) or
          <codeph>/etc/security/limits.d/20-nproc.conf</codeph> file (RHEL/CentOS 7) override the
        values in the <codeph>limits.conf</codeph> file. Ensure that any parameters in the override
        file are set to the required value. The Linux module <codeph>pam_limits</codeph> sets user
        limits by reading the values from the <codeph>limits.conf</codeph> file and then from the
        override file. For information about PAM and user limits, see the documentation on PAM and
          <codeph>pam_limits</codeph>.</p>
      <p>Execute the <codeph>ulimit -u</codeph> command on each segment host to display the maximum
        number of processes that are available to each user. Validate that the return value is
        131072.</p>
    </section>
    <section id="xfs_mount">
      <title>XFS Mount Options</title>
      <p>XFS is the preferred data storage file system on Linux platforms. Use the
            <codeph>mount</codeph> command with the following recommended XFS mount
          options:<codeblock>rw,nodev,noatime,nobarrier,inode64</codeblock></p>
      <p>See the <codeph>mount</codeph> manual page (<codeph>man mount</codeph> opens the man page)
          for more information about using this command.</p>
      <p>The XFS options can also be set in the <codeph>/etc/fstab</codeph> file. This example entry
        from an <codeph>fstab</codeph> file specifies the XFS options.</p>
      <codeblock>/dev/data /data xfs nodev,noatime,nobarrier,inode64 0 0</codeblock>
    </section>
    <section id="disk_io_settings">
      <title>Disk I/O Settings</title>
      <ul id="ul_oh4_vlk_2jb">
        <li>
          <p>Read-ahead value</p>
          <p>Each disk device file should have a read-ahead (<codeph>blockdev</codeph>) value of
            16384. To verify the read-ahead value of a disk device:</p>
          <codeblock># /sbin/blockdev --getra <varname>devname</varname></codeblock>
          <p>For example:</p>
          <codeblock># /sbin/blockdev --getra /dev/sdb</codeblock>
          <p>To set blockdev (read-ahead) on a device:</p>
          <codeblock># /sbin/blockdev --setra <varname>bytes</varname> <varname>devname</varname></codeblock>
          <p>For example:</p>
          <codeblock># /sbin/blockdev --setra 16384 /dev/sdb</codeblock>
          <p>See the manual page (man) for the <codeph>blockdev</codeph> command for more
            information about using that command (<codeph>man blockdev</codeph> opens the man
            page).</p>
            <note>The <codeph>blockdev --setra</codeph> command is not persistent. You must ensure
              the read-ahead value is set whenever the system restarts. How to set the value will
              vary based on your system.</note>
            <p>One method to set the <codeph>blockdev</codeph> value at system startup is by adding
              the <codeph>/sbin/blockdev --setra</codeph> command in the <codeph>rc.local</codeph>
              file. For example, add this line to the <codeph>rc.local</codeph> file to set the
              read-ahead value for the disk
              <codeph>sdb</codeph>.<codeblock>/sbin/blockdev --setra 16384 /dev/sdb</codeblock></p>
            <p>On systems that use systemd, you must also set the execute permissions on the
                <codeph>rc.local</codeph> file to enable it to run at startup. For example, on a
              RHEL/CentOS 7 system, this command sets execute permissions on the
              file.<codeblock># chmod +x /etc/rc.d/rc.local</codeblock></p>
            <p>Restart the system to have the setting take effect.</p>
        </li>
        <li>
          <p>Disk I/O scheduler</p>
          <p>The Linux disk I/O scheduler for disk access supports different policies, such as
              <codeph>CFQ</codeph>, <codeph>AS</codeph>, and <codeph>deadline</codeph>.</p>
          <p>The <codeph>deadline</codeph> scheduler option is recommended. To specify a scheduler
            until the next system reboot, run the following:</p>
          <codeblock># echo <varname>schedulername</varname> &gt; /sys/block/<varname>devname</varname>/queue/scheduler</codeblock>
          <p>For example:</p>
          <codeblock># echo deadline &gt; /sys/block/sbd/queue/scheduler</codeblock>
          <note>Using the <codeph>echo</codeph> command to set the disk I/O scheduler policy is not
              persistent, therefore you must ensure the command is run whenever the system reboots.
              How to run the command will vary based on your system.</note>
          <p>One method to set the I/O scheduler policy at boot time is with the
              <codeph>elevator</codeph> kernel parameter. Add the parameter
              <codeph>elevator=deadline</codeph> to the <cmdname>kernel</cmdname> command in the
            file <codeph>/boot/grub/grub.conf</codeph>, the GRUB boot loader configuration file.
            This is an example <cmdname>kernel</cmdname> command from a <codeph>grub.conf</codeph>
            file on RHEL 6.x or CentOS 6.x. The command is on multiple lines for readability.</p>
          <codeblock>kernel /vmlinuz-2.6.18-274.3.1.el5 ro root=LABEL=/
                 <b>elevator=deadline</b> crashkernel=128M@16M  quiet console=tty1
                 console=ttyS1,115200 panic=30 transparent_hugepage=never 
                 initrd /initrd-2.6.18-274.3.1.el5.img</codeblock>
          <p>To specify the I/O scheduler at boot time on systems that use <codeph>grub2</codeph>
            such as RHEL 7.x or CentOS 7.x, use the system utility <codeph>grubby</codeph>. This
            command adds the parameter when run as
            root.<codeblock># grubby --update-kernel=ALL --args="elevator=deadline"</codeblock></p>
          <p>After adding the parameter, reboot the system.</p>
          <p>This <codeph>grubby</codeph> command displays kernel parameter
            settings.<codeblock># grubby --info=ALL</codeblock></p>
          <p>For more information about the <codeph>grubby</codeph> utility, see your operating
            system documentation. If the <codeph>grubby</codeph> command does not update the
            kernels, see the <xref href="#topic3/grubby_note" format="dita">Note</xref> at the end
            of the section.</p>
        </li>
      </ul>
    </section>
    <section id="huge_pages">
      <title>Transparent Huge Pages (THP)</title>
      <p>Disable Transparent Huge Pages (THP) as it degrades Greenplum Database performance. RHEL
        6.0 or higher enables THP by default. One way to disable THP on RHEL 6.x is by adding the
        parameter <codeph>transparent_hugepage=never</codeph> to the <cmdname>kernel</cmdname>
        command in the file <codeph>/boot/grub/grub.conf</codeph>, the GRUB boot loader
        configuration file. This is an example <cmdname>kernel</cmdname> command from a
          <codeph>grub.conf</codeph> file. The command is on multiple lines for
        readability:<codeblock>kernel /vmlinuz-2.6.18-274.3.1.el5 ro root=LABEL=/
           elevator=deadline crashkernel=128M@16M  quiet console=tty1
           console=ttyS1,115200 panic=30 <b>transparent_hugepage=never</b> 
           initrd /initrd-2.6.18-274.3.1.el5.img</codeblock></p>
      <p>On systems that use <codeph>grub2</codeph> such as RHEL 7.x or CentOS 7.x, use the system
        utility <codeph>grubby</codeph>. This command adds the parameter when run as
        root.<codeblock># grubby --update-kernel=ALL --args="transparent_hugepage=never"</codeblock></p>
      <p>After adding the parameter, reboot the system.</p>
      <p>For Ubuntu systems, install the <codeph>hugepages</codeph> package and execute this command
        as root:<codeblock># hugeadm --thp-never</codeblock></p>
      <p>This <cmdname>cat</cmdname> command checks the state of THP. The output indicates that THP
        is
        disabled.<codeblock>$ cat /sys/kernel/mm/*transparent_hugepage/enabled
always [never]</codeblock></p>
      <p>For more information about Transparent Huge Pages or the <codeph>grubby</codeph> utility,
        see your operating system documentation. If the <codeph>grubby</codeph> command does not
        update the kernels, see the <xref href="#topic3/grubby_note" format="dita">Note</xref> at
        the end of the section.</p>
    </section>
    <section id="ipc_object_removal">
      <title>IPC Object Removal</title>
      <p>Disable IPC object removal for RHEL 7.2 or CentOS 7.2, or Ubuntu. The default
          <codeph>systemd</codeph> setting <codeph>RemoveIPC=yes</codeph> removes IPC connections
        when non-system user accounts log out. This causes the Greenplum Database utility
          <codeph>gpinitsystem</codeph> to fail with semaphore errors. Perform one of the following
        to avoid this issue.</p>
      <ul id="ul_l21_t2v_q5">
        <li>When you add the <codeph>gpadmin</codeph> operating system user account to the master
          node in <xref href="#topic23" type="topic" format="dita"/>, create the user as a system
          account. </li>
        <li> Disable <codeph>RemoveIPC</codeph>. Set this parameter in
            <codeph>/etc/systemd/logind.conf</codeph> on the Greenplum Database host systems.
            <codeblock>RemoveIPC=no</codeblock><p>The setting takes effect after restarting the
              <codeph>systemd-login</codeph> service or rebooting the system. To restart the
            service, run this command as the root
          user.</p><codeblock>service systemd-logind restart</codeblock></li>
      </ul>
    </section>
    <section id="ssh_max_connections">
      <title>SSH Connection Threshold</title>
      <p>Certain Greenplum Database management utilities including <codeph>gpexpand</codeph>,
            <codeph>gpinitsystem</codeph>, and <codeph>gpaddmirrors</codeph>, use secure shell (SSH)
          connections between systems to perform their tasks. In large Greenplum Database
          deployments, cloud deployments, or deployments with a large number of segments per host,
          these utilities may exceed the hosts' maximum threshold for unauthenticated connections.
          When this occurs, you receive errors such as: <codeph>ssh_exchange_identification:
            Connection closed by remote host</codeph>.</p>
      <p>To increase this connection threshold for your Greenplum Database system, update the SSH
            <codeph>MaxStartups</codeph> and <codeph>MaxSessions</codeph> configuration parameters
          in one of the <codeph>/etc/ssh/sshd_config</codeph> or <codeph>/etc/sshd_config</codeph>
          SSH daemon configuration files.</p>
        <p>If you specify <codeph>MaxStartups</codeph> and <codeph>MaxSessions</codeph> using a
          single integer value, you identify the maximum number of concurrent unauthenticated
          connections (<codeph>MaxStartups</codeph>) and maximum number of open shell, login, or
          subsystem sessions permitted per network connection (<codeph>MaxSessions</codeph>). For
          example:<codeblock>MaxStartups 200
MaxSessions 200</codeblock></p>
        <p>If you specify <codeph>MaxStartups</codeph> using the
          "<varname>start:rate:full</varname>" syntax, you enable random early connection drop by
          the SSH daemon. <varname>start</varname> identifies the maximum number of unauthenticated
          SSH connection attempts allowed. Once <varname>start</varname> number of unauthenticated
          connection attempts is reached, the SSH daemon refuses <varname>rate</varname> percent of
          subsequent connection attempts. <varname>full</varname> identifies the maximum number of
          unauthenticated connection attempts after which all attempts are refused. For
          example:<codeblock>Max Startups 10:30:200
MaxSessions 200</codeblock></p>
        <p>Restart the SSH daemon after you update <codeph>MaxStartups</codeph> and
            <codeph>MaxSessions</codeph>. For example, on a CentOS 6 system, run the following
          command as the <codeph>root</codeph>
        user:<codeblock># service sshd restart</codeblock></p>
      <p>For detailed information about SSH configuration options, refer to the SSH documentation
        for your Linux distribution.</p>
      <!--<li>On some SUSE Linux Enterprise Server platforms, the Greenplum Database utility
              <codeph>gpssh</codeph> fails with the error message <codeph>out of pty
              devices</codeph>. A workaround is to add Greenplum Database operating system users,
            for example <codeph>gpadmin</codeph>, to the tty group. On SUSE systems, tty is required
            to run <codeph>gpssh</codeph></li>-->
      <note id="grubby_note">If the <codeph>grubby</codeph> command does not update the kernels of a
        RHEL 7.x or CentOS 7.x system, you can manually update all kernels on the system. For
        example, to add the parameter <codeph>transparent_hugepage=never</codeph> to all kernels on
        a system. <ol id="ol_bxf_g3x_j1b">
          <li>Add the parameter to the <codeph>GRUB_CMDLINE_LINUX</codeph> line in the file
            parameter in
            <codeph>/etc/default/grub</codeph>.<codeblock>GRUB_TIMEOUT=5
GRUB_DISTRIBUTOR="$(sed 's, release .*$,,g' /etc/system-release)"
GRUB_DEFAULT=saved
GRUB_DISABLE_SUBMENU=true
GRUB_TERMINAL_OUTPUT="console"
GRUB_CMDLINE_LINUX="crashkernel=auto rd.lvm.lv=cl/root rd.lvm.lv=cl/swap rhgb quiet <b>transparent_hugepage=never</b>"
GRUB_DISABLE_RECOVERY="true"</codeblock></li>
          <li>As root, run the <codeph>grub2-mkconfig</codeph> command to update the
            kernels.<codeblock># grub2-mkconfig -o /boot/grub2/grub.cfg</codeblock></li>
          <li>Reboot the system.</li>
        </ol></note>
    </section>
    </body>
  </topic>
  <topic xml:lang="en" id="topic_qst_s5t_wy">
    <title id="ji162593">Synchronizing System Clocks </title>
    <body>
      <p>You should use NTP (Network Time Protocol) to synchronize the system clocks on all hosts
        that comprise your Greenplum Database system. See <xref href="http://www.ntp.org"
          scope="external" format="html">www.ntp.org</xref> for more information about NTP. </p>
      <p>NTP on the segment hosts should be configured to use the master host as the primary time
        source, and the standby master as the secondary time source. On the master and standby
        master hosts, configure NTP to point to your preferred time server.</p>
      <section id="ji162603">
        <title>To configure NTP</title>
        <ol id="ol_zjk_knz_thb">
          <li id="ji162604">On the master host, log in as root and edit the
              <codeph>/etc/ntp.conf</codeph> file. Set the <codeph>server</codeph> parameter to
            point to your data center's NTP time server. For example (if
              <codeph>10.6.220.20</codeph> was the IP address of your data center's NTP
            server):<codeblock>server 10.6.220.20</codeblock></li>
          <li id="ji162606">On each segment host, log in as root and edit the
              <codeph>/etc/ntp.conf</codeph> file. Set the first <codeph>server</codeph> parameter
            to point to the master host, and the second server parameter to point to the standby
            master host. For example:<codeblock>server mdw prefer
server smdw</codeblock></li>
          <li id="ji162609">On the standby master host, log in as root and edit the
              <codeph>/etc/ntp.conf</codeph> file. Set the first <codeph>server</codeph> parameter
            to point to the primary master host, and the second server parameter to point to your
            data center's NTP time server. For
            example:<codeblock>server mdw prefer
server 10.6.220.20</codeblock></li>
          <li id="ji162612">On the master host, use the NTP daemon synchronize the system clocks on
            all Greenplum hosts. For example, using <codeph><xref
                href="../utility_guide/ref/gpssh.xml" format="dita" scope="peer"
                type="topic"
            >gpssh</xref></codeph>:<codeblock># gpssh -f hostfile_gpssh_allhosts -v -e 'ntpd'</codeblock></li>
        </ol>
      </section>
    </body>
  </topic>
  <topic id="topic23" xml:lang="en">
    <title id="ji1621713">Creating the Greenplum Administrative User</title>
    <body>
      <p>Create a dedicated operating system user account on each node to run and administer
        Greenplum Database. This user account is named <codeph>gpadmin</codeph> by convention.</p>
      <note type="important">You cannot run the Greenplum Database server as
        <codeph>root</codeph>.</note>
      <p>The <codeph>gpadmin</codeph> user must have permission to access the services and
        directories required to install and run Greenplum Database. </p>
      <p>The <codeph>gpadmin</codeph> user on each Greenplum host must have an SSH key pair
        installed and be able to SSH from any host in the cluster to any other host in the cluster
        without entering a password or passphrase (called "passwordless SSH"). If you enable
        passwordless SSH from the master host to every other host in the cluster ("1-<i>n</i>
        passwordless SSH"), you can use the Greenplum Database <codeph>gpssh-exkeys</codeph>
        command-line utility later to enable passwordless SSH from every host to every other host
          ("<i>n</i>-<i>n</i> passwordless SSH"). </p>
      <p>You can optionally give the <codeph>gpadmin</codeph> user sudo privilege, so that you can
        easily administer all hosts in the Greenplum Database cluster as <codeph>gpadmin</codeph>
        using the <codeph>sudo</codeph>, <codeph>ssh/scp</codeph>, and <codeph>gpssh/gpscp</codeph>
        commands.</p>
      <p>The following steps show how to set up the <codeph>gpadmin</codeph> user on a host, set a
        password, create an SSH key pair, and (optionally) enable sudo capability. These steps must
        be performed as root on every Greenplum Database cluster host. (For a large Greenplum
        Database cluster you will want to automate these steps using your system provisioning
        tools.)</p>
      <note>See <xref href="ansible-example.xml#Untitled1"/> for an example that shows how to
        automate the tasks of creating the <codeph>gpadmin</codeph> user and installing the
        Greenplum Database software on all hosts in the cluster. </note>
      <ol id="ol_k23_3cz_thb">
        <li>Create the <codeph>gpadmin</codeph> group and user.
          <note>If you are installing Greenplum Database on RHEL 7.2 or CentOS 7.2 and want to
            disable IPC object removal by creating the <codeph>gpadmin</codeph> user as a system
            account, provide both the <codeph>-r</codeph> option (create the user as a system
            account) and the <codeph>-m</codeph> option (create a home directory) to the
              <codeph>useradd</codeph> command. On Ubuntu systems, you must use the
              <codeph>-m</codeph> option with the <codeph>useradd</codeph> command to create a home
            directory for a user.</note><p>This example creates the <codeph>gpadmin</codeph> group,
            creates the <codeph>gpadmin</codeph> user as a system account with a home directory and
            as a member of the <codeph>gpadmin</codeph> group, and creates a password for the
            user.<codeblock># groupadd gpadmin
# useradd gpadmin -r -m -g gpadmin
# passwd gpadmin
New password: &lt;changeme&gt;
Retype new password: &lt;changeme&gt;</codeblock></p>
          <note>Make sure the <codeph>gpadmin</codeph> user has the same user id (uid) and group id
            (gid) numbers on each host to prevent problems with scripts or services that use them
            for identity or permissions. For example, backing up Greenplum databases to some
            networked filesy stems or storage appliances could fail if the <codeph>gpadmin</codeph>
            user has different uid or gid numbers on different segment hosts. When you create the
              <codeph>gpadmin</codeph> group and user, you can use the <codeph>groupadd -g</codeph>
            option to specify a gid number and the <codeph>useradd -u</codeph> option to specify the
            uid number. Use the command <codeph>id gpadmin</codeph> to see the uid and gid for the
              <codeph>gpadmin</codeph> user on the current host.</note></li>
        <li>Switch to the <codeph>gpadmin</codeph> user and generate an SSH key pair for the
            <codeph>gpadmin</codeph>
          user.<codeblock>$ su gpadmin
$ ssh-keygen -t rsa -b 4096
Generating public/private rsa key pair.
Enter file in which to save the key (/home/gpadmin/.ssh/id_rsa):
Created directory '/home/gpadmin/.ssh'.
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
</codeblock>At
          the passphrase prompts, press Enter so that SSH connections will not require entry of a
          passphrase.</li>
        <li>(Optional) Grant sudo access to the <codeph>gpadmin</codeph> user.<p>On Red Hat or
            CentOS, run <codeph>visudo</codeph> and uncomment the <codeph>%wheel</codeph> group
            entry.<codeblock>%wheel        ALL=(ALL)       NOPASSWD: ALL</codeblock></p><p>Make sure
            you uncomment the line that has the <codeph>NOPASSWD</codeph> keyword. </p><p>Add the
              <codeph>gpadmin</codeph> user to the <codeph>wheel</codeph> group with this
            command.</p><codeblock># usermod -aG wheel gpadmin</codeblock></li>
      </ol>
    </body>
  </topic>
  <topic id="topic_acx_5xb_vhb">
    <title>Next Steps</title>
    <body>
      <ul id="ul_ibb_1yb_vhb">
        <li><xref href="install_gpdb.xml#topic1">Installing Greenplum Database Software</xref></li>
        <li><xref href="validate.xml#topic1">Validating Your Systems</xref></li>
        <li><xref href="init_gpdb.xml#topic1">Initializing a Greenplum Database System</xref></li>
      </ul>
    </body>
  </topic>
</topic>
